{
    "version": "https://jsonfeed.org/version/1",
    "title": "Willie's Blog",
    "description": "",
    "home_page_url": "https://gszhangwei.github.io",
    "items": [
        {
            "id": "https://gszhangwei.github.io/2025/06/18/PDD-practice-in-the-field-of-data/",
            "url": "https://gszhangwei.github.io/2025/06/18/PDD-practice-in-the-field-of-data/",
            "title": "Structured prompt driven development, practiced in the field of data",
            "date_published": "2025-06-18T11:00:00.000Z",
            "content_html": "<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>What sparks emerge when the Structured Prompt Driven Development (PDD) methodology meets complex data processing scenarios?</p>\n<p>A recent data project practice gave me the opportunity to systematically apply the PDD methodology to complex SQL development in BigQuery. Facing the dual challenges of 30+ business branch validation logic and billion-scale snapshot table performance optimization, traditional development approaches, especially without relevant complex SQL processing experience, would prove inadequate. After exploration and trial-and-error, I found a stable application path for PDD in the data domain, and more importantly, validated the practical effectiveness of this methodology in handling complex business logic.</p>\n<p>If you’re also seeking methods to improve complex SQL development efficiency, or curious about how PDD works in data engineering projects, this practical experience may provide valuable reference for you.</p>\n<h2 id=\"Project-Background\"><a href=\"#Project-Background\" class=\"headerlink\" title=\"Project Background\"></a>Project Background</h2><p><strong>Technical Environment</strong></p>\n<ul>\n<li><strong>Data Processing</strong>: Python + BigQuery + GCP</li>\n<li><strong>Core Challenges</strong>: Complex business scenarios, 30+ business branches + billion-scale data volume</li>\n</ul>\n<p><strong>Business Objectives</strong><br>Associate and compare predicted data with actual revenue data, identify discrepancies and complete missing information</p>\n<ul>\n<li><strong>Data Association</strong>: Comprehensive comparison between predicted and actual revenue data, automatic identification of difference points</li>\n<li><strong>Hierarchical Matching</strong>: Four-level progressive matching of Opportunity→Account→Market→Region, precise marking of matching depth</li>\n<li><strong>Standard Unification</strong>: Cross-table field caliber consistency processing, eliminating data silos</li>\n<li><strong>Decision Support</strong>: Output structured comparison datasets directly serving business analysis</li>\n</ul>\n<p><strong>Core Process</strong></p>\n<ul>\n<li><strong>Multi-condition Association</strong>: Sequentially match data at four levels (opportunity, account, market, and region), separately marking successful matches and unmatched records</li>\n<li><strong>Snapshot-based Extension</strong>: Expand unmatched rows based on business snapshot dates to ensure aligned data views for all time points</li>\n<li><strong>Business Data Completion</strong>: Complete missing fields (ID, name, status, etc.) from different data sources according to priority</li>\n<li><strong>Field Standardization:</strong>: Unify regional abbreviations to full names, convert specific contract types, and supplement additional marking fields</li>\n<li><strong>Aggregation and Deduplication</strong>: Merge all intermediate results, group and deduplicate by key dimensions, output final comparison report</li>\n</ul>\n<h2 id=\"Implementation-Process\"><a href=\"#Implementation-Process\" class=\"headerlink\" title=\"Implementation Process\"></a>Implementation Process</h2><h3 id=\"Phase-One-Exploration-and-Breakthrough-with-AI-Collaboration\"><a href=\"#Phase-One-Exploration-and-Breakthrough-with-AI-Collaboration\" class=\"headerlink\" title=\"Phase One: Exploration and Breakthrough with AI Collaboration\"></a>Phase One: Exploration and Breakthrough with AI Collaboration</h3><p><strong>Reality Challenges</strong></p>\n<p>As an engineer who hadn’t touched complex SQL development for two years, facing such business scenarios, honestly, my first reaction was “where to start?” Fortunately, AI became my most capable assistant.</p>\n<ol>\n<li><strong>Rapid Breakthrough in Business Understanding</strong>: AI helped me quickly deconstruct seemingly complex business requirements<ul>\n<li>Transformed abstract financial alignment requirements into specific data operation steps</li>\n<li>Clarified the core logic of multi-level matching</li>\n<li>Defined the data processing framework: <strong>Preprocessing → Extension → Completion → Output</strong></li>\n</ul>\n</li>\n<li><strong>Iterative Exploration of Technical Solutions</strong>: Initially relied entirely on AI generation, but quickly encountered reality challenges<ul>\n<li><strong>Complexity Challenge</strong>: The business logic complexity far exceeded AI’s one-time understanding capability</li>\n<li><strong>SQL Code Understanding Difficulty</strong>: Required understanding AI-generated code thinking to gradually grasp overall approach, quite time-consuming</li>\n<li><strong>Detail Explosion</strong>: Each abstract task hid countless technical details (table associations, matching strategies, exception handling, etc.)</li>\n</ul>\n</li>\n<li><strong>First Complete Attempt</strong>: After multiple rounds of prompt iterations, AI generated a “giant SQL” of 1,157 lines<ul>\n<li><strong>Result</strong>: Massive syntax errors in BigQuery </li>\n<li><strong>After fixes</strong>: Business logic validation failed</li>\n</ul>\n</li>\n</ol>\n<p><img loading=\"lazy\" src=\"/../images/sql_result_v1_en.png\" alt=\"sql_result_v1_en.png\"></p>\n<p><img loading=\"lazy\" src=\"/../images/sql_related_prompt_v1_en.png\" alt=\"sql_related_prompt_v1_en.png\"></p>\n<p><strong>Key Takeaway</strong>: Although SQL generation didn’t meet expectations, the most valuable outcome of this phase was establishing a complete AI-assisted verification process:</p>\n<p><code>Write structured prompts → Generate SQL code → Generate test datasets based on prompts → Complete logic verification</code></p>\n<p>This process establishment laid a solid foundation for subsequent PDD methodology application. Although this experiment ended in “failure,” it gave me new insights into complex task decomposition: AI’s true value lies not in solving all problems at once, but in building efficient human-machine collaboration modes.</p>\n<h3 id=\"Phase-Two-Understanding-Business-Essence-Reconstructing-Solutions\"><a href=\"#Phase-Two-Understanding-Business-Essence-Reconstructing-Solutions\" class=\"headerlink\" title=\"Phase Two: Understanding Business Essence, Reconstructing Solutions\"></a>Phase Two: Understanding Business Essence, Reconstructing Solutions</h3><p><strong>From Complex Back to Simple</strong></p>\n<p>After the “detours” of the first phase, I began re-examining this seemingly complex requirement. Suddenly, a key insight emerged: this is essentially a data comparison problem between two tables! Once grasping this core, the entire solution approach became instantly clear, and fear of the unknown gradually dissipated.</p>\n<p><strong>True Power of PDD Methodology</strong><br>Based on understanding the business essence, I began systematically reconstructing the solution:</p>\n<ol>\n<li><p><strong>Task Decomposition Strategy</strong></p>\n<p>Following the principle of “<strong>essence determines boundaries, features determine details</strong>,” decomposed complex SQL requirements into relatively independent functional units, making each unit focus on solving a clear problem—this is a more effective AI collaboration mode</p>\n<ul>\n<li>Using PDD methodology, transformed implementation logic into structured prompts</li>\n<li>Direct effect of task simplification: <strong>AI hallucinations significantly reduced, code generation accuracy substantially improved</strong></li>\n</ul>\n<p><img loading=\"lazy\" src=\"/../images/sql_related_prompt_v2_1_en.png\" alt=\"sql_related_prompt_v2_1_en.png\"><img loading=\"lazy\" src=\"/../images/sql_related_prompt_v2_2_en.png\" alt=\"sql_related_prompt_v2_2_en.png\"></p>\n</li>\n<li><p>Rapid Verification Loop</p>\n<p>Benefit by the verification process established in phase one, each unit could quickly complete accuracy verification. The reconstruction results comparison:</p>\n<p><img loading=\"lazy\" src=\"/../images/comparison_of_reconstruction_effects_en.png\" alt=\"comparison_of_reconstruction_effects_en.png\"></p>\n</li>\n</ol>\n<p>However, when switching to online environment with real billion-scale data validation, new challenges emerged, <strong>Performance Bottleneck</strong></p>\n<ul>\n<li><strong>Execution Time</strong>: Unable to complete in over 5 minutes</li>\n<li><strong>Intermediate Results</strong>: Expanded to 268+GB<ul>\n<li><img loading=\"lazy\" src=\"/../images/SQL_processing_result.png\" alt=\"SQL_processing_result.png\"></li>\n</ul>\n</li>\n<li><strong>Next Direction</strong>: SQL performance optimization became the new key focus</li>\n</ul>\n<h3 id=\"Phase-Three-Performance-Optimization\"><a href=\"#Phase-Three-Performance-Optimization\" class=\"headerlink\" title=\"Phase Three: Performance Optimization\"></a>Phase Three: Performance Optimization</h3><p>Facing performance bottlenecks, I adopted a systematic optimization strategy:</p>\n<ol>\n<li><p><strong>Problem Diagnosis and Root Cause Analysis</strong>:Through consultation with senior experts in the data domain within the team, deep analysis revealed the core problem</p>\n<ul>\n<li><strong>Core Issue</strong>: AI-generated SQL didn’t fully consider online data scale (already at billion level), performing multiple complex Join operations on large tables, seriously affecting query performance.</li>\n</ul>\n</li>\n<li><p><strong>Optimization Strategy Formulation</strong>: Based on problem root causes, we determined four key optimization directions</p>\n<ul>\n<li><strong>Reduce Join Frequency:</strong>: Large tables should perform Join operations only once, obtaining all necessary data at once</li>\n<li><strong>Front-end Data Filtering</strong>: Filter data before joining with large tables, effectively reducing driving table scale</li>\n<li><strong>Precise Join Type Selection</strong>: Reasonably choose Inner Join or Left Join based on business requirements</li>\n<li><strong>Step-by-step Query Optimization</strong>: Split complex queries into multiple steps, selecting only necessary fields, maximizing reduction of data transmission</li>\n</ul>\n</li>\n<li><p><strong>Prompt Fine-tuning</strong>: For performance bottleneck parts, I conducted deep reconstruction of related prompts</p>\n<ul>\n<li><strong>Locate Optimization Targets</strong>: Precisely identify SQL fragments requiring optimization and their corresponding prompts</li>\n<li><strong>Complex Problem Decomposition</strong>: According to established optimization directions, gradually decompose complex queries into independent prompt components</li>\n<li><strong>Individual Verification Optimization</strong>: Generate SQL separately for each prompt component and conduct performance verification, ensuring complete decomposition of complex JOINs</li>\n<li><strong>Task Decomposition Display</strong>: Optimized Prompt partial effects shown below:<br><img loading=\"lazy\" src=\"/../images/performance_enhanced_result.png\" alt=\"performance_enhanced_result.png\"></li>\n<li><strong>Before Optimization</strong>: SQL code execution timeout, no results in 5 minutes, involving data volume up to 268GB</li>\n<li><strong>After Optimization</strong>: SQL code execution completed in only 37 seconds, final result data volume only 3.7GB</li>\n</ul>\n</li>\n</ol>\n<p>Performance improvement exceeded 800%, data processing efficiency significantly enhanced.</p>\n<p><img loading=\"lazy\" src=\"/../images/sql_processing_result_final.png\" alt=\"sql_processing_result_final.png\"></p>\n<p>In this process, when optimization direction was determined, the completion logic actually needed rewriting, but through phases one and two exploration, the stable generation mode for expected SQL in the Data domain was established, so even rewriting was fine—this process was completed very quickly. Subsequent branch logic fixes were also handled, essentially similar steps, so won’t be demonstrated here.</p>\n<h2 id=\"Structured-Prompt-Driven-Development-PDD-Application-Methodology-in-Data-Domain\"><a href=\"#Structured-Prompt-Driven-Development-PDD-Application-Methodology-in-Data-Domain\" class=\"headerlink\" title=\"Structured Prompt Driven Development (PDD) Application Methodology in Data Domain\"></a>Structured Prompt Driven Development (PDD) Application Methodology in Data Domain</h2><p>Based on this project practice, I summarized the PDD application methodology in data domain, particularly SQL development, providing reference for similar scenarios.</p>\n<h3 id=\"Core-Applications-of-PDD-in-SQL-Development\"><a href=\"#Core-Applications-of-PDD-in-SQL-Development\" class=\"headerlink\" title=\"Core Applications of PDD in SQL Development\"></a>Core Applications of PDD in SQL Development</h3><p><strong>Scenario Classification and Strategy</strong></p>\n<ul>\n<li><strong>Simple Scenarios</strong>: Basic queries (no need for data preprocessing, small data volume requirements) can be directly generated without much adjustment</li>\n<li><strong>Complex Scenarios</strong>: Require structured verification and iterative optimization<ul>\n<li><strong>Definition of Done (DOD)</strong>: Pre-define acceptance criteria (such as output format, performance indicators) as validation baseline for initial SQL</li>\n<li><strong>Iterative Optimization</strong>: Identify problems according to DOD (logic errors, edge cases), gradually adjust prompts, generate new SQL until passing all test cases</li>\n<li><strong>Performance Tuning</strong>: Prioritize small-step refactoring (such as gradual optimization adjustments based on intermediate tables), avoid large-scale rewrites, reduce risks and maintain readability</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Overall-Practice-Process\"><a href=\"#Overall-Practice-Process\" class=\"headerlink\" title=\"Overall Practice Process\"></a>Overall Practice Process</h3><ul>\n<li><strong>Human Input</strong>: Structured prompts(written according to <a href=\"https://central.thoughtworks.net/blogs/ls/content/4612643020956565/structured-prompt-driven-development-workflow-transforming-software-development-from-2-days-to-1-hour-4f860bc0-64b6-4bee-b807-dc6f6ae6ffe0\">PDD methodology</a>)</li>\n<li><strong>AI Output</strong>: Initial SQL code</li>\n<li><strong>Verification Phase</strong>: Check functional correctness against DOD</li>\n<li><strong>Iterative Optimization</strong>: Adjust prompts based on problems (such as clarifying ambiguous logic)</li>\n<li><strong>Performance Tuning</strong>: Combine AI suggestions for optimization (such as JOIN strategies, data preprocessing strategies, etc.)</li>\n</ul>\n<h3 id=\"Implementation-Recommendations-and-Best-Practices\"><a href=\"#Implementation-Recommendations-and-Best-Practices\" class=\"headerlink\" title=\"Implementation Recommendations and Best Practices\"></a>Implementation Recommendations and Best Practices</h3><ol>\n<li><strong>Clarifying Business Requirements Before Formal Prompt Writing</strong><ul>\n<li>Use documentation or other appropriate non-technical methods to reach consensus with business personnel</li>\n<li>Clearly define the Definition of Done (DOD) to avoid frequent requirement changes during development (extremely important, especially in SQL scenarios, as it improves the efficiency of validating phased objectives)</li>\n<li>Establish business analysis documentation to facilitate handling edge cases. When encountering edge cases, review the business analysis documentation and add them to the corresponding tasks in structured prompts</li>\n</ul>\n</li>\n<li><strong>Determining Technical Solution Framework Based on Business Requirements</strong><ul>\n<li><strong>When implementation plan is unclear</strong>:<ul>\n<li>Communicate with AI based on organized data processing logic to obtain feasible abstract steps</li>\n<li>Combine personal understanding to determine the final abstract steps of the implementation approach</li>\n<li>Use structured prompts to generate SQL code based on the steps when the implementation approach becomes clear</li>\n</ul>\n</li>\n<li><strong>When Implementation Approach is Clear</strong>:<ul>\n<li>Define abstract steps for implementation requirements</li>\n<li>Refine each task according to the abstract steps</li>\n<li>Debug one task at a time; there’s no need to generate everything at once</li>\n<li>After the first task is stabilized, use the same pattern to write prompts for the remaining tasks</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Iterative Optimization</strong><ul>\n<li>For SQL that doesn’t meet expectations, locate the corresponding prompt area and supplement details</li>\n<li>Combine professional reasoning models to obtain SQL optimization suggestions or implementation approach improvements</li>\n<li>Update prompt records with content agreed upon with AI for future reuse</li>\n</ul>\n</li>\n<li><strong>Overall Strategy</strong><ul>\n<li>If unfamiliar with technical stack implementation details: First describe logic and framework in natural language, let AI generate SQL based on prompts, debug and finalize, then summarize and update back to the prompt file</li>\n<li>If very familiar with implementation details: Can first write structured prompts and gradually generate SQL code; alternatively, directly hand-write SQL, then use AI to organize prompts according to PDD methodology patterns, summarizing the implementation approach and updating it to the prompt</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"Personnel-Capability-Requirements-in-Complex-Scenarios\"><a href=\"#Personnel-Capability-Requirements-in-Complex-Scenarios\" class=\"headerlink\" title=\"Personnel Capability Requirements in Complex Scenarios\"></a>Personnel Capability Requirements in Complex Scenarios</h3><p><strong>Core Skills</strong></p>\n<ul>\n<li><strong>PDD Theory Mastery</strong>: Identify and eliminate AI hallucinations, such as correcting unreasonable JOIN logic, disabling subqueries, etc.</li>\n<li><strong>Requirements Analysis and Solution Design Capability</strong>: Transform vague requirements into precise technical specifications, accurately locate data sources, and build modular solutions</li>\n<li><strong>Structured Management</strong>: Transform unstructured requirements into structured expressions by organizing contextual information, and build high-quality prompts based on PDD theory</li>\n</ul>\n<p><strong>AI Cognitive Ability</strong></p>\n<ul>\n<li><strong>Advantage Recognition</strong>: Rapid prototyping, pattern recognition</li>\n<li><strong>Disadvantage Awareness</strong>: Edge case handling, contextual ambiguity resolution</li>\n<li><strong>Tool Selection</strong>: Use domain-specific models for technical tasks, general AI for general tasks</li>\n<li><strong>Verification Attitude</strong>: Maintain an optimistic iterative mindset, strictly verify results based on DOD and manual review</li>\n</ul>\n<h2 id=\"Summary-and-Future-Outlook\"><a href=\"#Summary-and-Future-Outlook\" class=\"headerlink\" title=\"Summary and Future Outlook\"></a>Summary and Future Outlook</h2><p>Through this in-depth practice of PDD in the data domain, I have gained the following five key insights:</p>\n<ol>\n<li><strong>Goal-Oriented Path Planning</strong>: Clear Definition of Done (DOD) helps provide relatively clear directional guidance for the development process, reducing the possibility of getting lost when facing complex problems. Reasonably set goal frameworks can usually improve problem-solving efficiency.</li>\n<li><strong>Methodological Adaptability Exploration</strong>: When facing entirely new technical domains, PDD core principles demonstrate certain applicability and migration potential. Through attempts and validation in different scenarios, we can gradually accumulate understanding of the methodology’s boundary conditions and applicable scope.</li>\n<li><strong>Problem-Driven Progressive Breakthrough</strong>: When facing technical challenges or AI hallucination problems, divide-and-conquer, step-by-step strategies are far more effective than direct abandonment. Continuous exploration within set time frames and seeking professional support at key nodes can significantly improve problem-solving success rates.</li>\n<li><strong>Cumulative Effect of Efficiency Improvement</strong>: In the initial exploration of new technical fields, AI tools may not immediately bring significant efficiency improvements. Substantial efficiency enhancement usually requires experiencing process optimization, experience pattern precipitation, and continuous practice accumulation.</li>\n<li><strong>Positive Cycle of Personal Capability Growth</strong>: Under the AI First Software Delivery (AIFSD) model, we pursue not only current efficiency optimization but also achieving spiral personal capability improvement through continuous practice, ultimately bringing sustainable efficiency enhancement. Repeated application of unified methodology allows experience accumulation and cognitive improvement to mutually promote each other, ultimately forming a benign human-AI collaboration ecosystem.</li>\n</ol>\n<p><img loading=\"lazy\" src=\"/../images/cognitive_improve_process_by_using_AI_en.png\" alt=\"cognitive_improve_process_by_using_AI_en.png\"></p>\n<p>Through this practice, we have preliminarily explored the application possibilities of PDD methodology in complex SQL scenarios in the data domain and summarized a feasible path for handling complex SQL in the Data field (click to read the original text to view structured prompt templates). This provides a practical foundation for reference in subsequent similar scenario work.</p>\n<p>Looking to the future, with application in more actual projects and continuous optimization of the methodology, PDD has the potential to bring certain degrees of improvement in software development efficiency and quality. Through continuous practical reflection and experience accumulation, we expect to gradually improve the applicable boundaries and operational details of this methodology.</p>\n<p>Finally, thank you for reading!</p>\n",
            "tags": [
                "AI",
                "Prompts",
                "Governance"
            ]
        },
        {
            "id": "https://gszhangwei.github.io/2025/06/09/AI-assisted-software-delivery-full-process-maturity-model-white-paper-en/",
            "url": "https://gszhangwei.github.io/2025/06/09/AI-assisted-software-delivery-full-process-maturity-model-white-paper-en/",
            "title": "AI-Assisted Software Delivery Maturity Model:A Comprehensive Guide from L0 to L5",
            "date_published": "2025-06-09T11:00:00.000Z",
            "content_html": "<h2 id=\"Introduction-and-Background\"><a href=\"#Introduction-and-Background\" class=\"headerlink\" title=\"Introduction and Background\"></a>Introduction and Background</h2><p>Facing the rapidly changing market and technological environment, an increasing number of enterprises are beginning to explore the application of artificial intelligence (AI) in software delivery processes to enhance efficiency and innovation capabilities. However, different organizations have varying levels of practical maturity in AI-enabled software engineering, urgently requiring a grading model to guide their evolution paths. Just as the autonomous driving field adopts L0 to L5 level definitions to describe the evolution process from manual driving to fully autonomous driving, the software delivery field can also adopt a similar grading approach. This article is aimed at practitioners and managers in the AI-assisted software delivery field, proposing an L0-L5 maturity grading model for “AI-assisted full-process software delivery,” comprehensively elaborating the characteristics and practical methods of each maturity level from requirements analysis, design, development, testing to deployment and operations. This article will also provide typical scenarios and industry cases for each level, helping practitioners understand how AI-enabled software delivery can be implemented and bring benefits. Additionally, we have designed a set of operational maturity self-assessment tools, including key judgment criteria and visual assessment dimensions, for teams to evaluate their current level. Finally, this article will provide evolution path recommendations for each level, clarifying the measures, transformation elements, and key success factors for advancing from the current level upward, providing reference for enterprises to formulate AI engineering capability enhancement plans.</p>\n<h2 id=\"Overview-of-AI-Assisted-Software-Delivery-Maturity-Model\"><a href=\"#Overview-of-AI-Assisted-Software-Delivery-Maturity-Model\" class=\"headerlink\" title=\"Overview of AI-Assisted Software Delivery Maturity Model\"></a>Overview of AI-Assisted Software Delivery Maturity Model</h2><p>The AI-assisted software delivery maturity model is divided into six levels from L0 to L5, depicting an evolutionary path where software delivery processes gradually transition from being completely human-driven to AI-autonomous leadership. In the lower-level stages, software development remains human-centered, with AI providing only limited tool support; while in the higher-level stages, AI not only undertakes major development work but can even orchestrate the entire process, achieving “AI-led” intelligent development. This model resembles a pyramid-shaped hierarchical pathway where, as levels increase, the corresponding software process platforms, data and knowledge accumulation, and AI capabilities progressively strengthen. Each level complements the others, and enterprises must first establish solid process systems and data foundations before effectively leveraging higher-level AI capabilities. This evolutionary pattern mirrors the automotive industry’s progression from L0 (no assistance) to L5 (full autonomous driving): the L0 stage focuses primarily on manual operations and specifications, while the L5 stage is managed by an AI “super brain” capable of overseeing the overall development and operations of software projects. Practitioners can leverage this model to assess their organization’s current state of AI-enabled software delivery and develop phased capability enhancement roadmaps accordingly.</p>\n<p><img loading=\"lazy\" src=\"/../images/AIFSD_maturity_model.png\" alt=\"AIFSD_maturity_model.png\"></p>\n<p><em>Figure 1: AI-Assisted Software Delivery Maturity Model L0–L5 Schematic Diagram (From Manual-Driven to AI Autonomous Evolution). This model depicts in a graded manner the depth and breadth of AI adoption by organizations throughout the entire software lifecycle, including requirements, development, testing, deployment, and operations. Lower maturity levels primarily rely on manual processes and standards, while higher levels gradually transition to AI-dominated human-AI collaboration, ultimately culminating in a fully intelligent delivery ecosystem.</em></p>\n<p>Next, we will elaborate in detail on the definitions, AI capability characteristics, human-AI division of labor, and practical key points under the Structured Prompt-Driven Development (PDD) methodology for each level from L0 to L5. Each level will be combined with typical use scenarios or industry cases to illustrate how that level is applied in actual business contexts and the benefits it generates.</p>\n<h2 id=\"L0-Level-Traditional-Delivery-Mode-Without-AI-Assistance\"><a href=\"#L0-Level-Traditional-Delivery-Mode-Without-AI-Assistance\" class=\"headerlink\" title=\"L0 Level: Traditional Delivery Mode Without AI Assistance\"></a>L0 Level: Traditional Delivery Mode Without AI Assistance</h2><p><strong>Definition and AI Capabilities</strong>: L0 level represents organizations that have not yet introduced any AI intelligent capabilities in software delivery, relying entirely on traditional human resources and existing tools to complete work at all stages. The core of this stage is to establish a clear software development process system and strictly follow standardized processes (such as CMMI) for requirements, design, coding, testing, and operations. Teams rely on trained engineers and comprehensive process documentation to ensure project implementation, with the orderly execution of development processes primarily achieved through personnel experience and adherence to specifications. In other words, L0-level software delivery is characterized by “human-driven” processes, where all decisions and creative activities are completed by humans, and AI appears only as basic tools (such as code editors, static analyzers) without participating in intelligent decision-making.</p>\n<p><strong>Human-AI Division of Labor</strong>: At the L0 stage, AI capabilities are essentially absent. While the tools used may include certain automation functions (such as code highlighting, syntax auto-completion, refactoring tools provided by IDEs), these belong to pre-programmed rules or simple algorithmic support, not AI intelligence. Therefore, in terms of human-AI division of labor, humans are the absolute subject: requirements analysis, architecture design, coding implementation, test case writing, defect identification and fixing, deployment and operations - all stages are completed manually. AI’s role is limited to accelerating the speed of manual execution (such as static code scanning improving code review efficiency), but it does not intelligently transform the process itself.</p>\n<p><strong>PDD Practice</strong>: Since generative AI has not been introduced, L0 level basically has no “prompt-driven” development practices. Developers may search for information through search engines and use scripts to automate some repetitive tasks, but this does not fall within the PDD scope. At this stage, it can be considered that the Prompt-Driven Development methodology has not yet started. Knowledge acquisition during the development process mainly relies on manual queries and experience transfer, rather than depending on large language models. Practitioners at the L0 stage focus on process standardization and personnel skill development, without yet involving AI empowerment.</p>\n<p><strong>Typical Scenarios and Cases</strong>: Most traditional software project teams have been at L0 maturity. For example, a financial industry software development team that strictly follows CMMI specifications has comprehensive templates and checklists at all project stages, with personnel conducting requirements reviews and architecture design, and manually writing all code and test scripts. Even when continuous integration tools are used, they are manually configured and triggered, essentially remaining human-controlled software delivery pipelines. The benefits of this model are reflected in orderly and controllable processes, with output quality dependent on team experience and specification execution. However, efficiency and innovation are constrained by the upper limits of personnel capabilities. With the development of AI technology, the completely human-driven model exposes shortcomings such as relatively low efficiency and difficulty in rapidly responding to changes. Practitioners often regard L0 as a baseline, measuring current efficiency and quality to provide comparative basis for subsequently introducing AI methods.</p>\n<h2 id=\"L1-Level-Basic-AI-Assisted-Development\"><a href=\"#L1-Level-Basic-AI-Assisted-Development\" class=\"headerlink\" title=\"L1 Level: Basic AI-Assisted Development\"></a>L1 Level: Basic AI-Assisted Development</h2><p><strong>Definition and AI Capabilities</strong>: L1 level marks the beginning of organizations introducing preliminary AI assistance in software delivery processes, primarily manifested through the application of tools such as intelligent programming assistants. At this stage, AI possesses code understanding and generation capabilities based on large models, but its scope of influence is limited to local aspects such as programming assistance. For example, utilizing large models like Claude to achieve intelligent code completion (capable of completing entire lines or segments of code based on context, rather than just syntax-rule-based completion), automatically generating function comments, providing code refactoring suggestions, and automatically generating unit tests. These AI capabilities significantly improve development efficiency and code quality, but AI still lacks autonomous decision-making authority over global projects. In short, AI at the L1 stage is equivalent to an “intelligent assistant”: capable of understanding context and providing suggestions or fragments, yet unable to independently complete complex tasks.</p>\n<p><strong>Human-AI Division of Labor</strong>: In the L1 stage, humans still dominate the main software delivery activities, while AI plays a supporting role. Developers use tools similar to GitHub Copilot to automatically complete boilerplate code during coding, and testers have ChatGPT draft test cases based on requirement specifications, which are then reviewed and modified by humans. Key decisions such as architecture solution selection and module design are still formulated by humans, and AI outputs require human review and judgment. The human-AI relationship at the L1 stage can be vividly compared to driving assistance: engineers hold the steering wheel, AI provides navigation or power assistance, but the ultimate route and control remain in human hands.</p>\n<p><strong>PDD Practice</strong>: At the L1 level, Prompt-Driven Development practices begin to emerge, but they are mostly scattered individual attempts. Developers might ask ChatGPT questions when encountering problems, or write unstructured prompts to have AI generate code for specific functionalities. Each engineer adopts AI in different ways, and unified team processes have not yet been formed. Common practices include:</p>\n<ul>\n<li><strong>Direct Code Generation Using Chat Format</strong>: Developers describe the required function’s functionality in natural language, have AI return code snippets, and then integrate them into the project themselves.</li>\n<li><strong>Explanation and Optimization Prompts</strong>: When code reports errors or runtime results don’t meet expectations, prompts are used to request AI to explain the cause of problems and provide modification suggestions.</li>\n<li><strong>Documentation and Testing Prompts</strong>: Writing prompts to have AI automatically generate documentation explanations based on code, or produce initial drafts of test cases based on requirement descriptions.</li>\n</ul>\n<p>These prompt practices are not systematic processes, but rather means by which engineers spontaneously utilize AI to improve personal work efficiency. For example, a developer can use prompts to have AI generate boilerplate code for CRUD interfaces, saving 20%-50% of time; test engineers use prompts to have AI generate test cases based on user stories, then manually review and adjust them, thereby accelerating test writing. It’s worth noting that this stage lacks standardized prompt writing specifications, and AI usage depends more on individual skills and experience.</p>\n<p><strong>Typical Scenarios and Benefits</strong>: Typical cases include developers using AI-assisted tools such as Cursor, Windsurf, GitHub Copilot, and others for automatic code completion in actual projects. In these scenarios, AI is used as individual tools by each person and has not yet been deeply embedded into team processes. Nevertheless, L1-level practices have already brought significant benefits: productivity typically gains considerable improvement, with some reports showing that individual efficiency can increase by 20% to 50%. Meanwhile, code quality has also improved—AI-generated standardized code and testing suggestions help reduce low-level errors. However, due to the lack of global coordination, team collaboration benefits are limited, and AI’s value is mainly reflected in reducing individual burden rather than transforming overall processes. This is the preliminary stage of organizations moving toward AI empowerment, a process “from nothing to something”: allowing employees to become familiar with AI tools, using small-scale successes to prove value and lay the foundation for further AI integration.</p>\n<h2 id=\"L2-Level-Team-Collaborative-AI-Integration\"><a href=\"#L2-Level-Team-Collaborative-AI-Integration\" class=\"headerlink\" title=\"L2 Level: Team Collaborative AI Integration\"></a>L2 Level: Team Collaborative AI Integration</h2><p><strong>Definition and AI Capabilities</strong>: L2 level marks AI assistance transitioning from individual to team-oriented, achieving preliminary end-to-end integration across the entire software delivery pipeline. AI capabilities expand to understanding engineering context and even covering tasks like requirements, coding, testing, and deployment through multi-agent collaboration. This means different AI Agents emerge for different roles: one AI responsible for parsing requirements and breaking down high-level requirements into development tasks; another AI writing corresponding code; AI automatically generating and executing test cases; and even AI Agents helping with deployment and release. A series of intelligent agents can work collaboratively, assisting humans in completing the entire development process in a one-stop manner.</p>\n<p><strong>Human-AI Division of Labor</strong>: In the team collaborative AI integration stage, human-AI relationships enter a collaborative mode. Humans are no longer using AI in isolation, but teams jointly formulate AI usage strategies. Clear AI participation phases emerge in the development process: for example, AI automatically generates detailed requirement specifications based on user stories, then humans review them; AI produces code based on specifications, with humans conducting code reviews and integration; AI generates test cases and executes them, with testers only analyzing failed cases; operations personnel have AI Agents monitor logs and automatically propose performance optimization suggestions. Human roles partially shift from direct executors to supervisors and coordinators: humans formulate tasks and supervise AI completion, incorporate AI outputs into processes, and handle parts that AI cannot resolve or high-risk components. Although AI can already assume multiple roles such as “digital architect,” “automated coder,” and “virtual tester,” ultimate project responsibility still lies with the team. This can be likened to extending human-AI pair programming to the entire team: each phase has AI assistants working together, but humans must coordinate these assistants to work in harmony.</p>\n<p><strong>PDD Practice</strong>: At the L2 stage, Prompt-Driven Development begins to systematically integrate into team development workflows. Organizations establish shared Prompt libraries and usage standards, ensuring team members use consistent prompt patterns across various phases to obtain predictable AI outputs. Typical PDD practices at this stage include:</p>\n<ul>\n<li><strong>Requirements Phase</strong>: BAs or product managers use carefully designed Prompt templates to have AI automatically refine user stories into requirement specifications or prototypes;</li>\n<li><strong>Development Phase</strong>: Teams prepare Prompt paradigms for common coding tasks (such as prompt templates for REST API interface implementation), calling these templates during development to efficiently produce standardized code;</li>\n<li><strong>Testing Phase</strong>: QA teams maintain Prompt libraries for test case generation, enabling quick generation of test cases covering main paths for different types of requirement descriptions;</li>\n<li><strong>Deployment Phase</strong>: Operations teams use Prompts to guide AI in writing deployment scripts, infrastructure configurations, or log analysis reports.</li>\n</ul>\n<p>In L2, Prompt-driven has become part of team workflows: everyone collectively improves prompt engineering, exchanges which prompts work better, and even uses internal tools to manage Prompt versions. Teams may also integrate AI into CI&#x2F;CD pipelines by calling LLM APIs, implementing functions like automated code review and automatic performance analysis. PDD practices at this stage upgrade AI from personal assistant to team assistant, with inputs and outputs from various phases forming connections, making Prompts a “programming language” that drives software production.</p>\n<p><img loading=\"lazy\" src=\"/../images/PDD_Iterative_Loop_Schematic.png\" alt=\"PDD_Iterative_Loop_Schematic.png\"></p>\n<p><em>Figure 2: Typical Iterative Cycle Diagram of Prompt-Driven Development (PDD). Each development iteration is divided into three steps: first, developers write Prompts describing the required functionality; then AI generates code or solutions based on the Prompts; finally, developers validate AI outputs and make adjustments (such as error correction and optimization) before entering the next cycle. Unlike the traditional Copilot mode where engineers lead and AI assists in generating fragments, in PDD mode AI generates the vast majority of code, and engineers’ primary work shifts to describing requirements and optimizing AI outputs. This new paradigm of human-AI division of labor receives preliminary practice at the L2 level.</em></p>\n<p><strong>Typical Scenarios and Benefits</strong>: L2 level practices have already emerged in some leading-edge teams. For example, our team established a shared Prompt library that enables developers or testers to generate most test cases with one click based on user stories, with AI then executing tests and producing reports. Another example is our use of conversational AI to parse requirement documents and break down tasks, generating preliminary technical designs that are then reviewed by humans for details. In terms of industry cases, Cognizant’s “Devin” has been promoted as the world’s first AI software engineer agent, capable of automatically producing code and completing deployment given high-level requirements. Although practical experience reveals that current AI agents can only complete simple, small-scale applications and the technology is not yet fully mature, it validates the feasibility of L2 level capabilities.</p>\n<p>In terms of benefits, compared to L1 level individual efficiency improvements, L2 level brings team-level efficiency leaps and quality consistency. Reports indicate that productivity in certain phases may increase by two to three times. Through standardized Prompts and AI assistant collaboration, teams reduce repetitive labor, decrease human errors, and significantly improve development speed and test coverage. Meanwhile, teams begin accumulating data from AI-project interactions, laying the foundation for higher levels of autonomy. However, it must be emphasized that L2 level AI is still limited to medium-to-low complexity scenarios and often struggles with large, complex systems, still requiring human leadership to tackle difficult problems. Therefore, L2 is more viewed as a “collaborative enhancement” stage—AI gives teams “wings to soar,” but has not yet independently undertaken complete delivery work.</p>\n<h2 id=\"L3-Level-AI-Led-Complex-System-Development\"><a href=\"#L3-Level-AI-Led-Complex-System-Development\" class=\"headerlink\" title=\"L3 Level: AI-Led Complex System Development\"></a>L3 Level: AI-Led Complex System Development</h2><p><strong>Definition and AI Capabilities</strong>: L3 level signifies that AI has reached the capability to autonomously develop complex software systems. At this stage, AI can not only complete code generation for individual modules but also understand and control the system requirements and architecture of large-scale projects. It can automatically design overall architecture based on high-level requirements, generate high-quality code, implement comprehensive testing, and finally complete deployment. In other words, AI’s capabilities extend to having a “big picture perspective,” enabling it to handle complex projects such as large-scale enterprise applications, high-performance computing systems, and real-time control systems, rather than being limited to simple CRUD applications. This level of AI is equivalent to possessing the combined capabilities of a senior architect + full-stack developer + test engineer. It’s worth noting that although AI is powerful enough to output complete systems, human experts still need to intervene and provide guidance for certain extremely complex or highly customized requirements. Therefore, L3 does not eliminate the human role but rather positions AI as the primary developer, with humans transitioning to minimal intervention in complex edge cases.</p>\n<p><strong>Human-AI Division of Labor</strong>: In the L3 stage, the development process exhibits characteristics of “AI-first, human supervision.” When a new requirement arrives, AI typically provides the initial solution: AI automatically writes product specifications or design documents based on past knowledge, then engineers review and adjust; subsequently, AI generates the main code framework and unit modules, with humans only making modifications during code reviews or for critical algorithms; testing is intelligently completed by AI through self-generation and self-execution, with manual work mainly focusing on special tests that AI hasn’t covered; deployment processes are also automatically completed by AI pipelines, significantly reducing manual configuration operations. It can be seen that most work outputs (documents, code, tests, deployment scripts) involve AI participation or even leadership. Humans increasingly play the roles of quality guardians and strategic decision-makers: gatekeeping AI outputs at milestone points, handling parts that AI is not good at or beyond its experience range, and setting overall strategies. The entire organization forms “AI-first operations”: before employees begin any task, they typically first have AI generate an initial draft or suggested solution, then proceed with subsequent work based on this. This transformation greatly improves the starting point of work, allowing people to focus on higher-level problems. It can be said that L3 level achieves extensive and deep AI empowerment in software development: AI is everywhere, but humans control the direction behind the scenes.</p>\n<p><strong>PDD Practice</strong>: In the L3 stage, prompt-driven development has been deeply integrated into enterprise standard processes, forming mature methodologies. First, organizations establish prompt patterns and paradigms for different types of tasks, available for employees to invoke in various scenarios, bringing prompt usage into an industrialized stage. Since AI participates in almost all aspects, prompt engineering practices also cover requirements, design, development, testing, and operations. For example:</p>\n<ul>\n<li>Requirements&#x2F;Design Prompts: Product managers use structured prompt templates to have AI output complete PRD documents or prototype design drafts, then manually adjust details. These prompts may include industry-specific vocabulary and format requirements to ensure AI outputs comply with company standards.</li>\n<li>Code Generation Prompts: Development teams accumulate extensive domain code development patterns and develop related platforms for prompt template governance. When implementing certain common functions, engineers only need to select corresponding code implementation patterns on the platform and have AI combine business details, allowing AI to batch produce module code.</li>\n<li>Testing and Operations Prompts: Testing personnel and operations personnel jointly formulate prompts to have AI automatically deduce potential failures based on system design and generate failure recovery scripts, or generate problem diagnostic reports based on monitoring data.</li>\n</ul>\n<p>Additionally, L3 stage organizations may have dedicated Prompt Engineer&#x2F;Architect roles (established according to organizational needs), responsible for maintaining and optimizing prompt libraries, ensuring that prompt-driven approaches function efficiently across the entire company. Prompt writing gradually becomes standardized and professionalized, with processes similar to code reviews to ensure prompt quality. As AI capabilities improve, some prompts can be generated and improved by AI itself (meta-prompt optimization), forming AI self-improvement loops. This mature PDD practice allows AI to fully play its role: AI becomes the default first executor, while prompts become the interface language for human-AI collaboration.</p>\n<p><strong>Typical Scenarios and Benefits</strong>: Many leading technology companies are advancing toward L3 capabilities. For example, a large software enterprise mandates “AI first, then manual”: whether writing design documents, code, or test cases, employees must first invoke the internal engineering practice prompt governance platform to generate initial drafts, then refine based on these. Another example is enterprises that have developed internal knowledge bases and LLM search tools, supporting employees in querying system architecture and historical implementation details through conversational methods, thereby significantly accelerating understanding and development speed. In these practices, AI participates in the starting point of almost every task, becoming the default assistant for engineers’ daily work.</p>\n<p>The benefits brought by L3 level are company-wide productivity leaps and quality assurance. Due to AI’s extensive involvement, teams deliver more features in the same amount of time, and time-to-production is shortened. Meanwhile, automated testing and analysis improve quality baselines, reducing bugs and failures. More importantly, the L3 stage lays the foundation for further achieving full automation: enterprises accumulate extensive structured prompts for AI-human collaboration and related data, improve AI governance frameworks, and cultivate employee culture of trusting and utilizing AI. Managers gradually notice that as AI takes on more work, teams can attempt more ambitious innovation projects because AI can always provide solution suggestions for human decision-making. It should be noted that advancing toward L3 also brings challenges—such as ensuring the correctness, consistency, explainability, and traceability of AI-generated content, making the establishment of corresponding governance mechanisms even more critical (detailed in subsequent sections on self-assessment tools and governance dimensions). Overall, L3 level announces that organizations have entered a new stage of “comprehensive AI empowerment deployment”: AI is ubiquitous and reliability reaches practical levels, with human resources beginning to shift from specific implementation to high-level supervision and innovation tasks.</p>\n<h2 id=\"L4-Level-Autonomous-Agent-Driven-Innovation-Development\"><a href=\"#L4-Level-Autonomous-Agent-Driven-Innovation-Development\" class=\"headerlink\" title=\"L4 Level: Autonomous Agent-Driven Innovation Development\"></a>L4 Level: Autonomous Agent-Driven Innovation Development</h2><p><strong>Definition and AI Capabilities</strong>: Level L4 represents a stage of high autonomy and innovation in AI-empowered software delivery. At this stage, AI is not only capable of autonomously completing established software development tasks but can also proactively propose new solutions and improvements based on insights into the environment and requirements. This means AI evolves from an executor to an “innovation engine”: capable of analyzing large amounts of data, identifying potential market opportunities or technical optimization points, and then automatically designing and implementing new features or applications.<br>Technically, L4 typically consists of more powerful intelligent agents—these AI agents possess advanced decision-making, planning, and contextual reasoning abilities, enabling them to execute complex task chains without explicit human instructions. For example, an AI agent can automatically monitor user feedback and system performance data, discover improvement opportunities in a module, autonomously create development tasks, complete coding, testing, and deploy improvements. Another example is the presence of autonomous AI project managers within companies, who proactively generate new product concepts or feature proposals based on strategic goals and product usage data. In short, L4 AI possesses creative thinking close to that of human product managers and architects, driving software evolution proactively, surpassing “task completion on demand” to begin leading development directions. </p>\n<p><strong>Human-AI Division of Labor</strong>: When AI attains autonomy and creativity, the human-AI division of labor further changes, presenting a new pattern of “AI-led, human-guided”. Specifically, many daily decisions and task arrangements are actively executed by AI agents, with humans mainly setting goals and constraints at the strategic level and intervening to evaluate major decisions proposed by AI. For example, task allocation and tracking may be handled by AI project management agents: AI automatically assigns work items to different engineering AIs or human engineers based on priority and tracks progress; problem diagnosis and repair can be autonomously performed by operations AI, which upon detecting system anomalies automatically creates issues, locates causes, provides preliminary repair plans, and notifies relevant personnel.<br>In these processes, practitioners mostly act as monitors, ensuring AI decisions align with company strategies and intervening when AI deviates from expectations or encounters ethical&#x2F;compliance issues. At the L4 stage, human teams can confidently delegate a large amount of repetitive and coordination work to AI agents, freeing up time to focus on innovation strategies. It can be said that AI becomes a team member, even taking on the cumbersome and heavy management and support work within the team, elevating humans to the roles of mentors and final decision-makers. A hallmark change is that many future work meetings will be driven by AI intelligence, for example, AI can lead daily stand-ups, summarize team progress in real-time, and proactively identify project bottlenecks, with human members cooperating with AI’s rhythm to complete work. This highly autonomous model brings unprecedented efficiency and scale benefits but also requires organizations to have mature AI governance and trust mechanisms to support it.</p>\n<p><strong>PDD Practice</strong>: At the L4 stage, prompts are no longer just tools for humans to command AI; AI itself also generates and uses prompts. Since AI agents can autonomously decompose tasks and invoke other models or tools to execute them, each autonomous action is often backed by dynamically generated prompts by AI. For example, an AI agent receiving a high-level goal will automatically construct a series of prompts to ask code generation models to write certain modules or call operation models to check system status, a process similar to human engineers assigning tasks to different experts, except the communication language remains prompts.<br>From a human perspective, PDD at L4 mainly manifests as:</p>\n<ul>\n<li>High-Level Goals to Prompt Chains: Humans set strategic goals or constraints for AI, which converts them into a series of internal subtask prompts, completing solution reasoning through self-dialogue. This can be seen as a self-evolving version of prompt-driven development.</li>\n<li>Dynamic Prompt Adjustment: AI agents can dynamically adjust prompt content based on real-time feedback, for example, if a subtask fails, AI modifies the prompt and retries (similar to COT and ReAct frameworks, giving AI some self-correction ability).</li>\n<li>Prompt Best Practice Library Maintained by AI: At L4, humans likely no longer write many prompts directly, as AI has taken over most prompt construction work. However, organizations still maintain prompt governance rules (e.g., prohibiting certain sensitive words, following specific formats) and monitor the effectiveness of AI-generated prompts.</li>\n</ul>\n<p>Therefore, prompt engineering enters a latent operation stage—it remains the cornerstone for AI to complete complex tasks, but most prompts are automatically generated by AI according to scenarios, with humans only providing high-level guidance and adjusting AI prompt strategies when necessary. Overall, PDD at L4 reaches high maturity: prompt language becomes a universal interface for communication and collaboration between AIs and between AI and humans, with various development activities driven by a series of prompt chains, many of which no longer require manual intervention.</p>\n<p><strong>Typical Scenarios and Benefits</strong>: A vivid example of L4 is the emergence of some unattended operation and intelligent decision-making systems. For example, a leading internet company has built an internal AI assistant to automatically handle GitHub issues: this AI monitors newly submitted issues around the clock, can classify priorities, assign responsible persons, provide preliminary solutions, and notify relevant stakeholders. As a result, a large number of trivial matters are efficiently handled without human involvement, and the development team only needs to focus on high-priority or AI-unsolvable issues.<br>Another example is some DevOps teams deploying intelligent deployment steward AI, which automatically completes building, testing, deployment to specific environments, and regression testing when detecting new code merged into the main branch, all without human intervention. If abnormalities are found, it immediately rolls back and records analysis reports.<br>In terms of benefits, L4 brings huge time savings and collaboration cost reductions. Many internal communications and coordination work within the team are replaced by AI pipelines, reducing human waiting and repeated communication, significantly accelerating project delivery. At the business level, since AI can autonomously identify improvement opportunities, enterprise innovation cycles accelerate, potentially launching new features quickly to gain competitive advantages. Another important gain is scale effects: organizations can undertake more projects and larger user volumes without significantly increasing manpower, as AI agents take on a considerable portion of the work.<br>Of course, moving to L4 also requires management to have foresight and risk control capabilities: it is necessary to establish supervision mechanisms for AI decisions, contingency plans, and cultivate employees to adapt to new ways of working with AI. In summary, L4 represents software delivery entering a “semi-autonomous” or even near “fully autonomous” state, with AI beginning to play a leading role and creating unprecedented value for enterprises.</p>\n<h2 id=\"L5-Level-Fully-Autonomous-AI-Delivery-Ecosystem\"><a href=\"#L5-Level-Fully-Autonomous-AI-Delivery-Ecosystem\" class=\"headerlink\" title=\"L5 Level: Fully Autonomous AI Delivery Ecosystem\"></a>L5 Level: Fully Autonomous AI Delivery Ecosystem</h2><p><strong>Definition and AI Capabilities</strong>: L5 represents the pinnacle of AI-assisted software delivery maturity, signifying the construction of a comprehensively intelligent autonomous software engineering ecosystem. At this stage, enterprises possess highly sophisticated AI platforms and infrastructure, with AI almost completely dominating the entire software delivery process, requiring human intervention only in rare cases for high-level decision-making or intervention. Specifically, L5-level AI can be vividly described as a “super brain” - equivalent to a central AI system that integrates development, testing, deployment, and operations functions, capable of coordinating the overall situation like a senior project manager while executing various details like an expert development team (truly representing artificial general intelligence in the software delivery domain). When new business requirements are proposed, humans need only describe business objectives or product vision to the AI in natural language, and the AI super brain can autonomously complete all work from requirements analysis, architecture design, and code implementation to testing verification, deployment, and subsequent monitoring optimization, continuously learning and improving throughout the process. L5-stage AI capabilities far exceed the programming realm, integrating cognitive reasoning, planning and learning, and cross-domain knowledge, achieving human expert-level performance or higher across all aspects of software engineering, with high reliability and adaptability. L5 can be described as an AI-native software factory: software development is no longer a series of manual tasks, but an AI-driven automated workflow capable of producing software at high speed and scale while continuously evolving based on feedback.</p>\n<p><strong>Human-AI Division of Labor</strong>: Upon reaching L5 level, the characteristics of human-AI division of labor are “AI autonomy with human-in-the-loop supervision” - AI is responsible for “doing,” while humans are responsible for “oversight.” Most daily decisions, optimizations, and executions are completed autonomously by the AI ecosystem, with humans primarily undertaking three responsibilities: First is strategic planning - executives define business strategies and objectives, from which AI derives product and technical implementation plans; Second is governance and review - ensuring AI behavior operates within legal, ethical, and business rule frameworks, such as conducting compliance checks on AI-designed solutions and approving important release milestones; Third is emergency intervention - when AI encounters novel problems it cannot solve or deviates from course, human experts intervene to handle the situation and provide feedback for AI learning. In essence, humans are completely liberated from specific development activities, instead focusing on setting direction and supervising results. Team organizational structures also change accordingly: departments may no longer be divided by traditional development, testing, and operations functions, but rather operate around AI platforms, establishing new functional departments such as “AI Platform Maintenance Groups” and “AI Ethics and Risk Management Committees” to ensure the smooth and efficient operation of this AI autonomous ecosystem. It’s important to emphasize that despite AI’s high degree of autonomy, human supervision remains indispensable - similar to how Level 5 autonomous driving still requires safety operators for monitoring, human oversight ensures that software AI does not deviate from company interests or social norms.</p>\n<p><strong>PDD Practice</strong>: In the L5 stage, Prompt-driven development achieves high-level abstraction. Humans no longer need to write specific low-level prompts, but instead directly interact with AI systems using natural language instructions, marking the true arrival of the natural language programming era. This can be seen as a higher-level manifestation of prompts: business strategy itself becomes a kind of “macro prompt,” which AI understands and expands into a series of bottom-up development actions. The AI ecosystem internally remains full of prompt interactions, but these are all generated and processed autonomously by AI, forming a closed-loop adaptive prompt chain system. For example, the AI super brain automatically adjusts prompts and strategies for the next phase based on results from the previous phase (similar to automatic parameter tuning and meta-learning) to continuously optimize output quality. From an external perspective, human input to AI is more like conversing with a senior manager, discussing requirements and constraints; AI then internally converts these into specific implementation step prompts. At this point, prompt engineering focuses more on system architecture rather than specific wording: how to design communication protocols between AIs, memory sharing mechanisms, feedback loops, etc. It can be said that prompt-driven becomes the internal working language of AI systems in L5, with humans only needing to focus on whether the mechanisms for AI understanding human intent are sound. Looking forward, as AI continues to self-optimize, perhaps even such explicit prompts will fade, and AI will be able to work through more advanced reasoning methods. However, based on current concepts, PDD still plays a crucial role in L5, with humans upgrading from “prompt writers” to “prompt architects” and “intent validators.”</p>\n<p><strong>Typical Scenarios and Benefits</strong>: Since L5 represents a future vision, the real world currently has no cases that have fully achieved L5 maturity, though some top technology companies are already showing early signs. For example, some in the industry have proposed the concept of “Software 3.0,” envisioning a future where software is automatically generated and deployed by AI based on requirements, completely revolutionizing traditional development processes. It’s foreseeable that enterprises at the L5 stage will lead the market: self-built AI systems that are more intelligent and better aligned with their own business than commercial tools, thereby forming competitive barriers that are difficult to replicate. In terms of benefits, L5 level will bring enterprises order-of-magnitude efficiency improvements (some predict 10 to 100 times increase in employee productivity), along with unprecedented innovation speed and business flexibility. Simultaneously, labor costs and error rates will be dramatically reduced, bringing software engineering into a highly sustainable state. However, climbing to L5 also comes with high investment and high risk: requiring continuous R&amp;D investment to train AI, establish comprehensive data and knowledge assets, and strong governance frameworks to ensure reliable AI behavior. Not all organizations need to nor have the capability to reach L5 maturity - managers should weigh target maturity levels based on their own strategies. In summary, L5 level depicts a new AI-native software production paradigm: under this paradigm, enterprises use AI as their core driving force, software delivery becomes unprecedentedly efficient and intelligent, and humans can concentrate their energy on vision and creation.</p>\n<h2 id=\"Maturity-Self-Assessment-Tool-Evaluation-Standards-and-Visualization-Dimensions\"><a href=\"#Maturity-Self-Assessment-Tool-Evaluation-Standards-and-Visualization-Dimensions\" class=\"headerlink\" title=\"Maturity Self-Assessment Tool: Evaluation Standards and Visualization Dimensions\"></a>Maturity Self-Assessment Tool: Evaluation Standards and Visualization Dimensions</h2><p>To drive the improvement of AI-assisted software delivery capabilities, practitioners need to first assess the current maturity level of their teams. To this end, we have designed a maturity self-assessment tool that covers key judgment criteria and visual assessment dimensions, helping teams identify their position, recognize gaps, and formulate improvement roadmaps. This assessment tool primarily includes the following elements:</p>\n<ul>\n<li><p><strong>Key Judgment Criteria</strong>: We have established a series of judgment criteria from five dimensions: people, process, technology, data, and governance. Each dimension corresponds to several checkpoints used to determine the maturity level achieved by the organization in that aspect. Specifically:</p>\n<ul>\n<li><p>People and Skills:<br>This examines the team’s proficiency with AI tools, AI-related skill training, and role allocation. For example, does the team have dedicated AI engineers or Prompt engineers (AI-assisted development enablement)? Can most developers proficiently use AI programming assistants? Does the organizational culture support human-AI collaboration? This dimension measures human readiness in an AI-enabled environment.</p>\n</li>\n<li><p>Process and Collaboration:<br>This evaluates whether AI is integrated into software delivery processes and team collaboration methods. For example, are AI participation steps defined in requirements, development, and testing processes? Has the team established standard Prompt usage processes or AI result review mechanisms? Do different roles achieve information sharing and collaboration through AI? This dimension reflects the institutionalization level of AI applications.</p>\n</li>\n<li><p>Technical and Tools:<br>This measures the completeness of enterprise AI infrastructure and tool chains. Such as whether intelligent code completion tools, automated testing solutions, and AI analysis tools embedded in continuous delivery pipelines have been deployed? Has the organization built its own large language model application platform or used mature third-party AI platforms (such as Azure OpenAI, GCP AI, AWS AI services)? The technical dimension determines the upper limit of AI capabilities that can be leveraged.</p>\n</li>\n<li><p>Data and Knowledge:<br>This examines whether the organization’s data assets and knowledge management support AI’s efficient operation. For example, has a high-quality Prompt knowledge base&#x2F;knowledge graph been constructed for AI retrieval? Are code repositories and documentation digitized and structured to facilitate AI semantic search and understanding? Are there mechanisms to feed new knowledge generated during projects back to AI model training (continuous learning)? The data dimension is the source of AI “intelligence,” and mature data governance strategies are prerequisites for advanced AI applications.</p>\n</li>\n<li><p>Governance and Security:<br>This reviews risk control and governance measures for AI applications. This includes whether AI output review standards and error correction processes have been established, whether there are data privacy and security policies to ensure AI usage, whether there are clear AI ethics and compliance guidelines, and whether there are emergency response mechanisms when AI decisions fail. The governance dimension ensures AI operates reliably within controllable boundaries.</p>\n<p>For each dimension, we have transformed the typical characteristics of L0-L5 levels into graded judgment criteria. For example, in the “People” dimension: L0 level might correspond to “team members do not use AI tools or only have individual attempts,” L3 level might correspond to “all R&amp;D personnel use AI tools daily and have received training, quickly learning and mastering new AI tools when they emerge,” while L5 corresponds to “the organization has established new AI collaboration roles, employees primarily engage in supervision and innovation work, with routine development undertaken by AI.” By comparing against these standards, managers can determine approximately which level each dimension has reached.</p>\n</li>\n</ul>\n</li>\n<li><p><strong>Rating and Self-Evaluation Process</strong>: It is recommended to adopt survey questionnaires or scorecards for self-evaluation. For each checkpoint mentioned above, teams can assign scores (for example, 1-5 points corresponding from beginner to excellent level). Then compare each dimension’s score with the level standards to determine the maturity level of that dimension. It should be noted that not all dimensions will uniformly reach the same L-level—for instance, technical tools may already be quite advanced (approaching L3), while governance mechanisms may still remain at L1 level. The self-evaluation tool allows separate assessment of each dimension, thereby identifying weak points.</p>\n</li>\n<li><p><strong>Visualization of Assessment Dimensions</strong>: To intuitively present evaluation results, we recommend using multi-dimensional visualization methods such as radar charts (spider charts) to plot the maturity levels of the five dimensions—personnel, processes, technology, data, and governance—on the same chart. This way, teams can clearly see their strengths and weaknesses in various aspects at a glance. For example, Figure 3 illustrates a team’s scoring profile across different dimensions, where the blue area represents the current level and the red dashed line represents the target level. Through this chart, one can intuitively understand which areas the team needs to focus on improving. Another useful visualization is a heat matrix, with levels as the horizontal axis and the five major dimensions as the vertical axis, highlighting the current level to help teams clarify how far they are from the next level in each aspect. Using these visualization assessment dimensions can make the abstract concept of maturity concrete and assist in internal communication and decision-making.</p>\n</li>\n</ul>\n<p><img loading=\"lazy\" src=\"/../images/Comparative_analysis_of_AIFSD_maturity.png\" alt=\"Comparative_analysis_of_AIFSD_maturity.png\"></p>\n<p><em>Figure 3: Example of Team AI Maturity Self-Evaluation Radar Chart. The blue area represents the team’s current scores in each dimension, and the red outline represents the expected target level. This chart helps identify weak points, as the example team lags behind in the “Data &amp; Knowledge” and “Governance &amp; Security” dimensions compared to other dimensions, requiring priority improvement.</em></p>\n<ul>\n<li><strong>Interpretation of Self-Evaluation Results</strong>: Through the above tools, teams can obtain their “positioning profile” under the L0-L5 model. It is worth emphasizing that the purpose of self-evaluation is to identify improvement directions, not to pursue the highest level. Not all teams must aim for L5; the most suitable maturity level should be determined based on organizational strategy and return on investment. Self-evaluation results should help teams answer: In which aspects do we already have a good foundation? Which aspects have obvious shortcomings that limit further AI application? Based on these insights, managers can plan improvement initiatives more strategically. For example, if technical tools and data foundations are in place but personnel skills are insufficient, training and cultural development should be strengthened; if personnel and process readiness are good but appropriate AI tools are lacking, technology introduction should be considered. Self-evaluation results can also serve as a baseline for measuring progress: regularly repeat assessments and observe score improvements in each dimension to track the effectiveness of AI maturity development.</li>\n</ul>\n<h2 id=\"Evolution-Path-and-Key-Success-Factors\"><a href=\"#Evolution-Path-and-Key-Success-Factors\" class=\"headerlink\" title=\"Evolution Path and Key Success Factors\"></a>Evolution Path and Key Success Factors</h2><p>After clarifying the current maturity level and gaps, organizations need to develop a path for evolving from their existing level to higher AI maturity. Teams starting from different points have varying focuses during their advancement process, but generally speaking, each level improvement involves elements such as technology introduction, process transformation, personnel development, and governance enhancement. The following provides evolution path recommendations by level to help managers understand the measures and key success factors required for upgrades:</p>\n<h3 id=\"From-L0-to-L1-Initial-Introduction-of-AI-Assistance\"><a href=\"#From-L0-to-L1-Initial-Introduction-of-AI-Assistance\" class=\"headerlink\" title=\"From L0 to L1: Initial Introduction of AI Assistance\"></a>From L0 to L1: Initial Introduction of AI Assistance</h3><p><strong>Main Challenges</strong>: The team has no AI usage experience and may have a wait-and-see or resistant mindset; insufficient infrastructure and data preparation.</p>\n<p><strong>Evolution Measures</strong>:</p>\n<ul>\n<li>Pilot and Training:<br>Select an area with obvious pain points (such as coding or testing) for AI tool pilots, such as deploying code auto-completion or automated test case generation tools. Provide training to help engineers master usage methods and share pilot benefits to build confidence.</li>\n<li>Basic Environment Preparation:<br>Ensure the development environment allows AI tools to run, such as upgrading IDEs and configuring necessary plugins. Prepare sample projects and data so AI can produce useful results (for example, providing partial codebase context for code generation AI).</li>\n<li>Clear Application Scenarios:<br>Define specific scenarios and boundaries for AI intervention, such as requiring engineers to attempt using AI to generate partial code when developing new modules, but not mandating AI use in critical safety modules (depending on risk assessment).</li>\n</ul>\n<p><strong>Change Factors</strong>: Management needs to create an atmosphere that supports innovation and encourages teams to try new tools; tolerate potential inefficiencies or errors that may occur initially, and maintain a positive attitude toward improvement. Establish feedback mechanisms to collect user opinions and continuously optimize AI tool configuration and usage strategies.</p>\n<p><strong>Key Success Factors</strong>: Top-down leadership support is crucial—managers should personally participate in or pay attention to pilots, providing resource allocation and positive publicity. Selecting appropriate pilot projects is also critical, preferably tasks with tight timelines or insufficient manpower, allowing AI advantages to be fully demonstrated. Use early success cases to prove AI value, eliminate skepticism, and pave the way for comprehensive promotion.</p>\n<h3 id=\"From-L1-to-L2-Expanding-AI-Applications-and-Team-Collaboration\"><a href=\"#From-L1-to-L2-Expanding-AI-Applications-and-Team-Collaboration\" class=\"headerlink\" title=\"From L1 to L2: Expanding AI Applications and Team Collaboration\"></a>From L1 to L2: Expanding AI Applications and Team Collaboration</h3><p><strong>Main Challenges</strong>: AI applications transition from individual use to team-wide adoption, requiring overcoming inconsistent usage among different members, with data and processes becoming bottlenecks.</p>\n<p><strong>Evolution Measures</strong>:</p>\n<ul>\n<li>Establishing Team Standards:<br>Develop best practices and specification documents for AI usage, such as unified Prompt writing styles, checking AI-generated code during code reviews, and identifying AI contributions in version management. Encourage members to share their AI usage experiences and consolidate them into team knowledge.</li>\n<li>Introducing Team-Level Tools:<br>Deploy collaborative AI platforms, such as enterprise ChatGPT or locally deployed open-source large models, to facilitate team context sharing. Integrate AI into project management and CI pipelines, for example, automatically sending user stories to AI for task list generation, and having AI bots participate in Merge Request reviews.</li>\n<li>Expanding Application Scope:<br>While maintaining coding assistance, attempt to apply AI in more areas: using AI to record key points in real-time during requirements analysis meetings and organize requirement documents; introducing AI in testing phases to generate more test scenarios based on specifications; having AI analyze logs to identify fault causes in operations. Gradually achieve AI coverage across the entire process, not just development.</li>\n<li>Data Preparation and Integration:<br>Begin building team knowledge bases, digitally storing historical requirements, designs, code, test results, and other materials as sources for AI to obtain background knowledge. Collect AI output data (such as AI-generated code and problem-fixing suggestions) to provide materials for future training or rule improvements.</li>\n</ul>\n<p><strong>Change Factors</strong>:<br>Process changes are needed to adapt to AI team collaboration, such as adjusting Scrum processes to allocate time and steps for AI-assisted segments in each Sprint planning. Role adjustments gradually emerge, possibly designating “AI Collaboration Leaders” to supervise AI output and quality. Tool integration is a technical focus, requiring time to connect AI platforms with existing development toolchains.</p>\n<p><strong>Key Success Factors</strong>:<br>Ensure team buy-in, meaning most members genuinely adopt AI tools rather than paying lip service—this can be achieved by selecting AI advocates as role models, continuous training, and positive incentives. Establishing rapid feedback loops is also important: when AI suggestions prove ineffective or even erroneous, promptly adjust usage strategies or tool parameters to prevent the team from losing trust in AI. Managers should focus on efficiency and quality metrics, using quantitative data to demonstrate the value of L2-stage team collaborative AI (such as improved code output speed, reduced defect rates, etc.) to consolidate momentum for advancement.</p>\n<h3 id=\"From-L2-to-L3-Deepening-AI-Empowerment-and-Autonomy\"><a href=\"#From-L2-to-L3-Deepening-AI-Empowerment-and-Autonomy\" class=\"headerlink\" title=\"From L2 to L3: Deepening AI Empowerment and Autonomy:\"></a>From L2 to L3: Deepening AI Empowerment and Autonomy:</h3><p><strong>Main Challenges</strong>: Further improving AI’s dominant role requires more powerful models, more comprehensive data support, and more mature governance. Teams need to adapt to the transition from “human-AI collaboration” to “AI-led, largely automated” working methods.</p>\n<p><strong>Evolution Measures</strong>:</p>\n<ul>\n<li>AI Capability Upgrade:<br>Introduce or train more advanced large models and specialized AI components to meet complex project requirements. For example, introduce models capable of architectural design and complex reasoning, or train proprietary models to familiarize them with domain-specific architectural patterns and business rules. Technically, this may require investment in GPU computing resources or the introduction of external AI services.</li>\n<li>Full-Process Automation Transformation:<br>Review existing software delivery processes and replace or enhance automatable parts with AI services. For instance, implement “documentation as code”: enable bidirectional synchronization between requirement&#x2F;design documents and code implementation, where AI updates code based on documents or vice versa. Another example is expanding the scope of AI automated analysis in continuous integration, performing intelligent quality checks and risk predictions for each build. The goal is to minimize manual operations in routine processes and free human resources from repetitive activities.</li>\n<li>Knowledge Platform Construction:<br>Build a unified AI knowledge platform that integrates various types of knowledge from coding, design, testing, and operations. Establish bidirectional tracking of code and documentation, and traceability from requirements to implementation, enabling AI to easily access comprehensive knowledge to support decision-making. This may require developing knowledge graphs, vector databases, etc., to structure enterprise knowledge assets. In the L3 stage, without a solid data and knowledge foundation, AI cannot truly understand complex systems.</li>\n<li>AI Governance System:<br>Establish more comprehensive AI governance strategies, including AI output quality verification processes, AI decision-making authority allocation, and human takeover regulations for exceptional situations. Particularly when AI begins to involve architecture and major decisions, it’s necessary to clarify which areas AI can decide autonomously and which must be reviewed and approved by humans. Establish AI performance indicators (such as the proportion of AI-generated code passing tests, the number of vulnerabilities detected by AI, etc.) to continuously evaluate AI performance and promptly correct deviations when discovered.</li>\n</ul>\n<p><strong>Change Factors</strong>: Organizational structure adjustments may occur at this stage. For example, establishing a dedicated “AI Platform Team” responsible for model and knowledge platform construction and maintenance; equipping each product team with AI domain experts to assist business teams in efficiently using AI. Process-wise, there’s a trend toward integration: the boundaries between development and testing may gradually blur, as AI can simultaneously generate code and tests. Teams shift toward organizing by function or product rather than traditional functional divisions.</p>\n<p><strong>Key Success Factors</strong>: High-quality data and knowledge are the foundation of L3 evolution; without them, AI intelligence is like building a tower on sand. Practitioners and managers must ensure sufficient resources are invested in organizing and maintaining knowledge bases, providing AI with material to work with. Additionally, gradual transition is important: rather than having AI take over complex projects all at once, start with subsystems or independent modules for experimentation. When AI operates reliably in small scopes, then expand the success. Accumulating successful cases will help teams build trust in AI’s deep participation. Finally, proper governance is key to success or failure: neither complete laissez-faire approach that leads to uncontrolled risks, nor overly strict management that renders AI ineffective. A balance between safety and efficiency must be found. Establishing cross-departmental AI governance committees and regularly reviewing AI project effectiveness can provide safeguards for high-autonomy exploration.</p>\n<h3 id=\"From-L3-to-L4-Empowering-AI-Autonomy-and-Innovation\"><a href=\"#From-L3-to-L4-Empowering-AI-Autonomy-and-Innovation\" class=\"headerlink\" title=\"From L3 to L4: Empowering AI Autonomy and Innovation\"></a>From L3 to L4: Empowering AI Autonomy and Innovation</h3><p><strong>Main Challenges</strong>: Transforming AI from an execution tool into a proactive innovation entity requires significant conceptual shifts and technological leaps. How to trust AI to make correct decisions, stimulate AI creativity, and integrate it into business innovation processes presents new challenges for practitioners and managers.</p>\n<p><strong>Evolution Measures</strong>:</p>\n<ul>\n<li>Deploy Autonomous Agents:<br>Introduce autonomous AI agent frameworks to enable AI with independent decision-making and continuous action capabilities. For example, use open-source frameworks like Google ADK and langgraph to develop customized intelligent agents, granting AI the ability to execute task chains without human intervention. Start with low-risk domains for testing, such as having AI agents handle regular performance optimization: they can proactively identify bottlenecks, attempt optimization solutions, and test effectiveness. Gradually expand to more critical areas.</li>\n<li>human-AI Collaborative Innovation Processes:<br>Reshape innovation processes by integrating AI into the early stages of product ideation and development. For instance, establish “AI+Human” joint brainstorming mechanisms: let AI analyze user feedback data to propose new feature suggestions, while humans discuss and evaluate feasibility with AI. For viable ideas, have AI generate prototypes or technical solutions, then let teams decide whether to implement. This approach treats AI as a product manager&#x2F;consultant, leveraging its broad search and pattern recognition advantages to provide inspiration for humans.</li>\n<li>Decision-Making Authority Gradient:<br>Gradually increase AI decision-making authority. Initially, grant AI “advisory rights”: AI can proactively initiate certain routine decisions (such as task allocation, defect fixes) but require human confirmation. As AI reliability improves, expand its “execution rights” scope: for example, let AI automatically fix and deploy similar recurring defects without requiring approval each time. Eventually, within clearly defined boundaries, grant AI complete autonomy (such as AI independently executing low-impact operational adjustments), with humans primarily focusing on high-level strategy and exception handling. This process requires dynamic adjustment in practice to ensure AI has room to perform while staying within bounds.</li>\n<li>Risk Control and Monitoring:<br>For risks that AI autonomous actions might trigger, establish comprehensive monitoring and rollback mechanisms. For example, when introducing AI autonomy in critical systems, set up “sandbox environments” or dual-track systems—AI actions are first executed and validated in shadow systems before applying to real systems. Configure anomaly alerts to promptly notify humans for intervention once AI behavior shows abnormalities. Every problem caused by AI autonomous decisions should be recorded and analyzed to improve AI risk control rules.</li>\n</ul>\n<p><strong>Change Factors</strong>:<br>Culture and trust become decisive factors at this stage. Organizations must cultivate a culture that trusts AI while being prepared to correct errors: employees trust that AI can handle many tasks well while remaining vigilant and tolerant of potential AI mistakes. Management must encourage experimentation through words and actions, ensuring employees believe that using AI autonomous systems won’t result in punishment for occasional errors, but will be treated as learning and improvement opportunities. Organizational structures may further evolve, such as establishing “AI Innovation Labs” specifically to incubate new product concepts proposed by AI and collaborate with business departments for implementation.</p>\n<p><strong>Key Success Factors</strong>:<br>Taking small steps and conducting closed testing is an effective method to reduce risks while promoting innovation. Allowing AI to explore creativity in controlled environments and then expanding to production after success is a prudent path. Talent composition is also crucial: this stage requires hybrid talent who understand both business and AI to serve as bridges, capable of understanding AI-generated ideas while evaluating their commercial value. Top-level support remains important—transformative solutions proposed by AI may sometimes exceed conventional expectations, requiring management to embrace change. Finally, adjust incentive mechanisms to accommodate new human-AI roles: for example, when AI takes on more foundational work, how to motivate employees to focus on higher-value tasks and how to evaluate AI work effectiveness both require new assessment and incentive methods to ensure AI and employees collaborate to create maximum value rather than conflict with each other.</p>\n<h3 id=\"From-L4-to-L5-Building-an-AI-Native-Delivery-Ecosystem\"><a href=\"#From-L4-to-L5-Building-an-AI-Native-Delivery-Ecosystem\" class=\"headerlink\" title=\"From L4 to L5: Building an AI-Native Delivery Ecosystem\"></a>From L4 to L5: Building an AI-Native Delivery Ecosystem</h3><p><strong>Main Challenges</strong>: Evolution to L5 means entering uncharted territory, requiring systematic reconstruction in technical systems, organizational models, and business strategies. The investment is enormous, the difficulty extremely high, and there are few industry precedents to follow.</p>\n<p><strong>Evolution Initiatives</strong>:</p>\n<ul>\n<li>Building Core AI Platforms:<br>Enterprises need to independently construct highly customized AI platforms and toolchains, fully integrating development, testing, and operations functions. For example, developing their own large language models and continuously training them to fully understand the enterprise’s business domain and coding standards; building a unified AI programming hub that connects IDEs, version management, deployment pipelines, and monitoring systems to achieve AI control over the entire lifecycle. This typically requires assembling top-tier AI research and engineering talent, potentially collaborating with universities and research institutions for breakthrough innovations.</li>\n<li>Data and Simulation-Driven Approach:<br>The L5 ecosystem requires robust data flow and simulation support. Building comprehensive data collection and feedback mechanisms where massive data generated during software operation (user behavior, performance metrics, failure scenarios) automatically becomes fuel for training AI models, continuously improving their capabilities. Introducing advanced simulation environments allows AI to test new designs and optimization strategies in virtual spaces, reducing the risk of errors in real environments. This can draw from autonomous driving approaches, accelerating AI maturity through simulation training.</li>\n<li>Comprehensive Organizational Transformation:<br>Company architecture transforms toward “AI-native” structure. For example, traditional IT departments evolve into “AI capability centers,” business departments are also equipped with AI experts, and AI analysis reports become standard inputs in decision-making processes. New CXO roles such as CAIO (Chief AI Officer) may emerge to coordinate the AI ecosystem. Business processes are reshaped to fully leverage AI automation and intelligence advantages, such as directly connecting sales and customer service with development platform data, enabling AI to capture market demands in real-time and drive development iterations.</li>\n<li>Value Chain Reconstruction:<br>Considering business model changes under L5 capabilities and positioning for the future. For instance, when software delivery speed and efficiency improve by an order of magnitude, should companies adopt on-demand customization and ultra-fast iteration product strategies? AI-native ecosystems may give birth to entirely new businesses (such as opening internal AI development capabilities as services). Leadership should consider how to transform AI advantages into market leadership. This requires deep integration of technology strategy with enterprise strategy.</li>\n</ul>\n<p><strong>Change Elements</strong>: Strategic determination and long-term investment are necessary conditions for L5 evolution. Since L5 implementation may take a long time with uncertain returns, management needs vision and patience, continuously investing funds and resources. Company-wide repositioning is also a massive challenge: as AI takes over most work, employee roles need complete transformation, and corporate culture needs reshaping (from “how people do well” to “how people enable AI to do well”). This involves extensive training, psychological preparation, and organizational change management. External ecosystem coordination cannot be ignored: when enterprises achieve high AI autonomy internally, they still need to manage relationships with customers and regulatory agencies—ensuring that AI-generated software and decisions are accepted and trusted by external stakeholders. This may require establishing and promoting industry standards.</p>\n<p><strong>Key Success Factors</strong>: Technical breakthroughs and innovation are the primary factors; without excellent AI technical capabilities, L5 cannot be achieved. Enterprises should attract top AI talent, encourage internal innovation, and actively file patents to consolidate leading advantages through practical experience. Risk management remains important: while pursuing full autonomy, mechanisms must be in place to prevent catastrophic risks from AI system failures or major errors (such as establishing AI ethics review committees and testing AI responses in extreme scenario simulations). Setting progressive milestones helps teams maintain motivation on the long journey—breaking down the L5 vision into achievable phased goals, implementing step by step, such as first achieving “unattended nighttime build and release,” then “unattended minor version updates.” Each achievement should be celebrated and publicized to consolidate confidence and morale. Finally, a pragmatic and flexible attitude is essential: while L5 is the ultimate goal, managers should always assess real benefits and maintain balance between investment and returns, not blindly pursuing impressive full automation while ignoring actual business value. Successful L5 should be a natural, opportunistic result rather than a castle in the air divorced from business logic.</p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>Artificial intelligence is accelerating the transformation of software delivery methods, evolving from small coding assistance tools all the way to the “super brain” vision of full-process automation. The L0-L5 maturity model proposed in this article depicts a gradual evolution roadmap for enterprises: from the traditional “human-led, standards-driven” model, evolving to “human-AI collaborative co-creation,” and ultimately envisioning a new paradigm of “AI-led” software engineering. Through in-depth elaboration of each level and case analysis, we can see that every level advancement represents a coordinated leap in technical capabilities, process mechanisms, and personnel skills. Enterprises should combine their current situation, use maturity self-assessment tools to identify their position, clarify gaps, and steadily advance toward higher levels of AI empowerment through phased strategies. It should be emphasized that maturity building is a long-term organizational capability development that cannot be achieved overnight and should not involve blind competition. The correct approach is to be business value-oriented and achieve a balance between improving efficiency and controlling risks. Management’s vision, perseverance for change, and the collective efforts of all personnel will determine the success or failure of this transformation. Looking to the future, current exploration and efforts will lay the foundation for enterprises’ competitive advantages in the “AI + software delivery” era. We hope that the model and methodology provided in this article can offer valuable reference for enterprise decision-makers, helping everyone seize opportunities in AI-driven software engineering transformation and unleash greater innovation potential and business value.</p>\n",
            "tags": [
                "AI",
                "Prompts",
                "Governance"
            ]
        },
        {
            "id": "https://gszhangwei.github.io/2025/06/03/AI-assisted-development-of-new-qualitative-productivity-methodology/",
            "url": "https://gszhangwei.github.io/2025/06/03/AI-assisted-development-of-new-qualitative-productivity-methodology/",
            "title": "AI-enabled software delivery maturity model interpretation and AI-assisted development of new qualitative productivity methodology",
            "date_published": "2025-06-03T11:00:00.000Z",
            "content_html": "<h2 id=\"引言\"><a class=\"anchor\" href=\"#引言\">#</a> 引言</h2>\n<p>近年来，生成式AI在软件开发领域掀起了巨大浪潮。越来越多开发者开始尝试利用大模型辅助编码和设计，从代码自动补全到对话式问答，AI正在逐步融入软件交付过程。然而，不同团队对AI的应用深度千差万别：有的还停留在零散尝试阶段，有的已将AI视为“编程伙伴”。为了帮助企业和团队评估自身AI赋能程度，并规划升级路径，我们提出了“AI辅助软件交付成熟度模型”，用L0到L5六个级别来描述AI在软件交付中的参与度和成熟度（类似于自动驾驶领域的L0–L5分级，从完全人工到完全自主）。本文将详细阐述这一模型各级别的定义、演进路线，以及关键转折点和评估维度，并探讨与之配套的方法论框架和实践策略。</p>\n<p>值得注意的是，目前业界整体仍处于初级阶段。据调研，即使在科技前沿的硅谷，公司员工中从未使用过任何AI编码助手（Coding Agent）的人仍超过90%。但领先企业已经展示了AI赋能开发的巨大潜力：例如在Meta和Google，约有30%的代码由AI生成，相当于每3行代码就有1行出自AI。这意味着一场静悄悄的效率革命正在发生。为了不在未来的竞争中掉队，软件团队需要尽早认识自身所处的成熟度级别，理解升级路径，并制定相应的AI融合战略。</p>\n<p>接下来，我们将围绕“AI辅助软件交付成熟度模型”的六个级别（L0–L5），逐层解析各级特征与演进路径，探讨每一级跃迁的关键里程碑，以及如何从人机分工、AI能力、决策权和效率四个维度评估团队所处阶段。在此基础上，我们还将介绍**结构化提示驱动开发（Prompt-Driven Development, PDD）**的演进、完整的AI增强开发方法论框架，以及其中蕴含的人机协作公式与正向认知循环机制。希望这篇长文能为软件开发管理者、企业决策者和技术爱好者提供有价值的思想启迪和实践指南。</p>\n<h2 id=\"快速回顾ai-辅助软件交付-l0-l5-六级阶梯\"><a class=\"anchor\" href=\"#快速回顾ai-辅助软件交付-l0-l5-六级阶梯\">#</a> 快速回顾：AI-辅助软件交付 L0-L5 六级阶梯</h2>\n<p>对 L0–L5 的详细定义与典型特征，我已在上一篇公众号文章《AI辅助软件交付全流程成熟度模型白皮书》中给出逐级拆解，这里不再逐条展开，只用一张「电梯速览」帮大家把记忆拉回：</p>\n<table>\n<thead>\n<tr>\n<th>级别</th>\n<th>人-机角色</th>\n<th>AI 能力外延</th>\n<th>交付范式</th>\n<th>升级拐点</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>L0</strong></td>\n<td>人工独奏</td>\n<td>无 AI 参与</td>\n<td>传统瀑布 / Scrum</td>\n<td>—</td>\n</tr>\n<tr>\n<td><strong>L1</strong></td>\n<td>人主导、AI 辅助</td>\n<td>代码片段/文档生成</td>\n<td>“AI-增强键盘”</td>\n<td>引入 IDE 助手</td>\n</tr>\n<tr>\n<td><strong>L2</strong></td>\n<td>团队协作</td>\n<td>流程多点嵌入</td>\n<td>Prompt-协作</td>\n<td>建立团队 Prompt 库</td>\n</tr>\n<tr>\n<td><strong>L3</strong></td>\n<td>人监控、AI 共创</td>\n<td>子系统级交付</td>\n<td>AI-First Dev</td>\n<td>PDD 工业化</td>\n</tr>\n<tr>\n<td><strong>L4</strong></td>\n<td>人把关、AI 主导</td>\n<td>端到端交付</td>\n<td>AI-Led Dev</td>\n<td>赋权自动部署</td>\n</tr>\n<tr>\n<td><strong>L5</strong></td>\n<td>人监督、AI 自治</td>\n<td>自主演化</td>\n<td>自然语言编程</td>\n<td>全链路自治</td>\n</tr>\n</tbody>\n</table>\n<p>想深入了解每一级的能力边界、里程碑和风险点，可点击<a href=\"https://mp.weixin.qq.com/s/M8RzmW-T07-zncPBx8sYHg\">《AI辅助软件交付全流程成熟度模型白皮书》</a>回顾。<br />\n接下来，本文将把聚光灯放在模型深入理解以及方法论沉淀上面。</p>\n<h2 id=\"每一级别的关键转折点与升级里程碑\"><a class=\"anchor\" href=\"#每一级别的关键转折点与升级里程碑\">#</a> 每一级别的关键转折点与升级里程碑</h2>\n<p>软件团队在AI辅助成熟度上升级往往并非线性渐进，而是在达到某些<strong>里程碑事件</strong>后实现跃迁。下面我们分级概括各层级从当前状态跃升到下一状态的关键转折点：</p>\n<ul>\n<li><strong>L0 → L1：启用AI助手</strong>。里程碑事件是团队<strong>开始引入AI工具</strong>用于开发辅助。例如，有开发者率先尝试了GPT助手来生成代码片段，或团队购入了代码补全AI的订阅。管理层可能尚未正式推动，但基层工程师的自发采用意味着团队踏出了从完全人工到借助AI的第一步。这一转折体现了观念的转变——开始接受“机器也能写代码”，哪怕只是很小部分。</li>\n<li><strong>L1 → L2：团队规范应用AI</strong>。关键转折是<strong>从个人尝试上升为团队实践</strong>。管理层意识到AI带来的效率提升，决定在整个团队推广。里程碑表现为<strong>制定规范和培训</strong>：团队建立AI使用指南，分享最佳提示词套路；在代码审查流程中正式加入AI建议；可能由团队架构师牵头，引入企业版AI开发平台等。这一阶段的标志是AI不再仅仅是“有就用，用了也不错”，而是成为团队开发流程的有机组成部分。</li>\n<li><strong>L2 → L3：深度集成与部分自治</strong>。里程碑事件包括：<strong>AI开始直接介入核心开发环节</strong>，完成过去需要人工多步完成的任务。例如团队部署了自己的Chatbot接入内部知识库，能够回答开发中的业务逻辑问题；又或者实现了CI流水线中自动生成测试和修复简单缺陷的AI Agent。如果某一天团队允许AI根据提炼的需求说明生成完整的模块代码，并在通过自动测试后直接并入主干分支（当然仍有人整体review），那就是重大转折点。这意味着AI第一次在流程中<strong>闭环完成</strong>从任务输入到输出的过程，人只是验收，表明团队迈入深度协同阶段。</li>\n<li><strong>L3 → L4：AI主导权的授予</strong>。从协同走向主导，组织需要跨越信任和治理上的鸿沟。一个里程碑是<strong>赋予AI更高决策权限</strong>：例如，团队设定某类变更（文档格式、样式修正、低风险性能改进）AI可以自行生成并自动部署，无需人工审批；又或是在事故响应中，AI监测到已知类型故障可以自主执行预案恢复。这样的授权标志着AI从工具变成“准同事”，被赋予了<strong>行动自主权</strong>。另一个标志事件是<strong>开发流程的重塑</strong>：团队也许会重构系统架构以方便AI操作（如把GUI配置改成代码配置，让AI修改配置更方便），或者建立统一的API层供AI操控各种开发工具。凡此种种，都是在为AI全面主导铺路。当AI能够<strong>自主驱动大部分开发活动</strong>而不需要人时时干预时，就意味着成熟度达到L4。</li>\n<li><strong>L4 → L5：AI全面自治与自我进化</strong>。迈向最终阶段的里程碑非常高阶，例如：<strong>AI可以根据用户或市场反馈自主提出新特性</strong>，并完成实现上线——相当于AI具备了产品经理和工程师双重角色的能力；又或者，企业上线了一个<strong>完全由AI组成的开发运营团队</strong>来负责某条产品线，人类只观察指标和结果。当AI系统开始表现出<strong>自我优化</strong>行为（比如自动优化自己的提示策略、模型不断在新数据上训练提高），并能处理未知的新需求或新环境时，可以说已触及L5的门槛。此外，可能的标志是<strong>行业监管或标准认可</strong>：类似自动驾驶需要法规许可，AI完全自主开发或部署某些关键系统可能也需要外部认可，当有案例证明AI可以安全地全权负责软件系统时，才是真正的L5时代。总的来说，从L4到L5更多是质变，代表着开发范式彻底转向AI自治，人类退居指导和监督的位置。</li>\n</ul>\n<p>以上这些里程碑为团队指明了升级方向。每个转折点的实现，都预示着团队在技术、流程和信任上达到了新的高度。例如，从L2到L3的转折体现了对AI<strong>信任度</strong>的提升和<strong>技术集成</strong>的深化，而L3到L4的转折更考验<strong>治理机制</strong>和<strong>基础设施</strong>是否为AI赋权做好了准备。组织在规划AI路线图时，可以参考这些关键节点，评估自身差距，逐步实现突破。</p>\n<h2 id=\"通过四个维度评估自身所处等级看能力结果\"><a class=\"anchor\" href=\"#通过四个维度评估自身所处等级看能力结果\">#</a> 通过四个维度评估自身所处等级，看能力结果</h2>\n<p>理解了模型级别和演进路径后，一个实际问题是：<strong>我们的团队/组织目前处于哪个级别？</strong> 为了自我评估，可以从以下四个核心维度考察，这四个维度对应了AI融合程度的不同方面：</p>\n<ol>\n<li><strong>人机分工比例</strong> – 考察当前<strong>工作任务在人工和AI之间的分配</strong>情况。问问自己：日常开发中，有多少比例的工作量由AI承担？如果几乎所有编码、测试、文档等都由人力完成，AI只是偶尔辅助一下，你可能在L1以下。若是AI已经承担了显著部分如代码生成、测试编写，而人更多在做验证和复杂设计，那么至少在L3。简单来说，人机分工从“人100%：机0%”（L0）逐步演进到“人少量指导：机绝大部分执行”（L5）。你所处的位置取决于AI目前在团队里是否只是<strong>锦上添花</strong>，抑或已经成为<strong>主力劳动力</strong>。</li>\n<li><strong>AI能力范围</strong> – 评估AI在团队中被用来完成<strong>哪些类型的任务</strong>，覆盖范围有多广。是仅用于代码自动补全这一种场景，还是贯穿需求分析、编码、测试、部署多个环节？能力范围越广，成熟度越高。举例来说，如果AI只会帮忙生成函数代码或回答编程问题，这是初级应用（L1-L2）。如果AI还能基于设计文档生成模块、自动编写测试用例、参与运维监控，那显然已经迈入高级阶段（L3+）。另外还要看AI处理问题的<strong>复杂度</strong>：是只能处理单一步骤的简单任务，还是能够解决跨模块的综合问题？范围越广、能力越通用，等级越高。</li>\n<li><strong>决策权转移</strong> – 观察团队<strong>将多少决策权交给了AI</strong>。也就是AI在多大程度上可以不经人批准地采取行动或做出决定。低级别时，AI仅提供建议，所有决定都由人做（典型的如L1阶段Copilot建议一行代码，你决定要不要用）。中等成熟度时，AI可以自主完成一些决定但在关键点需要人工确认（如L3阶段AI提交代码但需要人Review通过才能合入）。更高级时，AI可以<strong>自主决策并执行</strong>大部分操作，人只是例行检查或异常介入（L4/L5）。例如，你是否允许AI自动修复一个安全漏洞并部署补丁？CI失败时，AI能否自主重试构建或回滚版本？这些都是决策权移交的体现。衡量标准可以用一个谱系来看：从“AI无决策权”到“AI决策后人必审”再到“AI决策后人抽检”直至“AI全权决策，人类事后分析”。看看你的团队处于哪种状态，就大致判断了成熟度。</li>\n<li><strong>效率提升幅度</strong> – 最终也是最直观的指标：<strong>生产力相比无AI时代提升了多少</strong>？可以通过一些客观数据或主观感受来判断。比如，开发一个新功能从需求到上线用了多久？比过去快了几倍？软件发布的频率是否显著提高？缺陷率是否下降？团队成员感觉工作量是否减轻、产出是否变多？一般而言，L1阶段可能只是局部提升，例如某些编码任务加快了，但整体项目周期缩短有限；L2-L3阶段开始有明显加速，一个典型指标是<strong>发布频率提高</strong>（从几周一版变成每周多版），或开发人月成本降低等；L4阶段则会看到<strong>质变</strong>，许多以前需要等待的环节现在同步并行完成，总效率成倍甚至十倍提升；L5理论上效率提升接近极致，因为人力基本不再是瓶颈。举例来说，有报告显示使用AI对开发者的编码任务完成速度最高提升了<strong>55%</strong>，这是个人层面的效率增益。而团队层面若达到L4/L5，这个数字会更高，甚至可以做到以前一周的工作现在一天完成。通过衡量效率指标的变化幅度，也能印证团队大概处于成熟模型的哪个区间。</li>\n</ol>\n<p>通过上述维度评估，可以较全面地了解自身所处的成熟度等级。例如，如果你的团队发现：AI目前只是用于代码自动补全（能力范围窄），只有个别工程师在用（人机分工比例低），AI没有任何自主决定权（决策权无转移），整体开发效率也没有显著变化 —— 那大概率还是在L1初级阶段。反之，如果AI参与了需求分析到部署的大部分环节（能力范围广），承担了主要工作量（分工比例高），能够自动执行许多决策（决策权高转移），以至于交付速度翻了几倍（效率大幅提升），那么恭喜，你们已跻身业界领先的L4乃至L5探索者行列。<br />\n评估之后，团队可以针对自身短板制定升级计划。例如，若发现AI能力范围局限，可考虑扩展AI应用场景（如引入AI做测试、运维）；如果决策权始终无法放心下放，可能需要完善AI结果验证机制以建立信任；效率提升不明显，或许是还停留在零散使用阶段，需要整体流程的革新。总之，这四个维度提供了一个分析框架，帮助团队定位“成熟度有多高”。如果想再升一级，可以根据《AI辅助软件交付全流程成熟度模型白皮书》中的5维度评估框架，做深入问诊，找到导致卡在某一等级的根因，针对 5 维度中的弱项列行动清单（培训、工具建设、数据治理、合规机制等）。</p>\n<h2 id=\"structured-prompt-driven-developmentpdd的演进路径\"><a class=\"anchor\" href=\"#structured-prompt-driven-developmentpdd的演进路径\">#</a> Structured Prompt-Driven Development（PDD）的演进路径</h2>\n<p>在AI赋能开发的背景下，“<strong>结构化提示驱动开发</strong>” (Structured Prompt-Driven Development, PDD) 正迅速崛起为一种全新的开发范式。简而言之，PDD的核心是<strong>开发者主要通过向大语言模型(LLM)提供提示词（prompt），由AI生成所需的代码</strong>。开发者从“亲手写代码”转变为“利用结构化提示词与AI对话来产出代码”，聚焦点变成如何编写有效提示以及审查AI输出。这种范式转移与我们前述的成熟度模型密切相关，实际上PDD的推广和深化也是团队从低级迈向高级的重要推动力。下面我们结合实践，将PDD应用的<strong>演进路径</strong>分为三个阶段：</p>\n<ul>\n<li><strong>个人探索阶段：</strong> 在这一初始阶段，PDD主要存在于<strong>个人层面的尝试</strong>。一些走在前沿的开发者开始在日常工作中引入提示词工程的思想。例如，他们接到一个需求时，不再直接开始编码，而是先尝试给ChatGPT下达一系列指令，让其生成代码草稿；或者在调试时，通过描述问题现象让AI协助定位Bug。这个阶段的特征是<strong>自发性</strong>和<strong>非结构化</strong>：提示词的编写依赖个人摸索，没有统一模板，往往需要多轮试错才能获得满意输出。每个人使用LLM的方法各不相同，缺乏标准流程。但正是在不断试错中，开发者们逐渐掌握如何与AI配合。比如学会了将复杂任务拆解成小的提示一步步交给模型，或者总结出一些有效的提示词范式。个人探索阶段通常对应团队整体在L1-L2成熟度：有人在用AI，但尚未形成团队规范。一个生动案例是有开发者分享自己在<strong>零编码</strong>的情况下，用一系列提示指引AI完成了一个全栈Web应用，从数据库到前端都由AI生成。这类成功经验在团队中传播，会引发更多人尝试PDD，为下一阶段奠定基础。</li>\n<li><strong>团队标准化阶段：</strong> 随着PDD实践的成效开始显现，团队逐步将其提升到<strong>协作层面</strong>。这一阶段的标志是<strong>结构化提示驱动开发</strong>的出现——团队为如何编写和管理提示词制定了方法论。例如，大家可能约定在给模型下指令时遵循某种模板，将角色、目标、约束条件等信息结构化地包含在提示中。常见的一种结构化Prompt设计是像写微型文档一样编写提示词：首先明确角色定位（如“你是一个资深Java后端工程师”），然后列出任务目标，再提供必要的上下文或示例，最后明确输出要求。这种<strong>模板化、分段式</strong>的提示方式被证明能大幅提高AI理解正确率。团队标准化阶段往往会产出<strong>提示词指南或库</strong>：例如，公司内部维护着一套常用Prompt模板，针对不同场景（生成单元测试、代码审计、安全检查等）有不同的标准Prompt，开发者拿来套用即可，不必每次从零想提示。这种知识的积累使PDD变得更加高效、一致。同时，工作流也相应调整：可能在任务管理中增加“Prompt设计”这一步，由有经验的人编写高质量Prompt，再由普通开发者拿去执行AI生成代码。代码评审时也关注Prompt本身的有效性。可以说，这阶段PDD已经深入团队开发流程，成为<strong>团队协作的新默契</strong>。对应成熟度模型，团队如果能做到这一点，起码已经迈入L3，即AI深度协同阶段——因为只有信任AI能完成大部分代码产出时，大家才会花心思去标准化提示流程。团队标准化PDD带来的收益是明显的：AI输出质量更稳定，成功率更高，减少了来回试错；新人也能快速上手AI开发，因为有成熟套路可循。这个阶段也为下一步——让AI来帮助改进Prompt——打下了基础。</li>\n<li><strong>AI生成提示词阶段：</strong> 在PDD的最高阶段，出现了一个耐人寻味的现象：<strong>编写Prompt本身也可以交给AI来做</strong>。也就是说，开发者不再需要手工设计所有提示词，而是可以有更高一级的AI（或称Agent）根据需求<strong>自动生成一系列Prompt</strong>，交由底层的大模型执行，从而完成任务。这种机制可以理解为PDD的<strong>元进化</strong>——让AI学会如何使用AI。当前一些前沿探索已经初具这种雏形，例如AutoGPT、GPT-Engineer等开源项目尝试读取高层需求说明，然后自主拆解任务，生成针对各子任务的提示，让LLM逐一实现，最终拼装出完整结果。再比如，有工具可以根据代码库自动生成针对性的代码改进建议的Prompt，提交给Copilot执行。简而言之，此阶段人类更多是提出愿景或目标，由AI来拟定实现步骤和相应指令序列。人类角色进一步上升到<strong>监督者</strong>：监控AI生成的提示是否合理，必要时微调，然后让AI按这些提示完成代码生成工作。AI生成Prompt阶段代表了PDD的<strong>高度自治</strong>形态，也基本对应于成熟度模型的L4-L5水平——因为当AI能自己想该怎么问另一个AI、自己规划整个开发过程时，也就达到了几乎自主开发的境界。当然，目前这一领域还处于探索早期，自动生成的Prompt质量有待提高，往往还需要人工修正。但可以预见，随着大模型推理能力和规划能力增强，AI代理将越来越善于<strong>理解人类意图并翻译成一系列模型可执行的指令</strong>。一旦这一点成熟，人类只需提出“我要一个具有某某功能的应用”，AI代理便能自动产出所有相应Prompt并驱动模型完成开发，实现真正的端到端自动化。这个愿景正是PDD演进的终点。</li>\n</ul>\n<p>总结来说，Prompt-Driven Development从最初个人的兴趣尝鲜，逐步走向团队的方法论化，再迈向AI之间自洽对话的自治阶段，完美契合了AI辅助交付成熟度模型的升级之路。在团队实践中，引入PDD往往是迈向高成熟度的催化剂：它迫使我们把隐性的开发知识（如何写代码）显性化为对AI的指令，从而促进了<strong>知识资产的沉淀和标准化</strong>；同时又在持续使用中<strong>倒逼AI改进</strong>，形成正反馈循环。例如，有团队报告引入提示驱动开发后，程序员的沟通能力和战略思维变得更加重要，而编码细节交给AI完成——这正是人机协作模式改变的体现。可以预见，在未来的软件开发中，“不会写Prompt就像不会写代码一样致命”。每一家希望提升AI成熟度的组织，都应当审视如何推动PDD从个人实践走向团队标准，并前瞻布局让AI代理来参与甚至主导Prompt生成。唯有如此，才能充分释放大模型时代的开发红利。</p>\n<h2 id=\"完整的ai增强软件开发方法论框架\"><a class=\"anchor\" href=\"#完整的ai增强软件开发方法论框架\">#</a> 完整的AI增强软件开发方法论框架</h2>\n<p>要成功攀升AI交付成熟度，仅有工具和零散技巧还不够，还需要<strong>配套的整体方法论</strong>指导。从工作模式到治理体系，各个方面都需同步升级。基于行业经验，我们构建了一个涵盖六大核心方面的AI增强软件开发方法论框架，以确保团队在引入AI的过程中保持效率、质量与可控性的平衡。下面分别介绍这六个方面：</p>\n<ul>\n<li><strong>工作方式转变：</strong> AI时代的软件开发工作方式正从<strong>个人独立完成</strong>向<strong>人机协同共创</strong>转变。传统开发中，程序员亲自编写每行代码；而在AI增强模式下，程序员更像<strong>指挥</strong>，通过描述意图和要求来让AI“演奏”出代码。这要求开发者具备新的心态与技能，例如如何与AI对话、如何及时反馈引导AI改进。团队应提倡一种新的协作文化：把AI看作对等的“队友”，善于<strong>提出明确的问题</strong>并快速响应AI给出的结果。工作方式的转变还意味着组织流程要适应高频迭代、即时反馈的特点——例如每天都有大量AI生成的产出需要review，Scrum迭代可能变短等。总体而言，这一方面强调<strong>角色定位</strong>的变化：开发者从苦力型编码者升级为策略型引导师，工作重心从“怎么把代码敲出来”变成“怎么把需求讲清楚、结果把关好”。</li>\n<li><strong>能力增强：</strong> AI赋能的核心价值在于<strong>增强人类开发者的能力边界</strong>。具体表现在：开发人员借助AI可以触达以前无法企及的知识和技能。例如，通过AI助手，一个对某技术不熟悉的新人也能快速获取专业解决方案；AI提供的洞见可以启发更优的设计思路，使普通开发者具备类似资深专家的判断力。这方面方法论关注如何<strong>最大化人机合力</strong>的效果。一方面，要培训人掌握AI工具，善用AI提供的知识库和建议；另一方面，也要优化AI，使之更懂团队的领域知识，输出更贴合团队风格的成果。这可能涉及训练自有大模型、构建域内知识图谱供AI调用等。在能力增强框架下，有一个重要理念是**“人机优劣互补”<strong>：人擅长创造性、判断力和道德责任，AI擅长记忆、计算和海量数据分析。团队应设计协作流程，让AI去承担繁重单调却需要高精度的任务，而让人去做需要创新、决策和责任承担的任务。通过明确分工、优势互补，整体能力将大幅提升。例如，让AI自动生成大部分测试用例，人则专注于设计少数棘手的测试场景；AI处理代码格式和样板，人则集中精力处理业务逻辑难题。实践证明，这种分工下的</strong>人机组合**常能取得1+1&gt;2的效果。</li>\n<li><strong>知识资产工程：</strong> 在AI参与开发后，<strong>知识本身成为关键的生产要素</strong>。团队需要有意识地运营和沉淀自身的知识资产，供AI充分利用。这包括代码、设计文档、架构决策记录、故障案例、业务领域规则等等。传统开发中，知识散落在员工大脑和各种文档里，而AI要发挥作用，就必须<strong>获取并内化团队的知识</strong>。知识资产工程的方法论关注两个方面：其一是<strong>知识的结构化和供给</strong>，即如何整理企业内显性和隐性的知识，使之以AI可读取的形式存在。这可能涉及建立内部知识库，搭建文档问答系统，向AI提供API文档、数据库模式、业务术语解释等，让AI随时调阅。其二是<strong>知识从AI处产出并反馈</strong>，即将AI交互过程中生成的新知识归档，比如高质量的Prompt及其输出、AI在问答中给出的解法等，都应纳入知识库循环利用。通过这种知识资产的双向流动，企业能够逐步构建起**“AI+人”共建的知识体系**。另外，还要重视<strong>模型更新和训练</strong>：当业务知识有变化时，及时更新供AI参考的数据；甚至定期用企业自有数据对模型微调，使其逐渐融入组织经验。这一切都是为了确保AI的“智慧”始终掌握公司最正确、最新的知识，使其输出可靠可用。</li>\n<li><strong>工具与平台：</strong> 有好的方法也需要合适的工具来实现。AI增强开发需要一套<strong>新的工具链</strong>支撑。首先是<strong>AI服务平台</strong>，例如接入OpenAI、Baidu文心等大模型服务，或者部署开源模型，本质是要有稳定、高性能的模型调用渠道。其次是<strong>集成开发环境（IDE）的AI插件</strong>，如Visual Studio Code的Copilot扩展，JetBrains系列的AI助手等，提供便捷的人机交互接口。再次，<strong>AI辅助测试、部署工具</strong>也必不可少，例如自动生成测试用例的框架、智能CI/CD流水线工具等。对于更高成熟度团队，可能需要构建<strong>自定义的AI Agent框架</strong>，以编排多个模型和工具协作完成复杂任务。工具方面的方法论强调选型和集成：选取适合团队技术栈和安全要求的AI方案（比如代码必须私有，可能要用本地部署的大模型）；将AI能力嵌入现有DevOps流水线，实现<strong>端到端打通</strong>。还需考虑<strong>扩展性和性能</strong>，确保工具能够应对随着AI介入而<strong>指数增长</strong>的自动化任务量。在具体执行层面，一个有效做法是为团队打造<strong>AI助手中枢</strong>：也就是一个统一的平台界面，让开发者能在其中与各种AI工具交互，比如一个聊天窗口同时连通代码生成AI、文档问答AI、运维助手AI等，避免分散使用带来的混乱。总结来说，这一方面关注“工欲善其事，必先利其器”，通过搭建<strong>完善的AI赋能工具链</strong>来保障其它方面目标的实现。</li>\n<li><strong>工程治理：</strong> 引入AI为开发提速的同时，也带来了新的<strong>治理挑战</strong>。比如AI生成的代码质量如何保证？安全与合规如何监管？出现问题责任如何划分？方法论框架中“工程治理”部分旨在建立一系列政策、规范和机制，确保AI参与下的软件工程依然<strong>可控、可靠和可信</strong>。首先是<strong>质量治理</strong>：团队应制定AI生成产出的审核流程，例如任何AI生成的代码必须经过代码审查和测试验证；对于AI提出的设计方案，架构师要review其可行性。可以引入<strong>AI审查AI</strong>的手段，比如用第二个模型去审核第一个模型产出的正确性，从而双重保险。其次是<strong>安全合规治理</strong>：明确哪些敏感代码或数据不能让AI接触，使用大模型时添加脱敏措施，防范隐私泄露；关注AI输出的开源协议和版权问题，避免侵权。再次是<strong>模型治理</strong>：跟踪大模型的版本和变更，对其行为定期评估，防止模型产生偏见、有害输出等。可以设立<strong>AI输出日志和追溯</strong>机制，每次AI自动化操作要记录其依据和理由，以备审计和责任认定。最后还有<strong>人员角色调整</strong>带来的治理，比如引入“提示词审核员”角色，对重要Prompt进行把关；或者设立“AI运营”岗位，专门监控多个AI Agent的运行状态。工程治理的总体目标是在享受AI效率红利的同时，把潜在风险降到最低，让团队对AI“放心”。正所谓“<strong>信任但要验证</strong>”，好的治理机制就是给AI加上一套“红绿灯”和“护栏”，既不束缚其手脚，又能防止其偏离正确轨道。</li>\n<li><strong>流程优化：</strong> 最后一方面，是考虑<strong>整个软件开发流程如何因AI而优化重塑</strong>。当AI加入后，许多传统流程瓶颈可以打破。方法论指导团队重新审视开发各环节，以充分发挥人机协作潜能。例如，在需求分析阶段，引入AI帮助快速产出原型或技术调研报告，加速决策；设计阶段，让AI根据非功能需求自动选择技术方案、生成架构草图，减少冗长会议讨论；开发阶段，更是可以<strong>并行化</strong>：过去串行的开发-测试-部署流程，现在AI可以同时生成代码和测试，测试通过的模块AI立即部署到测试环境接受反馈，实现准实时迭代。CI/CD流程也迎来升级，出现了<strong>智能流水线</strong>：AI自动根据代码改动调整流水线步骤，发现构建失败会自我尝试修复，再运行。此外，<strong>AIOps</strong>（AI运维）融入流程，使监控和故障处理更加 proactive：AI提前预测问题并触发流程去修补，而不是被动等待报警。优化流程还意味着<strong>精益思想</strong>的应用——识别并去除因为AI引入而可能出现的新浪费。例如，有时AI生成了无用的代码，需要流程上增加过滤；又比如Prompt设计和调优可能花费较多时间，需要纳入迭代计划。持续改进是关键：团队应该定期回顾，哪些环节AI用得好效率就高，哪些地方还可以进一步自动化。通过不断微调流程，才能适应逐步升级的AI能力，真正做到<strong>事半功倍</strong>。当流程优化到位，软件交付将呈现出前所未有的敏捷和弹性，让团队能够在竞争中迅速响应变化、交付更高质量的软件。</li>\n</ul>\n<p>以上六个方面构成了AI增强软件开发的方法论全景图：从人的工作方式，到人的能力提升；从知识和工具资产，到治理和流程调整。它们彼此关联、缺一不可。实践证明，在推进AI应用时，如果只关注某一两个方面而忽略其他，往往会遇到瓶颈。比如只上工具不改流程，可能无人用好工具；只强调效率不顾治理，可能引发风险。唯有综合平衡发展，建立起<strong>系统性的AI+软件工程框架</strong>，才能将六个齿轮扣合起来，带动组织在成熟度阶梯上稳步攀升。</p>\n<h2 id=\"方法论公式解析交付效率-α-人类认知输出-1α-ai执行因子\"><a class=\"anchor\" href=\"#方法论公式解析交付效率-α-人类认知输出-1α-ai执行因子\">#</a> 方法论公式解析：“交付效率 = α × 人类认知输出 × (1–α) × AI执行因子”</h2>\n<p>在引入AI的人机协作中，有一个有趣的公式可以帮助我们思考<strong>人和AI各自作用的权重</strong>以及最佳配合方式。令HCG =人类认知输出，AIEF = AI执行因子，α=人类的参与比例，则：<br />\nDelivery Efficiency=∑(α×HCG×(1−α)×AIEF)<br />\n这个公式看起来简单，却蕴含着重要理念。下面我们来解析其中各要素及其意义：</p>\n<ul>\n<li><strong>人类认知输出：</strong> 这是指人类在软件开发中贡献的<strong>智力产出</strong>，包括创造力、抽象思维、情境判断、经验洞察等等。简单说，就是人脑能够提供的价值，例如对需求的理解、对问题的分析、对架构的创意设计等。这些是AI目前无法完全替代的高层次认知活动。人类认知输出越高，意味着人的想法越有深度和准确性，给项目带来的<strong>思想含金量</strong>越大。</li>\n<li><strong>AI执行因子：</strong> 代表AI在执行层面的<strong>能力因子</strong>，包括计算速度、并行处理能力、基于海量知识给出解决方案的能力等等。可以把它看成AI的“效率系数”和“正确率系数”的综合。如同一个超高速且基本不知疲倦的工程师，AI执行因子高就意味着它完成具体实现工作的速度快、质量高、覆盖广。</li>\n<li><strong>α 和 (1–α)：</strong> 这两个系数可理解为<strong>人和AI在整个工作中的参与度比例</strong>。其中α是人类的参与比例，(1–α)则是AI的参与比例。举例来说，如果α=0.7，那么人力主导70%，AI承担30%；α=0.5则表示双方平衡，各半。α并非固定不变，可以随着团队成熟度和任务性质动态调整。初期α可能大一些（人负责主导），后期AI能力增强后α可以适当降低，让AI多干活。</li>\n</ul>\n<p>公式写成乘法形式而非加法，意味深长：它暗示着<strong>只有人类认知和AI执行双方面都有贡献时，交付效率才会为正并产生效益</strong>。如果α=1（即完全依赖人，AI不参与，(1–α)=0），则效率=0，说明没有AI辅助，人的产出达不到AI+人协作情况下的最大化（这里0是相对值，表示相比理想协作的效率损失）；同理，如果α=0（完全依赖AI，人类不参与认知，AI缺乏人的引导，等于无头绪地乱干），效率结果也是0。这揭示了一个道理：<strong>人或AI任何一方的缺席都会使效率大打折扣</strong>。要获得高效的交付，必须人机结合，发挥双方所长。人提供方向和智慧火花，AI提供速度和勤勉执行，两者相乘，才能产出远超单方的成果。<br />\n进一步，从数学角度看，α×(1–α)这一项在α=0.5时达到最大值0.25。这可以理解为，当人机参与度各约一半时，效率提升潜力<strong>最大</strong>。虽然这不是精确的定量指导，但它形象地表示：过度依赖人或过度依赖AI都非最佳，<strong>最佳状态是两者均衡互补</strong>。当然，不同任务平衡点会不同，例如创意设计类工作可能需要α高一些（人主导），而批量代码生成可让α低一些（AI多做）。但总原则是找到一个让人和AI协同效应最大的配比点。在这个配比下，人类认知的优势被充分利用，AI的强劲执行力也被尽可能调动起来，两者相互促进而非相互制约。<br />\n举个具体例子来说明公式的意义：设想一个团队在传统模式下人均每天产出X单位“认知输出”且执行力转化率100%，那么交付效率 baseline 是X。如果引入AI后，人每人每天仍贡献X的想法，但因为AI接手繁琐实现，他们可以腾出精力构思更多创意，假设人认知输出提升到1.2X，同时AI执行因子带来额外效率，使得最终实现产出相当于原来的1.5倍。在理想配合下，如果α和(1–α)取均衡，那么综合效率可能接近1.2×1.5=1.8倍。反之，如果人完全甩手不管（α≈0），AI缺少人正确引导，产出质量可能很低；或者人拒绝AI帮助（α≈1），那只会保持在X，增长有限。这例子虽不严格但能说明：<strong>交付效率取决于人机双方贡献的乘积而非简单相加</strong>，因此要提高效率，不能靠单方面极端努力（只强化人或只强化AI），而要让两者都发挥作用，并找到那个最佳协作点。<br />\n这一理念对管理者有重要启示。在部署AI赋能时，不应走两个极端：要么担心AI不靠谱什么都让人来（浪费AI潜能），要么迷信AI万能把人置于旁观（AI缺乏人类智慧指导往往效果不佳）。正确的做法是<strong>让人类专注贡献高价值认知</strong>（提高α部分的“人类认知输出”量和质），同时<strong>让AI充分发挥执行威力</strong>（提高AI执行因子，同时通过流程让AI多参与，即提高(1–α)部分的比例），并通过不断试验调整二者配比，直到找到效率最优的平衡点。这其实正是前文方法论各方面要解决的问题：训练人更聪明地用AI，升级工具和流程让AI多干活且干得好，完善治理防范AI偏差……归根结底都是为了提升公式右侧两个乘数的值，并让α×(1–α)的乘积尽可能大。<br />\n总而言之，这个公式用简洁的形式表达了**“人机协作大于各自”<strong>的思想。最大化交付效率需要我们既发挥人之所长，又尽展AI所能，让人机形成真正的协同增效而非此消彼长的关系。这也是AI辅助软件工程的精髓：不是人PK机器，而是</strong>人类+机器 &gt; 人类 or 机器**。深刻领会这一点，有助于我们在实践中时刻提醒自己：引入AI不是为了削弱人的作用，而是为了把人的智慧通过AI这个杠杆撬动出更巨大的生产力。</p>\n<h2 id=\"人机互动的正向认知循环机制human-thought-giving-ai-knowledge-awakening-human\"><a class=\"anchor\" href=\"#人机互动的正向认知循环机制human-thought-giving-ai-knowledge-awakening-human\">#</a> 人机互动的正向认知循环机制：“Human → Thought Giving → AI → Knowledge Awakening → Human”</h2>\n<p>当人和AI开始紧密配合工作时，会产生一种特殊的<strong>正向认知循环</strong>，驱动着人机双方共同进步、不断迭代优化。这个循环可以概括为：“<strong>Human → Thought Giving → AI → Knowledge Awakening → Human</strong>”，其含义如下：</p>\n<ul>\n<li><strong>Human → Thought Giving（人类给予想法）</strong>：首先由人类出发，提出想法或问题交给AI。这里的人类“想法”可以是各种形式的提示或指令，例如描述一个需求、提出一个问题、设定一个目标。我们把这一步称为**“给予想法”<strong>，意为人类将自己的认知输入贡献出来，交由AI处理。这一步中，人类的作用是引导方向、提供上下文和目标。正如Karpathy所说，使用人工智能的关键在于人类能够创造快速的“生成-验证”循环，也就是</strong>迅速地把念头付诸给AI去尝试**。这一环节，人类要善于表达问题，清晰地把自己的需求或思路转化成AI能理解的Prompt，这是循环良性运转的前提。</li>\n<li><strong>AI → Knowledge Awakening（AI启发知识）</strong>：接下来AI根据人提供的想法进行处理和生成，输出结果。这个结果可能是一段代码、一个回答、一项分析，或者是对人类想法的延伸和细化。本质上，AI在此扮演**“知识唤醒者”<strong>的角色，它往往能提供一些人类未知的信息或新的思路，唤起人类对问题更深入的认识。这就是</strong>知识觉醒<strong>的含义：AI的回答启发了人类，可能纠正了原先的错误认知，或者提供了多种解决方案中的一种最佳路径，抑或引来了新的灵感。哪怕AI的输出不完美，也提供了有价值的参考基线，供人类进一步思考改进。可以说，在这一环节中，AI把广博的数据和训练中学到的模式提炼出来，赋予人类</strong>额外的知识增量**。例如，一个开发者卡在bug上久攻不下，询问AI后得到提示从而茅塞顿开；又或者AI生成了多种设计方案对比，让架构师一下子拓宽了视野。这些都是“知识唤醒”的表现。</li>\n<li><strong>Human（人类认知提升）</strong>：当AI的输出传递回来，人类阅读、理解之后，自身的知识状态发生了改变。要么学到了新知识，要么发现了思路谬误得以及时修正，要么受到启迪产生了新的想法。总之，经过AI的反馈，人类对手头问题的认知<strong>提升到一个更高层次</strong>。这是循环中非常关键的一步——人类从AI那儿获得了“觉醒的知识”，变得比之前更聪明或更具洞察力。这时，人的脑海中可能会出现新的想法、进一步的问题或改进方案。于是，<strong>循环再次开始</strong>：人类带着更新的认识，提出下一个想法或问题，进入下一轮“Thought Giving”。每一轮循环，人类的问题通常会越来越精准、要求越来越高，而AI的回答也在不断逼近最终满意的结果。人机互动就这样螺旋式上升。</li>\n</ul>\n<p>这种人机正反馈循环如果配合得当，能产生惊人的效果：<strong>每一轮交互都比前一轮水平更高</strong>。人借助AI不断校准方向，AI在人的引导下不断优化输出，最终达到单靠人或单靠机器都无法企及的高度。例如，在软件设计讨论中，人提出初步想法→AI根据大量设计知识给出改进建议→人受到启发完善设计→AI再根据新思路产出更详细的方案… 如此往复，可能很快就能收敛到一个极佳的设计方案，而这原本可能需要人类团队长时间的头脑风暴才能达成。<br />\n一个形象的比喻是：人机正向认知循环好比<strong>双人攀岩</strong>，人和AI互相拉扯着一起往上攀登智慧的高峰。人类给AI指引方向（相当于往上搭人梯），AI则将人类推得更高（提供垫脚石），然后人类站上新的高度再看得更远，继续指引AI，循环往复。只要双方默契良好，这种协作就会形成<strong>指数级的认知提升</strong>。<br />\n当然，要让这个循环正向发展，前提是<strong>人类对AI输出进行理性甄别和引导</strong>。如果人一味盲从AI而不加思考，那么循环可能变成错误的放大；或者人类固执己见拒绝AI的有益建议，循环也无法成立。所以，组织在推动人机协作时，应培养成员这样的能力：<strong>快速理解AI输出的含义，判断其可靠性，从中提取有价值的部分，并据此调整下一步提问或任务</strong>。这其实就是Karpathy所强调的，使用AI需要一个<strong>快速“生成-验证”<strong>的流程——AI生成结果，人类迅速验证和消化，再即时给出新的指令修正。这种高频的反馈让AI也能及时根据人类指示改变方向，避免越走越偏。现代一些AI辅助开发工具（如Cursor等编程助手）已经体现了这一点：它们不仅给出AI建议，还提供对比、可视化的界面方便开发者快速验证修改。这些都是在帮助人机形成有效的正向循环。<br />\n人机正向认知循环带来的另一个积极效应是</strong>人和AI共同进化</strong>。人类通过循环不断学习AI的新知识，视野变得更广、能力更强；而AI在循环中通过人类的反馈逐步收敛出更符合人意的行为——即使AI模型本身参数未变，但在与特定用户的交互中会“了解”他们的偏好，从而提供越来越契合的回答。久而久之，一个团队的开发者和AI工具之间会形成某种“默契”或“习惯”，彼此都变得更高效。这种共进化正是我们追求的终极目标之一：AI不仅帮助人完成任务，还在潜移默化中<strong>提升了人</strong>，而人在成长后又能提出更高价值的问题去驱动AI发挥更大作用。<br />\n总而言之，“Human → Thought Giving → AI → Knowledge Awakening → Human”的循环描述了人机协作的理想动态：每一轮互动，<strong>人赋予AI以方向，AI赋予人以洞见</strong>，双方互相成就，循环向上。这个机制解释了为什么简单使用AI一次往往不是最佳做法，而与AI多轮对话、持续迭代会越来越有效。也回答了一些人的疑惑：即使AI现在不能百分百正确，但只要我们与之配合，其实<strong>1加1可以大于2</strong>。认识并善用这种正向循环机制，开发团队就能在实践中不断放大AI的价值，实现真正的智慧飞跃。</p>\n<h2 id=\"结语\"><a class=\"anchor\" href=\"#结语\">#</a> 结语</h2>\n<p>站在当下这个人机共创的起点展望未来，我们不难预见：AI辅助软件交付的成熟度攀升，将重塑整个软件产业的格局。那些率先登上高阶的团队，无疑将在效率、质量和创新速度上遥遥领先，获得难以撼动的竞争优势。当然，通往L5的道路也伴随着挑战——技术的复杂性、组织变革的阵痛以及对未知的担忧。但正如本文反复强调的，关键在于<strong>正确理解人机关系：AI不是替代者，而是增效器</strong>。软件开发的思想火花依然源自人类，只是借助AI的能量被无限放大。<br />\n对于每一位软件开发管理者和企业决策者而言，现在都是一个思考和行动的时刻：评估我们的团队处在AI交付成熟度的哪个阶梯？下一步该如何迈进？也许起初只是鼓励工程师尝试一下Copilot，接着为团队制定几条AI编码规范，引入结构化的提示词开发流程，再往后可能考虑搭建自己的知识型AI助手……每一步的累积，都会为组织带来实实在在的效率提升和能力跃迁。而最终，当我们拥抱了AI赋能的完整方法论，从工作方式、能力培养、知识工程、工具平台、治理体系到流程优化都准备就绪时，就具备了攀登最高峰的底气。<br />\n可以预见，在不远的将来，“软件交付效率 = 人类认知输出 × AI执行因子”的模式将成为新常态。那时，“AI帮忙写代码”将像今天的持续集成一样平常，无AI参与的软件项目反而会令人惊讶。在这个变革过程中，希望本文提出的成熟度模型和相关方法论思考，能够为您所在团队的转型提供有益的参考。无论您如今是处于L1初级尝鲜，还是L3协同进阶，抑或已在冲击L4/L5的前沿探索，都请记住：<strong>拥抱AI，拥抱变化</strong>，让人机正向循环驱动我们不断超越极限。在人类智慧和机器智能的交相辉映中，软件世界的未来注定精彩纷呈，我们每个人都在见证和创造这一历史。让我们踏实走好每一级台阶，迎接由AI加速的卓越交付新时代！</p>\n",
            "tags": [
                "AI",
                "Prompts",
                "Governance"
            ]
        },
        {
            "id": "https://gszhangwei.github.io/2025/06/03/AI-assisted-software-delivery-full-process-maturity-model-white-paper/",
            "url": "https://gszhangwei.github.io/2025/06/03/AI-assisted-software-delivery-full-process-maturity-model-white-paper/",
            "title": "AI辅助软件交付全流程成熟度模型白皮书",
            "date_published": "2025-06-03T11:00:00.000Z",
            "content_html": "<h2 id=\"引言与背景\"><a href=\"#引言与背景\" class=\"headerlink\" title=\"引言与背景\"></a>引言与背景</h2><p>面对瞬息万变的市场和技术环境，越来越多企业开始探索人工智能（AI）在软件交付过程中的应用，以提升效率和创新能力。然而，不同组织在AI赋能软件工程上的实践成熟度各不相同，亟需一套分级模型来指引演进路径。正如自动驾驶领域采用L0到L5的级别定义来描述从人工驾驶到完全自动驾驶的演进过程，软件工程领域也可借鉴类似分级方法。本白皮书面向软件交付领域的实践者和管理者，提出“AI辅助软件交付全流程”的L0–L5成熟度分级模型，从需求分析、设计、开发、测试到部署与运维，全面阐述各成熟度级别的特征与实践方法。本文还将提供每一级的典型场景和行业案例，帮助实践者理解AI赋能的软件交付如何落地并带来效益。此外，我们设计了一套可操作的成熟度自评工具，包含关键判定标准和可视化评估维度，供团队评估自身所处级别。最后，白皮书将给出各等级的演进路径建议，明确从当前级别向上发展的措施、变革要素和关键成功因素，为企业制定AI工程能力提升规划提供参考。</p>\n<h2 id=\"AI辅助软件交付成熟度模型概述\"><a href=\"#AI辅助软件交付成熟度模型概述\" class=\"headerlink\" title=\"AI辅助软件交付成熟度模型概述\"></a>AI辅助软件交付成熟度模型概述</h2><p>AI辅助软件交付成熟度模型划分为L0到L5六个等级，描绘了软件交付过程从完全由人工驱动逐步走向以AI自主为主导的演进之路。在低级别阶段，软件开发仍以人工为核心，AI仅提供有限的工具支持；而在高级别阶段，AI不仅承担主要开发工作，甚至能统筹全流程，实现“机器主导”的智慧开发。这一模型类似一个金字塔形的分级路径，随着级别提高，对应的软件过程平台、数据和知识积累以及AI能力都逐步增强。各级别相辅相成，企业需先打好流程体系和数据基础，才能有效利用更高阶的AI能力。这种演进模式与汽车领域从L0（无辅助）到L5（完全自动驾驶）的分级如出一辙：L0阶段以人工操作和规范为主，而L5阶段则由一个能够掌控全局的AI“超级大脑”来负责软件项目的整体开发与运维。实践者可以借助该模型评估本组织AI赋能软件交付的现状，并据此制定分阶段的能力提升路线。</p>\n<p><img loading=\"lazy\" src=\"/../images/AIFSD_maturity_model.png\" alt=\"AIFSD_maturity_model.png\"></p>\n<p><em>图1：AI辅助软件交付成熟度模型L0–L5示意图（从人工驱动到AI自主演进）。该模型以分级方式描绘了组织在软件需求、开发、测试、部署和运维全过程中引入AI的深度和广度。低级别主要依靠人工和规范，高级别则逐步过渡为AI主导的人机协同，直到全智能化交付生态。</em></p>\n<p>接下来，我们将详细阐述L0到L5各级别的定义、AI能力特征、人机分工方式，以及在<strong>Structured Prompt-Driven Development</strong>（结构化提示驱动开发，简称<strong>PDD</strong>）方法论下的实践要点。每一级别都会结合典型使用场景或行业案例，说明该级别在实际业务中的应用方式及其产生的效益。</p>\n<h2 id=\"L0级：无AI辅助的传统交付模式\"><a href=\"#L0级：无AI辅助的传统交付模式\" class=\"headerlink\" title=\"L0级：无AI辅助的传统交付模式\"></a>L0级：无AI辅助的传统交付模式</h2><p><strong>定义与特征：</strong> L0级代表组织尚未在软件交付中引入任何AI智能能力，完全依赖传统的人力和既有工具完成各环节工作。此阶段的核心是建立明确的软件开发过程体系，并严格遵循标准化流程（如CMMI等）进行需求、设计、编码、测试和运维。团队依靠经过训练的工程师和完善的过程文档来保障项目实施，开发流程的有序执行主要靠人员的经验和对规范的遵循来实现。换言之，L0级的软件交付以“<strong>人工驱动</strong>”为特点，所有决策和创造活动都由人完成，AI仅作为基础工具（如代码编辑器、静态分析器）出现，并不参与智能决策。</p>\n<p><strong>AI能力与人机分工：</strong> 在L0阶段，AI能力基本缺席。所使用的工具尽管可能包含一定自动化功能（例如IDE提供的代码高亮、语法自动补全、重构工具等），但这些属于预先编程的规则或简单算法支持，并非AI智能。因此人机分工方面，人是绝对主体：需求分析、架构设计、编码实现、测试用例编写、缺陷定位修复以及部署运维等所有环节均由人工完成。AI的作用仅限于加快人工执行的速度（比如静态代码扫描提高代码审查效率），但对流程本身没有智能改造。</p>\n<p><strong>Prompt开发实践：</strong> 由于没有引入生成式AI，L0级别基本没有“提示词驱动”的开发实践。开发者可能会通过搜索引擎查资料、使用脚本自动化部分重复性任务，但这不属于PDD范畴。在这一阶段，可以认为<strong>Prompt-Driven Development方法论尚未起步</strong>。开发过程中的知识获取主要靠人工查询和经验传授，而非依赖大型语言模型。实践者在L0阶段关注的是流程的规范性和人员技能培养，暂未涉及AI赋能。</p>\n<p><strong>典型场景与案例：</strong> 大多数传统软件项目团队都曾处于L0成熟度。例如，一个严格遵循CMMI规范的金融行业软件开发团队，在项目各阶段都有完善模板和检查表，人力进行需求评审、架构设计，人工撰写所有代码和测试脚本。即使使用了持续集成工具，也是人工配置和触发，其本质仍是人为控制的软件交付管道。这种模式的<strong>效益</strong>体现在流程有序可控，产出质量依赖于团队经验和规范执行。但与此同时，<strong>效率和创新性受到人员能力上限制约</strong>。随着AI技术的发展，完全人工驱动的模式暴露出效率相对低下、难以快速响应变化等不足。实践者往往将L0视为基准线，通过度量当前效率和质量，为后续引入AI手段提供对比依据。</p>\n<h2 id=\"L1级：AI基础辅助的开发\"><a href=\"#L1级：AI基础辅助的开发\" class=\"headerlink\" title=\"L1级：AI基础辅助的开发\"></a>L1级：AI基础辅助的开发</h2><p><strong>定义与AI能力：</strong> L1级标志着组织开始在软件交付流程中引入初步的AI辅助，主要体现为<strong>智能编程助手</strong>等工具的应用。AI在此阶段具备基于大模型的代码理解和生成能力，但作用范围限于辅助编程等局部环节。例如，利用GPT等大模型实现<strong>智能代码补全</strong>（可以基于上下文完成整行或整段代码，而不只是基于语法规则的补全）、自动生成函数注释、提供代码重构建议，以及<strong>自动生成单元测试</strong>等。这些AI能力显著提高了开发效率和代码质量，但AI仍不具备对全局项目的自主决策权。简言之，L1阶段AI相当于“<strong>智能帮手</strong>”：能理解上下文，给出建议或片段，却无法独立完成复杂任务。</p>\n<p><strong>人机分工：</strong> 在L1阶段，人仍然主导主要的软件交付活动，而AI扮演<strong>辅助者</strong>角色。开发人员在编码时使用类似GitHub Copilot的工具自动补全样板代码，测试人员让ChatGPT根据需求说明草拟测试用例，再由人工审查修改。关键决策如架构方案选择、模块设计仍由人工制定，AI输出需要人审核和定夺。可以形象地将L1阶段的人机关系类比为<strong>驾驶辅助</strong>：工程师手握方向盘，AI提供类似导航或动力辅助，但最终路线和操控仍由人掌控。</p>\n<p><strong>Prompt驱动实践：</strong> 在L1级别，Prompt-Driven Development的实践开始萌芽，但多是<strong>分散的个人尝试</strong>。开发者可能在遇到问题时临时向ChatGPT提问，或者编写Prompt让AI生成一段特定功能代码。每位工程师采用AI的方式不尽相同，尚未形成团队统一的流程。常见实践包括：</p>\n<ul>\n<li><strong>代码生成Prompt：</strong> 开发人员以自然语言描述所需函数的功能，让AI返回代码片段，然后自行集成到项目中。</li>\n<li><strong>解释与调优Prompt：</strong> 当代码报错或运行结果不符预期时，用提示词请求AI解释问题原因并提出修改建议。</li>\n<li><strong>文档与测试Prompt：</strong> 编写提示让AI根据代码自动生成文档说明，或依据需求描述产出测试用例初稿。</li>\n</ul>\n<p>这些Prompt实践 <strong>并非系统性的流程</strong>，而是工程师自发利用AI提高个人工作效率的手段。例如，一位开发者可以通过Prompt让AI生成CRUD接口的样板代码，节省20%–50%的时间；测试工程师通过提示词让AI根据用户故事生成测试用例，然后人工审查调整，从而加速测试编写。值得注意的是，此阶段<strong>缺少标准化的Prompt编写规范</strong>，AI的使用更多取决于个人技能和经验。</p>\n<p><strong>典型场景与效益：</strong> 典型案例包括开发人员在实际项目中使用GitHub Copilot自动补全样板代码，以及客服人员借助ChatGPT起草回复邮件并由人工润色后发送。在这些场景中，<strong>AI作为个人工具</strong>被各自使用，尚未深度嵌入团队流程。尽管如此，L1级实践已带来了显著效益：生产力通常获得可观提升，据一些报告显示可使个人效率提高20%到50%。同时，代码质量也有所改进——AI生成的标准化代码和测试建议有助于减少低级错误。然而，由于缺乏全局统筹，团队协同效益有限，AI的价值主要体现在减轻个人负担而非变革整体流程。这是组织迈向AI赋能的初步阶段，一个“从无到有”的过程：让员工熟悉AI工具，用小范围成功来证明价值并为进一步集成AI奠定基础。</p>\n<h2 id=\"L2级：团队协同的AI集成\"><a href=\"#L2级：团队协同的AI集成\" class=\"headerlink\" title=\"L2级：团队协同的AI集成\"></a>L2级：团队协同的AI集成</h2><p><strong>定义与AI能力：</strong> L2级标志着AI辅助从个人走向团队，在软件交付全流程实现<strong>初步的端到端集成</strong>。AI能力扩展到理解工程上下文，甚至通过<strong>多智能体协作</strong>来覆盖需求、编码、测试、部署等各项任务。这意味着不同角色的AI代理出现：一个AI负责解析需求、将高层需求拆解为开发任务；另一个AI编写相应代码；还有AI自动生成测试用例并执行；甚至有AI代理帮助部署发布。一系列智能体可以协同工作，协助人类一站式地完成完整开发流程。当前业界已有初步尝试，例如早期引起关注的AI编程智能体“Devin”，号称输入一次Prompt即可让AI像工程师一样写代码、创建应用并完成部署。这类AI代理体现了L2级的雏形：AI掌握了一定的软件工程技能，涵盖架构、编码、测试等多个方面，在简单应用场景下接近“一键式”开发。</p>\n<p><strong>人机分工：</strong> 在团队协同的AI集成阶段，人机关系进入<strong>协作模式</strong>。人不再是孤立使用AI，而是<strong>团队共同制定AI使用策略</strong>。开发流程中出现明确的AI参与环节：比如由AI根据用户故事自动生成详细需求规格，然后由人审核；AI根据规格产出代码，由人做代码评查和集成；AI生成测试用例并执行，测试人员只对失败案例进行分析；运维人员让AI agent监控日志，自动提出性能优化建议等。人类角色从直接执行者部分转变为<strong>监督者和协调者</strong>：人工制定任务并监督AI完成，将AI产出纳入流程，并处理AI未解决或高风险的部分。尽管AI已经能够担任“数字架构师”、“自动编码员”、“虚拟测试员”等多种角色，但最终项目责任仍在团队。可以比喻为<strong>人机结对编程</strong>扩展到全团队：每个环节都有AI助手共同作业，但人要统筹这些助手协同配合。</p>\n<p><strong>Prompt驱动实践：</strong> 到了L2阶段，Prompt-Driven Development开始体系化地融入团队开发流程。组织会<strong>建立共享的Prompt库</strong>和使用规范，确保团队成员在各环节使用一致的提示词模式，从而获得可预期的AI输出。PDD在此阶段的典型实践包括：</p>\n<ul>\n<li><strong>需求阶段：</strong> BA或产品经理使用精心设计的Prompt模板，让AI将用户故事自动细化成需求规格或原型；</li>\n<li><strong>开发阶段：</strong> 团队为常见编码任务准备了Prompt范式（例如REST API接口实现的提示模板），开发时调用这些模板，高效地产出标准代码；</li>\n<li><strong>测试阶段：</strong> QA团队维护着<strong>测试用例生成Prompt库</strong>，可针对不同类型的需求描述快速生成覆盖主要路径的测试案例；</li>\n<li><strong>部署阶段：</strong> 运维团队使用Prompt指导AI编写部署脚本、基础架构配置或日志分析报告。</li>\n</ul>\n<p>在L2，Prompt驱动已成为<strong>团队工作流的一部分</strong>：大家共同改进Prompt工程学，交流哪种提示效果更好，甚至使用内部工具管理Prompt版本。团队还可能通过调用LLM的API将Prompt集成到CI&#x2F;CD流水线中，实现如自动代码审查、自动性能分析等功能。这一阶段的PDD实践，使AI从个人助手升级为团队助理，各环节输入输出形成衔接，<strong>Prompt变成驱动软件生产的一种“编程语言”</strong>。</p>\n<p><img loading=\"lazy\" src=\"/../images/PDD_Iterative_Loop_Schematic.png\" alt=\"PDD_Iterative_Loop_Schematic.png\"></p>\n<p><em>图2：提示驱动开发（PDD）的典型迭代循环示意图。每个开发迭代分为三个步骤：首先由开发者编写Prompt描述所需功能；接着AI根据Prompt生成代码或方案；然后开发者验证AI产出并进行调整（如纠错和优化），再进入下一轮循环。与传统Copilot模式下工程师主导、AI辅助生成片段不同，在PDD模式中AI生成了绝大部分代码，工程师的主要工作转变为<strong>如何描述需求</strong>以及<strong>调优AI输出</strong>。这种人机分工的新范式在L2级得到初步实践。</em></p>\n<p><strong>典型场景与效益：</strong> L2级的实践已在部分前沿团队中出现。例如，我们团队建立了<strong>共享Prompt库</strong>，使开发人员或测试人员能够根据用户故事一键生成大部分测试用例，再由AI执行测试并产出报告。又如，我们使用对话式AI对需求文档进行解析和任务拆分，生成初步的技术设计，再由人复核细节。在业界案例方面，Cognizant公司的“<strong>Devin</strong>”被宣传为全球首个AI软件工程师智能体，能够在给定高层需求的情况下自动产出代码并完成部署。虽然实践中发现当前这些AI智能体<strong>只能完成简单小型应用</strong>，技术尚未完全成熟，但它验证了L2级能力的可行性。</p>\n<p>从效益上看，相较L1级个人效率提升，L2级<strong>带来了团队层面的效率飞跃和质量一致性</strong>。有报告指出，在某些环节生产力可能提高两到三倍。通过标准化Prompt和AI助手协同，团队减少了重复劳动，降低了人为错误，开发速度和测试覆盖率显著提升。同时，团队开始积累AI与项目交互的数据，为更高级别的自主化打下基础。不过需要强调，L2级AI仍局限于<strong>中低复杂度</strong>场景，面对庞大复杂系统时往往力不从心，还需要人工主导攻克难题。因此L2更多被视为“协同增效”的阶段——AI让团队“如虎添翼”，但尚未独立承担整套交付工作。</p>\n<h2 id=\"L3级：AI主导的复杂系统开发\"><a href=\"#L3级：AI主导的复杂系统开发\" class=\"headerlink\" title=\"L3级：AI主导的复杂系统开发\"></a>L3级：AI主导的复杂系统开发</h2><p><strong>定义与AI能力：</strong> L3级意味着AI达到能够<strong>自主开发复杂软件系统</strong>的高度。在这一阶段，AI不仅可以完成单一模块的代码生成，还能理解和掌控<strong>大型项目的系统需求和架构</strong>。它能够根据高层需求自动设计整体架构、生成高质量代码，实现全面的测试，最后完成部署。换句话说，AI的能力拓展到“大局观”，可以处理大型企业级应用、高性能计算系统、实时控制系统等复杂项目，而不再仅限于简单CRUD应用。这一级别的AI相当于拥有资深架构师+全栈开发+测试工程师的综合能力。值得注意的是，尽管AI强大到可以输出完整系统，<strong>对于某些极端复杂或高度定制化的需求，人类专家仍需介入指导</strong>。因此L3并非消除了人的作用，而是把AI推上主要开发者的位置，人转为少量干预复杂边缘案例。</p>\n<p><strong>人机分工：</strong> 在L3阶段，开发流程呈现出<strong>“AI先行，人类监督”</strong>的特点。当一个新需求到来，通常<strong>先由AI给出初步方案</strong>：AI根据过往知识自动撰写产品规格或设计文档，然后工程师评审并调整；紧接着AI生成主要代码框架和单元模块，人只在代码评审或关键算法处进行修改；测试由AI智能完成自生成和自执行，人工主要关注AI未覆盖到的特殊测试；部署流程也由AI流水线自动完成，大幅减少人工配置操作。可以看到，大部分工作产出（文档、代码、测试、部署脚本）都有AI的参与甚至主导。人类更多扮演<strong>质量监护人和战略决策者</strong>角色：在里程碑节点对AI产出进行把关，处理AI不擅长或超出经验范围的部分，并设定总体策略。整个组织形成“<strong>AI优先的运作</strong>”：员工在动手做任何任务前，通常先让AI生成一个初稿或建议方案，再基于此进行后续工作。这一转变极大提高了工作起点的高度，使人可以专注于更高层次的问题。可以说L3级实现了软件开发中<strong>广泛而深入的AI赋能</strong>：AI无处不在，但人在幕后掌控方向。</p>\n<p><strong>Prompt驱动实践：</strong> 在L3阶段，Prompt驱动开发已经深度融合进企业的<strong>标准流程</strong>，形成成熟的方法论。首先，组织会针对不同类型任务建立<strong>Prompt模式和范式</strong>，供员工在各种场景下调用，使提示词使用进入工业化阶段。由于AI几乎参与所有环节，Prompt工程实践也覆盖了需求、设计、开发、测试、运维各方面。例如：</p>\n<ul>\n<li><strong>需求&#x2F;设计Prompt：</strong> 产品经理使用复杂Prompt模板让AI输出完整的PRD文档或原型设计草案，然后人工调整细节。这些Prompt可能包含行业特定词汇和格式要求，以确保AI产出符合公司标准。</li>\n<li><strong>Prompt生成代码：</strong> 开发团队积累大量<strong>领域代码开发模式</strong>（code patterns），开发相关平台进行Prompt治理。当需要实现某类常见功能时，工程师只需在平台上选择相应代码实现模式并让AI结合业务细节，AI即可批量产出模块代码。</li>\n<li><strong>测试与运维Prompt：</strong> 测试人员与运维人员联合制定Prompt，让AI根据系统设计自动推演潜在故障并生成故障演练脚本，或根据监控数据生成问题诊断报告。</li>\n</ul>\n<p>此外，L3阶段组织可能拥有<strong>专门的Prompt工程师&#x2F;架构师</strong>角色，负责维护和优化Prompt库，确保提示词驱动在全公司范围内高效发挥作用。Prompt编写逐渐标准化、专业化，有类似代码走查的流程保证Prompt质量。伴随AI能力提升，部分提示可以由AI自行生成和改进（元提示优化），形成AI自我改进循环。这种成熟的PDD实践让AI充分发挥作用：<strong>AI成为默认的第一执行人，而Prompt成为人与AI协作的接口语言</strong>。</p>\n<p><strong>典型场景与效益：</strong> 许多领先科技公司正朝L3能力迈进。例如，某大型软件企业规定“<strong>先AI，后人工</strong>”：无论是撰写设计文档、代码还是测试用例，员工都需先调用内部GPT（Aupro）平台生成初稿，再在此基础上完善。又如，有企业开发了内网知识库和LLM搜索工具，支持员工以对话方式查询系统架构和历史实现细节，从而大幅加快理解和开发速度。在这些实践中，AI几乎参与了每个任务的起点，成为工程师日常工作的<strong>默认助手</strong>。</p>\n<p>L3级带来的效益是<strong>公司范围的生产力飞跃</strong>和质量保证。由于AI介入广泛，各团队在相同时间内交付的功能增多，上市时间（time-to-market）缩短。同时，自动化的测试和分析提高了质量基线，减少漏洞和故障。更重要的是，L3阶段为进一步实现全自动化打下基础：企业积累了大量AI与人协作的数据，完善了AI治理框架，培养了员工信任和运用AI的文化。实践者会注意到，随着AI承担更多工作，团队可以尝试更大胆的创新项目，因为AI随时可提供方案建议供人决策。需要指出，迈向L3也伴随挑战——例如确保AI生成内容的正确性、一致性，建立相应的<strong>治理机制</strong>变得更加关键（详见后文自评工具与治理维度）。总体而言，L3级宣告组织进入“<strong>AI赋能全面展开</strong>”的新阶段：AI无处不在且可靠性达到实用水平，人力开始从具体实现转向高阶监督和创新任务。</p>\n<h2 id=\"L4级：自主智能体驱动的创新开发\"><a href=\"#L4级：自主智能体驱动的创新开发\" class=\"headerlink\" title=\"L4级：自主智能体驱动的创新开发\"></a>L4级：自主智能体驱动的创新开发</h2><p><strong>定义与AI能力：</strong> L4级是AI赋能软件交付的<strong>高度自治与创新阶段</strong>。在此阶段，AI不仅能够自主完成既定的软件开发任务，还可以根据对环境和需求的洞察，<strong>主动提出新的解决方案和改进</strong>。这意味着AI从执行者跃升为“创新引擎”：能够分析大量数据，识别潜在的市场机会或技术优化点，进而自动设计并实现新的功能或应用。技术上，L4级通常由更强大的智能体组成——这些AI代理具备高级的<strong>决策规划和上下文推理</strong>能力，可以在没有明确人类指令的情况下执行复杂任务链。例如，一个AI智能体可以自动监测用户反馈和系统性能数据，发现某模块的改进空间后自行创建开发任务、完成编码测试并部署改进。又例如，公司内部可能存在<strong>自治的AI项目经理</strong>，它会根据战略目标和产品使用数据，主动生成新产品概念或功能提议。简而言之，L4级的AI已具备接近人类产品经理和架构师的创造性思维，能<strong>前瞻性地驱动软件演进</strong>，使其能力超越“按要求完成任务”，开始引领开发方向。</p>\n<p><strong>人机分工：</strong> 当AI具有自主性和创新力后，人机分工关系进一步改变，呈现<strong>“机器主导、人类指导”</strong>的新格局。具体而言，许多日常决策和任务安排由AI智能体主动执行，人类主要在战略层面设定目标和约束，并介入评估AI提出的重大决策。举例来说，任务分配与跟踪可能由AI项目管理代理完成：AI根据优先级自动分配工作项给不同工程AI或人类工程师，并追踪进度；问题诊断与修复可以由运维AI自主进行，它发现系统异常会自动创建issue、定位原因并提供初步修复方案，然后通知相关人员。在这些过程中，实践者更多是<strong>监视者</strong>，确保AI的决策符合公司策略，并在AI偏离预期或遇到伦理&#x2F;合规问题时介入。L4阶段，人类团队可放心将大量重复性、协调性工作交给AI代理，从而腾出时间专注创新战略。可以说这时<strong>AI成为团队的一员</strong>，甚至承担了团队中繁琐沉重的管理和支撑工作，人的角色提升为导师和最终决策者。一个标志性的变化是：很多工作会议可能由AI发起并主持（例如每日站会由AI汇总进展并提出Blocker事项），人类成员配合AI的节奏完成工作。这种高度自治模式带来前所未有的效率和规模效益，但也要求组织有成熟的AI治理和信任机制来支撑。</p>\n<p><strong>Prompt驱动实践：</strong> 在L4阶段，Prompt已经不仅仅是人类用来指挥AI的工具，<strong>AI本身也在生成和使用Prompt</strong>。由于AI智能体可以自主拆解任务并调用其他模型或工具执行，每个自主行为背后往往有由AI动态生成的Prompt。比如，一个AI代理接到高层目标，会根据需要自动构造一系列Prompt去询问代码生成模型编写某模块，或调用运维模型去检查系统状态，其过程类似人类工程师将任务分派给不同专家，只是这里交流语言仍是Prompt。不过，从人类视角看，PDD在L4主要体现在：</p>\n<ul>\n<li><strong>高层目标到Prompt链：</strong> 人类给AI设定战略目标或约束，AI将其转换为内部一连串子任务Prompt，自己同自己的对话完成方案推演。这可以被视为Prompt驱动开发的自我演化版。</li>\n<li><strong>动态Prompt调整：</strong> AI智能体能根据实时反馈动态调整Prompt内容，例如如果某子任务失败，AI会修改提示重新尝试（这类似链式思考与ReAct等算法，让AI拥有一定的自纠正能力）。</li>\n<li><strong>Prompt最佳实践库由AI维护：</strong> 在L4阶段，人类很可能不再直接编写大量Prompt，因为AI已经接管了大部分提示构造工作。但组织仍会维护一个<strong>Prompt治理规则</strong>（例如不得使用某些敏感词、遵循某种格式）以及监控AI生成Prompt的有效性。</li>\n</ul>\n<p>因此，Prompt工程进入<strong>隐性运作</strong>阶段——它依然是AI完成复杂任务的基石，但大部分提示词由AI根据场景自动生成，人类只需在必要时提供高层指引和对AI Prompt策略进行调整。总的来说，PDD在L4达到了高度成熟：Prompt语言成为AI之间、AI与人之间沟通协作的通用接口，开发流程中的各个活动由一系列Prompt链驱动，但许多Prompt已不需要人工干预。</p>\n<p><strong>典型场景与效益：</strong> L4级的鲜明例子是一些<strong>无人干预运维</strong>和<strong>智能决策系统</strong>的出现。例如，某领先互联网企业构建了内部AI助手来<strong>自动处理GitHub问题单</strong>：该AI全天候监控新提交的issue，能自行分类优先级、指派负责人，并给出初步的解决思路同时通知相关利益人。结果是，大量琐碎的事务在无人工参与下被高效处理，开发团队只需关注高优先级或AI无法解决的问题。再如，一些DevOps团队部署了智能部署管家AI，当检测到新代码合入主干，它会自动完成构建、测试、部署到特定环境并运行回归测试，全过程无需人工介入。如果发现异常立即回滚并记录分析报告。<strong>效益方面</strong>，L4级带来的<strong>时间节省和协作成本降低是巨大的</strong>。团队内部的许多沟通、协调工作由AI流水线替代，减少了人为等待和反复沟通，项目交付速度大幅提升。在业务层面，由于AI能自主识别改进机会，企业创新周期加快，可能在竞争中迅速推出新功能，占领先机。还有一个重要收获是<strong>规模效应</strong>：组织可以在不大幅增加人力的情况下承担更多项目和更大用户量，因为AI代理承担了相当部分的工作。当然，迈向L4也要求管理层具备前瞻意识和风险控制能力：必须建立对AI决策的<strong>监督机制</strong>、应急预案，以及培养员工适应与AI共事的新工作方式。总而言之，L4代表着软件交付进入“<strong>半自动驾驶</strong>”甚至接近“全自动”的状态，AI开始展现出引领作用，为企业创造前所未有的价值。</p>\n<h2 id=\"L5级：全自主的AI交付生态\"><a href=\"#L5级：全自主的AI交付生态\" class=\"headerlink\" title=\"L5级：全自主的AI交付生态\"></a>L5级：全自主的AI交付生态</h2><p><strong>定义与AI能力：</strong> L5级是AI辅助软件交付成熟度的巅峰，意味着构建了一个<strong>全面智能的自主管理软件工程生态</strong>。在这一阶段，企业拥有高度完善的AI平台与基础设施，AI几乎完全主导了软件交付全流程，人类只需在极少数情况下进行高层决策或干预。具体来说，L5级的AI被形象地称为“<strong>超级大脑</strong>”，它相当于一个集成了开发、测试、部署、运维等职能的中央AI系统，能够像资深项目经理那样统筹全局，又如专家开发团队那样执行各个细节（真正意义上的<strong>通用人工智能</strong>）。当有新的业务需求提出，人类只需用自然语言向AI描述<strong>业务目标</strong>或<strong>产品愿景</strong>，AI超级大脑即可自主完成从需求分析、架构设计、代码实现到测试验证、部署上线乃至后续监控优化的全部工作，并在过程中不断学习改进。L5阶段的AI能力远超编程范畴，它融合了认知推理、规划学习、跨领域知识，在软件工程各方面达成人类专家水准甚至更高，并具备高度的可靠性和自适应性。可以说L5是一个<strong>AI原生的软件工厂</strong>：软件开发不再是一系列人工任务，而是一套AI驱动的自动化工艺流程，能够<strong>高速、规模化地产出软件</strong>，同时根据反馈持续演进。</p>\n<p><strong>人机分工：</strong> 达到L5级别时，人机分工的特征是<strong>“AI自主，人在环监督”</strong> - <strong>AI负责”做事”，人类负责”把关”<strong>。大部分日常决策、优化和执行都由AI生态自洽完成，人主要承担三个方面的职责：一是</strong>战略规划</strong>——高管定义业务战略和目标，AI据此衍生产品和技术实施方案；二是<strong>治理审核</strong>——确保AI的行为在法律、伦理、商业规则框架内，例如对AI设计的方案进行合规性检查，重要发布节点进行批准；三是<strong>应急干预</strong>——在AI遇到无法解决的新奇问题或出现偏差时，人类专家介入处理并将解决方案反馈给AI学习。简而言之，人从具体开发活动中完全解放出来，转而关注<strong>设定方向和监督结果</strong>。团队组织形态也因此改变：可能不再按传统开发、测试、运维职能划分部门，而是围绕AI平台运作，设立如“AI平台维护组”、“AI伦理与风险管理委员会”等新职能部门，确保这个AI自主生态平稳高效地运行。需要强调的是，尽管AI高度自治，但<strong>人的监督不可或缺</strong>——这类似自动驾驶L5下仍需要安全员监控一样，对于软件AI来说，人类监督确保AI不会偏离公司利益或社会规范。</p>\n<p><strong>Prompt驱动实践：</strong> 在L5阶段，Prompt驱动开发进入一个<strong>高度抽象</strong>的层次。人类几乎不直接编写底层Prompt，取而代之的是通过<strong>高层语义指令</strong>与AI系统交互。这可以看作Prompt在更高层的体现：业务战略本身就是一种“大Prompt”，AI理解并将其展开为自下而上的一系列开发行动。AI生态内部依然充满Prompt交互，但这些都是AI自行生成和处理的，形成一个闭环的<strong>自适应Prompt链</strong>系统。例如，AI超级大脑会根据上一阶段的结果自动调整下一阶段的提示和策略（类似于自动调参和元学习），以持续优化输出质量。从外部看，人类给AI的输入更像是与一个高级经理对话，讨论需求和约束；AI则在内部将其转化成具体实现步骤的提示。此时Prompt工程更关注<strong>体系结构</strong>而非具体措辞：如何设计AI之间沟通的协议、记忆共享机制、反馈循环等。可以说，Prompt驱动在L5成为AI系统的<strong>内在工作语言</strong>，人类只需关注AI理解人类意图的机制是否健全。展望而言，随着AI不断自我优化，也许连这种显式的Prompt都会淡化，AI能够通过更高级的推理方式工作。但就目前理念，PDD在L5依然发挥着关键作用，只是人类从“Prompt编写者”升级为“Prompt架构师”和“意图校对者”。</p>\n<p><strong>典型场景与效益：</strong> 由于L5代表着未来愿景，目前真实世界尚无全面达成L5成熟度的案例，然而一些顶尖科技企业已经显现出雏形。例如，某公司构建了自有的内部AI平台，可以<strong>智能生成微服务架构并快速产出产品级代码</strong>，其速度甚至超过了外包团队的人力产出。这家公司通过定制化的大模型和工具链，使AI生成的服务直接达到生产质量，无需大量人工修正。又如，业界有人提出“Software 3.0”的概念，设想未来软件由AI根据需求自动生成、部署，传统开发流程被颠覆。可以预见，在L5阶段企业将<strong>领先于市场</strong>：自建的AI系统比商用工具更智能、更贴合自身业务，从而形成难以复制的竞争壁垒。效益方面，L5级为企业带来的将是<strong>数量级的效率提升</strong>（有人预期员工生产效率提高10倍到100倍），以及前所未有的创新速度和业务灵活性。同时，人力成本和出错率大幅降低，软件工程进入高度可持续状态。然而，攀登至L5也伴随着高投入和高风险：需要持续的研发投入训练AI、建立完善的数据与知识资产，以及强大的治理框架确保AI行为可靠。并非所有组织都需要也并非都有能力达到L5成熟度——管理者应根据自身战略权衡目标成熟度。总而言之，L5级描绘了一个<strong>AI原生的软件生产新范式</strong>：在这个范式下，企业以AI为核心驱动力，软件交付变得前所未有的高效智能，人类可以将精力集中在愿景和创造上。</p>\n<h2 id=\"成熟度自评工具：评估标准与可视化维度\"><a href=\"#成熟度自评工具：评估标准与可视化维度\" class=\"headerlink\" title=\"成熟度自评工具：评估标准与可视化维度\"></a>成熟度自评工具：评估标准与可视化维度</h2><p>要推动AI辅助软件交付能力的提升，实践者需要首先评估团队当前所处的成熟度级别。为此，我们设计了一个<strong>成熟度自评工具</strong>，涵盖关键判定标准和可视化评估维度，帮助团队找准定位、识别差距并制定改进路线。该评估工具主要包括以下要素：</p>\n<ol>\n<li><strong>关键判定标准</strong>：我们从<strong>人员、流程、技术、数据、治理</strong>五个维度设定了一系列判定标准，每个维度对应若干检查点，用于判断组织在该方面达到的成熟水平。具体而言：<ul>\n<li><strong>人员与技能：</strong> 考查团队对AI工具的掌握程度、AI相关技能培训和角色分工情况。例如，团队中是否有专门的AI工程师或Prompt工程师（AI辅助开发赋能）？多数开发人员是否能够熟练使用AI编程助手？组织文化是否支持人机协作？这一维度衡量人在AI赋能环境下的准备程度。</li>\n<li><strong>流程与协作：</strong> 评估AI是否融入软件交付流程以及团队协作方式。例如，需求、开发、测试流程中是否定义了AI参与的步骤？团队是否建立了标准的Prompt使用流程或AI结果审核机制？不同岗位之间是否通过AI实现信息共享与协同？该维度反映AI应用的制度化水平。</li>\n<li><strong>技术工具：</strong> 衡量企业AI基础设施和工具链的完备性。如是否部署了代码智能补全工具、自动化测试方案、持续交付管道中嵌入AI分析工具等？是否构建了自己的大语言模型应用平台或使用了成熟的第三方AI平台（如Azure OpenAI、GCP AI、AWS AI等服务）？技术维度决定了AI能力可发挥的上限。</li>\n<li><strong>数据与知识：</strong> 检查组织的数据资产和知识管理是否支持AI高效工作。例如，是否构建了高质量的Prompt知识库&#x2F;知识图谱供AI检索？代码库和文档是否实现了数字化、结构化，方便AI进行语义搜索和理解？是否有机制将项目过程中产生的新知识反馈给AI模型训练（持续学习）？数据维度是AI“智慧”的源泉，成熟的数据策略是高阶AI应用的前提。</li>\n<li><strong>治理与安全：</strong> 审视AI应用的风险管控和治理措施。包括是否建立AI输出审核规范、错误纠正流程，是否有数据隐私和安全政策保障AI使用？有无明确的AI伦理与合规准则？当AI决策失误时有无应急处理机制？治理维度保证AI在可控范围内可靠运作。</li>\n</ul>\n</li>\n</ol>\n<p>每个维度我们将L0–L5级别的典型特征转化为分级判定标准。例如，在“人员”维度：L0级可能对应“团队成员不使用AI工具或仅有个别尝试”，L3级可能对应“全体研发人员日常使用AI工具并经过培训，出现新的AI工具会快速学习掌握”，L5则对应“组织新设AI协同岗位，员工主要从事监督和创新工作，常规开发由AI承担”。通过对照这些标准，管理者可以判定各维度大致处于哪个级别。</p>\n<ol start=\"2\">\n<li><p><strong>评分与自评流程：</strong> 建议采用<strong>调查问卷或打分卡</strong>的形式进行自评。针对上述每个检查点，团队可以评分（例如1~5分对应从初级到卓越）。然后将每个维度的得分与级别标准对照，确定该维度的成熟级别。需要注意的是，并非所有维度都会整齐划一地达到同一L级——例如技术工具可能已经比较先进（接近L3），但治理机制还停留在L1水平。自评工具允许各维度分别评估，从而找出<strong>短板</strong>。</p>\n</li>\n<li><p><strong>可视化评估维度：</strong> 为了直观呈现评估结果，我们建议使用<strong>雷达图（蜘蛛图）等多维度可视化方式，将人员、流程、技术、数据、治理五个维度的成熟度绘制在同一图表上。这样团队可以一目了然地看到自身在各方面的强项和弱项。例如，图3示意了一支团队在各维度上的评分轮廓，蓝色区域代表当前水平，红色虚线代表目标水平。通过此图可以直观了解该团队需要重点提升的领域。另一个有用的可视化是热力矩阵</strong>，以级别为横轴、五大维度为纵轴，高亮显示当前所在级别，帮助团队明确自己在每个方面上距离下一等级差距几何。使用这些可视化评估维度，可以将抽象的成熟度概念具体化，辅助内部沟通和决策。</p>\n</li>\n</ol>\n<p><img loading=\"lazy\" src=\"/../images/Comparative_analysis_of_AIFSD_maturity.png\" alt=\"Comparative_analysis_of_AIFSD_maturity.png\"></p>\n<p><em>图3：团队AI成熟度自评雷达图示例。蓝色区域为团队当前各维度评分，红色轮廓为预期目标水平。该图形有助于识别短板，如示例团队在“数据与知识”与“治理安全”维度明显落后于其他维度，需要优先改进。</em></p>\n<ol start=\"4\">\n<li><strong>自评结果解读：</strong> 通过以上工具，团队可以得到自身在L0–L5模型下的“<strong>定位画像</strong>”。值得强调的是，自评的目的是<strong>找准改进方向，而非追求最高级别</strong>。并非所有团队都必须以L5为目标，实际应结合组织战略和投入产出比来决定最适合的成熟度水平。自评结果应帮助团队回答：我们在哪些方面已经具备较好基础？哪些方面存在明显短板限制了AI进一步应用？基于这些认知，管理者可以更有针对性地规划提升举措。例如，如果技术工具和数据基础已到位但人员技能不足，则应加强培训和文化建设；如果人员和流程准备度很好但缺乏合适的AI工具，则应考虑技术引入。自评结果还可以作为衡量进步的<strong>基准线</strong>：定期重复评估，观察各维度评分提升情况，来跟踪AI成熟度建设的成效。</li>\n</ol>\n<h2 id=\"演进路径与关键成功因素\"><a href=\"#演进路径与关键成功因素\" class=\"headerlink\" title=\"演进路径与关键成功因素\"></a>演进路径与关键成功因素</h2><p>明确了当前成熟度和差距后，组织需要制定从现有级别向更高AI成熟度演进的路径。不同起点的团队在进阶过程中侧重点各异，但总的来说，每一级提升都涉及<strong>技术引入、流程变革、人员培养和治理完善</strong>等要素。以下分级别提供演进路径建议，帮助管理者理解升级所需的措施和关键成功因素：</p>\n<h3 id=\"从L0到L1：起步引入AI辅助\"><a href=\"#从L0到L1：起步引入AI辅助\" class=\"headerlink\" title=\"从L0到L1：起步引入AI辅助\"></a>从L0到L1：起步引入AI辅助</h3><p><strong>主要挑战：</strong> 团队尚无AI使用经验，可能存在观望和抗拒心理；基础设施和数据准备不足。</p>\n<p><strong>演进举措：</strong></p>\n<ul>\n<li><strong>试点与培训：</strong> 选择一个痛点明显的环节（如编码或测试）进行AI工具试点，比如部署代码自动补全或自动测试用例生成工具。提供培训让工程师掌握使用方法，分享试点收益以建立信心。</li>\n<li><strong>基础环境准备：</strong> 确保开发环境允许AI工具运行，例如升级IDE、配置必要的插件。准备好样本项目和数据以便AI产生有用结果（例如为代码生成AI提供部分代码库上下文）。</li>\n<li><strong>明确应用场景：</strong> 确定AI介入的具体场景和边界，比如规定工程师在新模块开发时应尝试使用AI生成部分代码，但不强制要求在关键安全模块使用AI（视风险而定）。</li>\n</ul>\n<p><strong>变革要素：</strong> 管理层需要营造支持创新的氛围，鼓励团队尝试新工具；容忍初期可能出现的低效或错误，以积极态度对待改进。建立反馈机制收集试用者意见，不断优化AI工具配置和使用策略。 <strong>关键成功因素：</strong> 自上而下的<strong>领导支持</strong>至关重要——管理者亲自参与或关注试点，给予资源倾斜和正面宣传。选择<strong>合适的试点项目</strong>也很关键，最好是时间紧张或人力不足的任务，让AI的优势充分显现。通过早期的<strong>成功案例</strong>证明AI价值，消除怀疑论调，为全面推广铺平道路。</p>\n<h3 id=\"从L1到L2：扩展AI应用与团队协同\"><a href=\"#从L1到L2：扩展AI应用与团队协同\" class=\"headerlink\" title=\"从L1到L2：扩展AI应用与团队协同\"></a>从L1到L2：扩展AI应用与团队协同</h3><p><strong>主要挑战：</strong> AI应用从个人走向团队，需克服不同成员使用不一致的问题，数据和流程开始成为瓶颈。</p>\n<p><strong>演进举措：</strong></p>\n<ul>\n<li><strong>建立团队规范：</strong> 制定AI使用的最佳实践和规范文档，例如统一Prompt编写风格、代码评审时检查AI生成代码、版本管理中标识AI贡献部分等。鼓励成员分享各自使用AI的经验，沉淀为团队知识。</li>\n<li><strong>引入团队级工具：</strong> 部署协同版的AI平台，如企业版ChatGPT或开源的大模型本地部署，方便团队共享上下文。将AI接入项目管理和CI流水线，例如自动将用户故事发送给AI生成任务清单，让AI Bot参与Merge Request审查等。</li>\n<li><strong>扩展应用范围：</strong> 在保持编码辅助的同时，尝试将AI用在更多环节：如需求分析会议上使用AI实时记录要点并整理需求文档；测试阶段引入AI根据说明生成更多测试场景；运维上让AI分析日志定位故障原因。逐步实现AI对<strong>全流程</strong>的覆盖，而不仅是开发一隅。</li>\n<li><strong>数据准备与整合：</strong> 开始建设团队知识库，把历次需求、设计、代码、测试结果等资料数字化存储，作为AI获取背景知识的来源。对AI输出的结果数据（如AI生成的代码、问题修复建议）也进行收集，为将来训练或规则改进提供素材。</li>\n</ul>\n<p><strong>变革要素：</strong> 需要<strong>流程变革</strong>来适应AI团队协作，例如调整Scrum流程，在每个Sprint计划中安排AI辅助环节的时间和步骤。<strong>角色调整</strong>也逐渐出现，可能指定“AI协作负责人”来监督AI输出和质量。<strong>工具整合</strong>是技术重点，要花时间打通AI平台与现有开发工具链。</p>\n<p><strong>关键成功因素：</strong> 确保<strong>团队 buy-in</strong>，也就是多数成员真正采纳AI工具而非阳奉阴违——可通过选定AI拥护者做榜样，持续培训和正向激励来实现。建立<strong>快速反馈循环</strong>也很重要：当AI建议被证明无效甚至出错时，要及时调整使用策略或工具参数，避免团队对AI失去信任。管理者应关注<strong>效率与质量指标</strong>，以量化数据证明L2阶段团队协同AI的价值（比如代码产出速度提升、缺陷率下降等），巩固推进动力。</p>\n<h3 id=\"从L2到L3：深化AI赋能与自主化\"><a href=\"#从L2到L3：深化AI赋能与自主化\" class=\"headerlink\" title=\"从L2到L3：深化AI赋能与自主化\"></a>从L2到L3：深化AI赋能与自主化</h3><p><strong>主要挑战：</strong> 进一步提高AI主导程度，需要更强大的模型、更完善的数据支撑和更成熟的治理。团队要适应从“人机协作”向“AI主导、大幅自动化”转变的工作方式。</p>\n<p><strong>演进举措：</strong></p>\n<ul>\n<li><strong>升级AI能力：</strong> 引入或训练更高级的大模型和专用AI组件，以应对复杂项目需求。例如，引入能够进行架构设计和复杂推理的模型，或训练自有模型使其熟悉本领域特定架构模式和业务规则。技术上可能需要投入GPU计算资源或引进外部AI服务。</li>\n<li><strong>全流程自动化改造：</strong> 梳理现有软件交付流程，将可以自动化的部分用AI服务替代或增强。例如实现“文档即代码”：让需求&#x2F;设计文档与代码实现双向同步，AI根据文档更新代码或者反过来更新文档。再如扩大持续集成中AI自动分析的范围，对每次构建都进行智能质量检查和风险预测。目标是尽量减少人工在常规流程中的手动操作，把人力从<strong>重复性活动</strong>中解脱出来。</li>\n<li><strong>知识中台建设：</strong> 构建统一的<strong>AI知识中台</strong>，整合代码、设计、测试、运维各类知识。建立代码和文档的双向追踪、需求到实现的溯源，让AI能够方便地获取全景知识以支持决策。这可能需要开发知识图谱、向量数据库等，将企业知识资产结构化。L3阶段，没有扎实的数据和知识底座，AI无法真正理解复杂系统。</li>\n<li><strong>AI治理体系：</strong> 制定更完善的AI治理策略，包括AI输出质量验证流程、AI决策权限划分、异常情况的人工接管规定等。特别是当AI开始涉足架构和重大决策时，需明确哪些范围AI可以自主决定，哪些必须人审核批准。建立AI绩效指标（如AI生成代码通过测试的比例、AI检测到的漏洞数量等）来持续评估AI表现，发现偏差及时纠正。</li>\n</ul>\n<p><strong>变革要素：</strong> <strong>组织结构调整</strong>可能在此阶段发生。例如成立专门的“AI平台团队”负责模型和知识中台的建设运维；让各产品团队配备AI领域专家，协助业务团队高效使用AI。<strong>流程方面</strong>则趋向融合：可能逐步模糊开发、测试的界限，因为AI可以同时生成代码和测试，团队转向以功能或产品为单位组织而非传统职能划分。</p>\n<p><strong>关键成功因素：</strong> <strong>高质量的数据和知识</strong>是L3演进的基石，没有它AI智能就是沙上建塔。管理者需确保投入足够资源整理和维护知识库，使AI有“料”可用。此外，<strong>渐进式过渡</strong>很重要：并非一蹴而就让AI接管复杂项目，而是先从子系统或独立模块入手试验，当AI在小范围内可靠运作后再扩大战果。成功案例累积将帮助团队建立对AI深度参与的信任。最后，<strong>治理得当</strong>是成败关键：既不能对AI完全放任导致风险失控，也不能管得太严让AI无所作为，须找到安全与效率的平衡。设置跨部门的AI治理委员会、定期审查AI项目效果，可以为高自主化探索保驾护航。</p>\n<h3 id=\"从L3到L4：赋能AI自主与创新\"><a href=\"#从L3到L4：赋能AI自主与创新\" class=\"headerlink\" title=\"从L3到L4：赋能AI自主与创新\"></a>从L3到L4：赋能AI自主与创新</h3><p><strong>主要挑战：</strong> 让AI从执行工具变为主动创新主体，需要重大理念转变和技术跃升。如何信任AI做出正确决策、激发AI创造力并融入业务创新流程，是管理者面临的新课题。</p>\n<p><strong>演进举措：</strong></p>\n<ul>\n<li><strong>部署自治代理</strong>：引入自治AI代理框架，让AI具备<strong>自主决策与连续行动</strong>能力。例如使用开源ADK、AutoGPT、langgraph等框架，或开发定制的智能体，赋予AI在无人干预下执行任务链的能力。先选择低风险领域试验，如让AI代理负责定期性能优化：它可主动发现瓶颈、尝试优化方案并测试效果。逐步扩展到更关键领域。</li>\n<li><strong>人机协同创新流程</strong>：重塑创新流程，将AI融入产品创意和研发的早期阶段。比如建立“AI+人”联合头脑风暴机制：让AI分析用户反馈数据提出新功能建议，人类与AI讨论评估可行性。对于可行想法，让AI产出原型或技术方案，再由团队决策是否实施。这样把AI当作产品经理&#x2F;顾问来使用，发挥其广泛搜索和模式识别优势，为人提供灵感。</li>\n<li><strong>决策权限梯度</strong>：逐步提升AI决策权限。开始可给AI <strong>“建议权”</strong>：AI可以主动发起某些常规决策（如任务分配、缺陷修复），但需人确认。随着AI表现可靠度提高，扩大其“<strong>执行权</strong>”范围：例如重复出现的类似缺陷让AI自动修复并部署，无需每次审批。最终在明确边界内赋予AI完全自主权（例如低影响的运维调整AI可自主执行），人类主要关注高层策略和异常处理。这个过程需在<strong>实践中动态调整</strong>，确保AI既有发挥空间又不越界。</li>\n<li><strong>风险控制与监控</strong>：针对AI自主行动可能引发的风险，建立完善的监控和回滚机制。例如重要系统引入AI自治时，设置“沙盒环境”或双轨制——AI的动作先在影子系统中执行并验证，再应用到真实系统。配置异常报警，一旦AI行为出现异常迅速通知人类介入处理。每次AI自主决策导致的问题都应记录分析，完善AI风控规则。</li>\n</ul>\n<p><strong>变革要素：</strong> <strong>文化和信任</strong>成为此阶段的决定性因素。组织必须培育一种<strong>信任AI</strong>又<strong>敢于纠错</strong>的文化：员工信任AI可以做好很多工作，同时对AI可能犯错保持警觉和宽容。管理层在言行上要鼓励尝试，让员工相信使用AI自主系统不会因偶发错误受到惩罚，而会作为学习改进机会。<strong>组织架构</strong>可能进一步演变，例如设立“AI创新实验室”专门孵化AI提出的新产品概念，与业务部门合作推进落地。</p>\n<p><strong>关键成功因素：</strong> <strong>小步快跑，封闭测试</strong>是降低风险推动创新的好方法。让AI在受控环境下尝试发挥创意，成功后再推广至生产，是稳妥路径。<strong>人才复合</strong>也很关键：在这个阶段需要既懂业务又懂AI的复合型人才作为桥梁，既能理解AI给出的创意又能评估其商业价值。<strong>高层支持</strong>依然重要——AI提出的变革性方案有时可能超出常规，需要管理层有胆识拥抱变化。最后，<strong>调整激励机制</strong>以适应人机新角色：例如，当AI承担更多基础工作后，如何激励员工专注更高价值任务、如何评价AI工作成效，都需要新的考核和激励办法，以确保AI与员工协同创造出最大价值而非彼此抵触。</p>\n<h3 id=\"从L4到L5：构建AI原生的交付生态\"><a href=\"#从L4到L5：构建AI原生的交付生态\" class=\"headerlink\" title=\"从L4到L5：构建AI原生的交付生态\"></a>从L4到L5：构建AI原生的交付生态</h3><p><strong>主要挑战：</strong> 向L5演进意味着进入无人区，需要在技术体系、组织模式和商业策略上进行系统性重构。投入巨大、难度极高，且行业鲜有先例可循。</p>\n<p><strong>演进举措：</strong></p>\n<ul>\n<li><strong>打造核心AI平台</strong>：企业需要自主构建高度定制化的AI平台和工具链，将开发、测试、运维等功能全面集成。例如开发自己的大模型并持续训练，使其完全理解本企业业务领域和代码规范；搭建统一的AI编程中枢，连接IDE、版本管理、部署管道、监控系统，实现AI对整个生命周期的掌控。这通常要求汇聚顶尖AI研究和工程力量，可能与高校、科研机构合作进行攻关。</li>\n<li><strong>数据与模拟驱动</strong>：L5生态需要强大的数据流和仿真支持。构建全面的数据采集和回馈机制，软件运行过程中产生的海量数据（用户行为、性能指标、故障情况）自动成为训练AI模型的燃料，不断提升其能力。引入高级模拟环境，让AI在虚拟空间中测试新的设计和优化策略，降低实环境出错风险。可以借鉴自动驾驶的思路，通过模拟训练加速AI成熟。</li>\n<li><strong>组织全面转型</strong>：公司架构朝着“AI原生”转型。例如传统IT部门演变为“AI能力中心”，业务部门也配备AI专家，决策流程中AI分析报告成为标配输入。可能诞生新的CXO角色如CAIO（首席AI官）来统筹AI生态。业务流程重塑，以充分发挥AI自动化和智能化优势，比如销售、客服等与研发平台数据直连，市场需求由AI实时捕捉并驱动开发迭代。</li>\n<li><strong>价值链重构</strong>：考虑L5能力下商业模式的变化，提前布局。如软件交付速度和效率提升一个数量级后，是否采取按需定制、超高速迭代的产品策略？AI原生生态下可能诞生全新业务（例如将内部AI开发能力开放为服务）。高层应思考如何<strong>将AI优势转化为市场领导力</strong>。这要求技术战略与企业战略高度融合。</li>\n</ul>\n<p><strong>变革要素：</strong> <strong>战略定力与长期投入</strong>是向L5演进的必要条件。因为L5的实现周期可能较长且回报不确定，管理层需有远见和耐心，持续投入资金和资源。<strong>全员再定位</strong>也是巨大挑战：随着AI接管大部分工作，员工角色需要彻底转型，企业文化需重新塑造（从“人如何做好”转为“人如何让AI做好”）。这涉及大量培训、心理建设和组织变革管理。<strong>外部生态协调</strong>亦不可忽视：当企业内部达到了高度AI自主，还需处理与客户、监管机构的关系——确保输出的软件和决策被外部利益相关者接受和信任。这可能需要行业标准的建立和推动。</p>\n<p><strong>关键成功因素：</strong> <strong>技术突破与创新</strong>是首要因素，没有卓越的AI技术能力就无法实现L5。企业应吸引顶尖AI人才，鼓励内部创新，并积极专利和保密以巩固领先优势。<strong>风险管理</strong>仍然重要：在追求全自主的同时，要有机制防范AI系统失控或重大失误的灾难性风险（例如建立AI伦理审查委员会，仿真极端场景测试AI反应）。<strong>渐进里程碑</strong>的设置能帮助团队在长征路上保持动力——将L5远景拆解为可实现的阶段性目标，一步步实现，如先实现“无人参与夜间构建发布”、再实现“无人参与小版本更新”等。每达成一步都庆祝和宣传，巩固信心和士气。最后，<strong>务实与灵活</strong>的态度必不可少：虽然L5是终极目标，但管理者应始终审视现实收益，在投入和产出间保持平衡，不盲目追求炫目的全面自治而忽略实际业务价值。成功的L5应当是水到渠成、顺势而为的结果，而非脱离商业逻辑的空中楼阁。</p>\n<h2 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h2><p>人工智能正加速重塑软件交付的方式，从辅助编码的小工具一路发展到全流程自动化的“超级大脑”愿景。本文提出的L0–L5成熟度模型，为企业描绘了一条逐步进化的路线图：从以人为主导、规范驱动的传统模式，演进到人机协同共创，最终展望以机器为主导的软件工程新范式。通过对各级别的深入阐述和案例剖析，我们可以看到，每提升一个等级，都是技术能力、流程机制和人员技能的协调跃升。企业应结合自身现状，利用成熟度自评工具找准位置，明确差距，以分阶段的策略稳步迈向更高的AI赋能水平。需要强调的是，成熟度建设是<strong>长期的组织能力建设</strong>，不能一蹴而就也不应盲目攀比。正确的做法是以业务价值为导向，在提升效率和控制风险之间取得平衡。管理层的远见、对变革的毅力和全员的共同努力，将决定这一转型的成败。展望未来，当下的探索和努力将奠定企业在“AI+软件交付”时代的竞争优势。希望本白皮书提供的模型和方法论能为企业决策者提供有益参考，助力大家在AI驱动的软件工程变革中抢占先机，释放更大的创新潜能和商业价值。</p>\n",
            "tags": [
                "AI",
                "Prompts",
                "Governance"
            ]
        },
        {
            "id": "https://gszhangwei.github.io/2025/05/22/PDD-in-Data-Domain/",
            "url": "https://gszhangwei.github.io/2025/05/22/PDD-in-Data-Domain/",
            "title": "PDD在DATA领域的应用实践",
            "date_published": "2025-05-22T11:00:00.000Z",
            "content_html": "<h2 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h2><p>当Structured Prompt Driven Development（PDD）方法论遇上复杂数据场景，会擦出怎样的火花？</p>\n<p>最近一次数据项目的实践，让我有机会将PDD深度应用于BigQuery复杂SQL开发中。面对30+业务分支验证、亿级数据量快照表的挑战，传统开发方式，尤其是在没有相关复杂SQL处理经验的情况下，会显得力不从心。经过一番探索与试错，我不仅找到了PDD在数据领域的稳定应用路径，更重要的是验证了这套方法论在处理复杂业务逻辑时的实际效能。</p>\n<p>如果你也在寻找提升复杂SQL开发效率的方法，或者好奇PDD如何在数据工程项目中发挥作用，那么这次实践经验或许能为你提供一些有价值的参考。</p>\n<h2 id=\"项目背景\"><a href=\"#项目背景\" class=\"headerlink\" title=\"项目背景\"></a>项目背景</h2><p><strong>技术环境</strong></p>\n<ul>\n<li><strong>数据处理</strong>：Python + BigQuery + GCP</li>\n<li><strong>核心挑战</strong>：业务场景复杂，30+业务分支 + 亿级数据量</li>\n</ul>\n<p><strong>业务目标</strong><br>将<strong>预测数据</strong>与<strong>实际收入数据</strong>关联对比，识别差异并补全缺失信息</p>\n<ul>\n<li><strong>数据关联</strong>：预测数据与实际收入全面对比，自动识别差异点</li>\n<li><strong>层级匹配</strong>：机会→账户→市场→区域四级递进匹配，精确标记匹配深度</li>\n<li><strong>标准统一</strong>：跨表字段口径一致性处理，消除数据孤岛</li>\n<li><strong>决策支撑</strong>：输出结构化对比数据集，直接服务业务分析</li>\n</ul>\n<p><strong>核心流程</strong></p>\n<ul>\n<li><strong>多条件关联</strong>：依次按机会、账户、市场和区域四级进行数据匹配，并对匹配成功与未匹配记录分别打标</li>\n<li><strong>基于快照扩展</strong>：对未匹配的行基于业务快照日期进行行扩展，保证所有时间点均有对齐的数据视图</li>\n<li><strong>业务数据补全</strong>：对ID、名称、状态等缺失字段，按照优先级从不同数据源进行补充</li>\n<li><strong>字段标准化</strong>：统一区域缩写为全称，转换特定合同类型，并补充额外标记字段</li>\n<li><strong>汇总去重</strong>：合并所有中间结果，按关键维度分组去重，输出最终对比报表</li>\n</ul>\n<h2 id=\"实现的整体过程\"><a href=\"#实现的整体过程\" class=\"headerlink\" title=\"实现的整体过程\"></a>实现的整体过程</h2><h3 id=\"第一阶段：AI协作下的探索与突破\"><a href=\"#第一阶段：AI协作下的探索与突破\" class=\"headerlink\" title=\"第一阶段：AI协作下的探索与突破\"></a>第一阶段：AI协作下的探索与突破</h3><p><strong>现实挑战</strong></p>\n<p>作为一个两年未曾接触复杂SQL开发的工程师，面对这样的业务场景，坦诚地说，我的第一反应是”从何下手”？幸运的是，AI成为了我最得力的助手。</p>\n<ol>\n<li><strong>业务理解的快速突破</strong>：AI帮助我快速解构了看似复杂的业务需求<ul>\n<li>将抽象的财务对齐需求转化为具体的数据操作步骤</li>\n<li>厘清了多层级匹配的核心逻辑</li>\n<li>明确了数据流向：<strong>预处理 → 扩展 → 补充 → 输出</strong></li>\n</ul>\n</li>\n<li><strong>技术方案的迭代探索</strong>：初期的完全依赖AI生成策略，但很快遭遇现实挑战<ul>\n<li><strong>复杂度挑战</strong>：业务逻辑的复杂程度远超AI的一次性理解能力</li>\n<li><strong>SQL代码理解困难</strong>：需要通过理解AI生成代码的思路，逐步理解整体思路，比较耗时</li>\n<li><strong>细节爆炸</strong>：每个高层任务背后隐藏着无数技术细节（表关联、匹配策略、异常处理等）</li>\n</ul>\n</li>\n<li><strong>第一次完整尝试</strong>：经过多轮迭代，AI生成了一个1157行的”巨型SQL”<ul>\n<li>初版SQL结果：<ul>\n<li><img loading=\"lazy\" src=\"/../images/sql_result_v1.png\" alt=\"sql_result_v1.png\"></li>\n</ul>\n</li>\n<li>初版Prompt：<ul>\n<li><img loading=\"lazy\" src=\"/../images/sql_related_prompt_v1.png\" alt=\"sql_related_prompt_v1.png\"></li>\n</ul>\n</li>\n<li>结果：BigQuery中大量语法错误</li>\n<li>修复后：逻辑验证仍然失败</li>\n</ul>\n</li>\n</ol>\n<p><strong>意外的核心收获</strong>：虽然SQL生成效果不如预期，但这一阶段最宝贵的成果是建立了一套完整的AI辅助验证流程：</p>\n<p><code>编写结构化提示词 → 生成SQL代码 → 基于提示词生成测试数据 → 逻辑验证</code></p>\n<p>这个流程的确立，为后续的PDD方法论应用奠定了坚实基础。正是在这个”失败”的实验中，我意识到：真正的价值不在于AI能否一次性解决所有问题，而在于如何与AI建立高效的协作模式。</p>\n<h3 id=\"第二阶段：理解业务本质，重构解决方案\"><a href=\"#第二阶段：理解业务本质，重构解决方案\" class=\"headerlink\" title=\"第二阶段：理解业务本质，重构解决方案\"></a>第二阶段：理解业务本质，重构解决方案</h3><p><strong>从复杂回归简单</strong></p>\n<p>经过第一阶段的”弯路”，我开始重新审视这个看似复杂的需求。突然间，一个关键洞察浮现：<strong>这本质上就是两张表的数据对比问题</strong>！一旦抓住这个核心，整个解决思路瞬间清晰起来。</p>\n<p><strong>PDD方法论的真正发力</strong><br>基于对业务本质的理解，我开始系统性地重构解决方案：</p>\n<ol>\n<li><p><strong>任务分解策略</strong></p>\n<p> 遵循”<strong>本质决定边界，特征决定细节</strong>“的原则，将庞大的SQL需求拆解为相对独立的功能单元，专注解决一个明确问题：</p>\n<ul>\n<li>利用PDD方法论，将实现逻辑转化为结构化提示词</li>\n<li>任务简化带来的直接效果：<strong>AI幻觉大幅减少，代码生成准确率显著提升</strong></li>\n</ul>\n<p> <img loading=\"lazy\" src=\"/../images/sql_related_prompt_v2_1.png\" alt=\"sql_related_prompt_v1_1.png\"><img loading=\"lazy\" src=\"/../images/sql_related_prompt_v2_2.png\" alt=\"sql_related_prompt_v1_2.png\"></p>\n</li>\n<li><p>快速验证闭环</p>\n<p> 得益于第一阶段建立的验证流程，每个单元都能快速完成准确性验证。<strong>重构成果对比</strong>如下：</p>\n<p> <img loading=\"lazy\" src=\"/../images/comparison_of_reconstruction_effects.png\" alt=\"comparison_of_reconstruction_effects.png\"></p>\n</li>\n</ol>\n<p>然而，当切换到线上环境真实的亿级数据验证时，新的挑战出现了，<strong>性能瓶颈</strong>：</p>\n<ul>\n<li><strong>执行时间</strong>：超过5分钟无法完成</li>\n<li><strong>中间结果</strong>：膨胀至268+GB<ul>\n<li><img loading=\"lazy\" src=\"/../images/SQL_processing_result.png\" alt=\"SQL_processing_result.png\"></li>\n</ul>\n</li>\n<li><strong>下一步方向</strong>：SQL性能优化成为新的攻坚重点</li>\n</ul>\n<h3 id=\"第三阶段：性能优化\"><a href=\"#第三阶段：性能优化\" class=\"headerlink\" title=\"第三阶段：性能优化\"></a>第三阶段：性能优化</h3><p>面对性能瓶颈，我采取了系统性的优化策略：</p>\n<ol>\n<li><p><strong>问题诊断与根因分析</strong>：通过向团队中数据领域的资深专家请教，深入分析后发现核心问题所在：</p>\n<ul>\n<li><strong>核心问题</strong>：AI生成SQL时未充分考虑线上数据规模（已达亿级别），导致对大表执行了多次复杂的Join操作，严重影响查询性能。</li>\n</ul>\n</li>\n<li><p><strong>制定优化策略</strong>：基于问题根因，我们确定了四个关键优化方向：</p>\n<ul>\n<li><strong>减少Join频次</strong>：大表尽量只进行一次Join操作，一次性获取所有必需数据</li>\n<li><strong>前置数据过滤</strong>：在与大表Join之前先进行数据筛选，有效缩小驱动表规模</li>\n<li><strong>精准Join类型选择</strong>：根据业务需求合理选择Inner Join或Left Join</li>\n<li><strong>分步查询优化</strong>：将复杂查询拆分为多个步骤，仅选取必要字段，最大化减少数据传输量</li>\n</ul>\n</li>\n<li><p><strong>提示词精细化改造</strong>：针对性能瓶颈部分，我对相关提示词进行了深度重构：</p>\n<ul>\n<li><strong>定位优化目标</strong>：精准识别需要优化的SQL片段及其对应提示词</li>\n<li><strong>复杂问题分解</strong>：按照既定优化方向，将复杂查询逐步拆解为独立的提示词组件</li>\n<li><strong>逐一验证优化</strong>：为每个提示词组件单独生成SQL并进行性能验证，确保复杂JOIN完全拆解到位</li>\n<li><strong>任务拆分展示</strong>：优化后的Prompt部分效果如下图所示：<ul>\n<li><img loading=\"lazy\" src=\"/../images/performance_enhanced_result.png\" alt=\"performance_enhanced_result.png\"></li>\n</ul>\n</li>\n<li><strong>优化前</strong>：查询超时5分钟无结果，涉及数据量高达数百GB</li>\n<li><strong>优化后</strong>：查询仅需37秒完成，最终结果数据量仅为3.7GB</li>\n</ul>\n</li>\n</ol>\n<p>性能提升幅度超过800%，数据处理效率显著提升。</p>\n<p><img loading=\"lazy\" src=\"/../images/sql_processing_result_final.png\" alt=\"sql_processing_result_final.png\"></p>\n<p>在这一个过程中，当确定优化方向后，补值逻辑其实是需要重写的，但是通过阶段一和阶段二的探索后，在Data领域稳定生成期望SQL的模式已经建立了，所以，即使重新写也没关系，这个过程非常快速的完成了。后续还有进行分之逻辑修复的处理，本质上步骤类似，就不在这里进行展示了。</p>\n<h2 id=\"结构化提示驱动开发-PDD-在数据领域的应用方法论\"><a href=\"#结构化提示驱动开发-PDD-在数据领域的应用方法论\" class=\"headerlink\" title=\"结构化提示驱动开发(PDD)在数据领域的应用方法论\"></a>结构化提示驱动开发(PDD)在数据领域的应用方法论</h2><p>基于项目实践，总结PDD在数据领域特别是SQL开发中的应用方法论，为类似场景提供参考。</p>\n<h3 id=\"PDD在SQL开发中的核心应用\"><a href=\"#PDD在SQL开发中的核心应用\" class=\"headerlink\" title=\"PDD在SQL开发中的核心应用\"></a>PDD在SQL开发中的核心应用</h3><p><strong>场景分类与策略</strong></p>\n<ul>\n<li><strong>简单场景</strong>：基础查询（无需预处理数据，数据量较小等需求）可直接生成，无需过多调整</li>\n<li><strong>复杂场景</strong>：需要结构化验证和迭代优化<ul>\n<li><strong>完成标准定义</strong>(DOD)：预先明确验收标准（如输出格式、性能指标），作为初版SQL的验证基准</li>\n<li><strong>迭代优化</strong>：根据DOD识别问题（逻辑错误、边缘场景），逐步调整提示词，生成新SQL直至通过所有测试用例</li>\n<li><strong>性能调优</strong>：优先小步重构（如根据中间表逐步调整优化），避免大规模重写，降低风险并保持可读性</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"实践流程\"><a href=\"#实践流程\" class=\"headerlink\" title=\"实践流程\"></a>实践流程</h3><ul>\n<li><strong>人类输入</strong>：结构化提示（按照<a href=\"https://mp.weixin.qq.com/s?__biz=MzkyNjg3NjM1MQ==&mid=2247483680&idx=1&sn=6086e262f296b9a7af0f362dcac4a43d&scene=21#wechat_redirect\">PDD方法论</a>编写）</li>\n<li><strong>AI输出</strong>：初版SQL代码</li>\n<li><strong>验证阶段</strong>：对照DOD检查功能正确性与性能</li>\n<li><strong>迭代优化</strong>：根据问题调整提示词（如澄清模糊逻辑）</li>\n<li><strong>性能调优</strong>：结合AI建议优化（如JOIN策略、数据预处理策略等）</li>\n</ul>\n<h3 id=\"实施建议与最佳实践\"><a href=\"#实施建议与最佳实践\" class=\"headerlink\" title=\"实施建议与最佳实践\"></a>实施建议与最佳实践</h3><ol>\n<li><strong>在正式进行prompt的编写之前，明确业务需求</strong><ul>\n<li>使用文档或其他非技术性的合适方式，与业务人员达成一致</li>\n<li>定义清楚DOD，避免开发过程中频繁变更需求（非常重要，尤其是在SQL场景下，能提高验证阶段性目标的效率）</li>\n<li>建立业务梳理文档，便于处理边缘场景。如遇edge case，回顾业务梳理文档，将其添加到结构化prompt的相应任务中</li>\n</ul>\n</li>\n<li><strong>基于业务需求文档确定技术解决方案框架</strong><ul>\n<li><strong>实现方案明确时</strong>：<ul>\n<li>定义实现需求的抽象步骤</li>\n<li>根据抽象步骤，细化每个任务</li>\n<li>一个任务一个任务地调试，不必一次性全部生成</li>\n<li>当第一个任务稳定后，使用相同模式编写其余任务的提示词</li>\n</ul>\n</li>\n<li><strong>实现方案不明确时</strong>：<ul>\n<li>基于梳理好的数据查询逻辑与AI沟通，获取可行的抽象步骤</li>\n<li>结合个人理解确定最终实现方案的抽象步骤</li>\n<li>基于实现方案清晰的步骤，使用结构化提示词生成SQL代码</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>迭代优化</strong><ul>\n<li>针对不符合期望的SQL，定位相应提示词区域并补充细节</li>\n<li>结合专业推理型模型，获取SQL优化建议或实现方案的改进</li>\n<li>更新prompt记录与AI达成的一致内容，便于复用</li>\n</ul>\n</li>\n<li><strong>总体策略</strong><ul>\n<li>若对技术栈实现细节不清楚：先用自然语言描述逻辑和框架，让AI基于prompt生成SQL，调试定版后再总结更新回prompt</li>\n<li>若对实现细节非常清楚：可直接手写SQL，完成后用AI按照PDD方法论组织提示词的模式，总结实现方案更新到prompt中</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"复杂场景下的人员能力要求\"><a href=\"#复杂场景下的人员能力要求\" class=\"headerlink\" title=\"复杂场景下的人员能力要求\"></a>复杂场景下的人员能力要求</h3><p><strong>核心技能</strong></p>\n<ul>\n<li><strong>PDD理论掌握</strong>：识别并消除AI幻觉，如修正不合理的JOIN逻辑</li>\n<li><strong>需求分析能力</strong>：将模糊需求转化为精准技术规范，定位数据源，设计模块化方案等</li>\n<li><strong>结构化管理</strong>：组织上下文信息，将非结构化需求结构化，基于PDD理论编写高质量提示词</li>\n</ul>\n<p><strong>AI认知能力</strong></p>\n<ul>\n<li><strong>优势识别</strong>：快速原型设计、模式识别</li>\n<li><strong>劣势认知</strong>：边缘场景处理、上下文歧义解决</li>\n<li><strong>工具选择</strong>：技术任务用垂直领域模型，通用任务用通用AI</li>\n<li><strong>验证态度</strong>：保持乐观迭代心态，严格基于DOD和人工复查验证结果</li>\n</ul>\n<p>这套方法论强调结构化思维、迭代优化和严格验证，通过合理的人机协作，能够实现高质量的SQL开发。</p>\n<h2 id=\"总结与未来展望\"><a href=\"#总结与未来展望\" class=\"headerlink\" title=\"总结与未来展望\"></a>总结与未来展望</h2><p>通过这次PDD在数据领域的深度实践，我收获了以下五个关键洞见：</p>\n<ol>\n<li><strong>目标导向的路径规划</strong>：明确的完成标准(DOD)有助于为开发过程提供相对清晰的方向指引，在面对复杂问题时能够减少迷失的可能性。合理设定的目标框架通常能够提升问题解决的效率。</li>\n<li><strong>方法论的适应性探索</strong>：在面对全新技术领域时，PDD核心原则表现出了一定的适用性和迁移潜力。通过在不同场景中的尝试和验证，我们可以逐步积累对该方法论边界条件和适用范围的认知。</li>\n<li><strong>问题驱动的渐进式突破</strong>：面对技术挑战，或AI幻觉时，采用具体问题具体分析、逐步突破的策略往往比轻易放弃更容易获得进展。在合理的时间框架内坚持探索，并适时寻求外部支持，有助于提升问题解决的成功率。</li>\n<li><strong>效能提升的积累效应</strong>：在新技术领域的初期探索中，AI工具可能无法立即带来显著的效率提升。效能的实质性改善通常需要经历流程优化、经验模式沉淀和持续实践的积累过程。</li>\n<li><strong>个人能力增长的正向循环</strong>：AIFSD模式下，我们追求的不仅是当下的效率优化，更是通过持续实践实现个人能力的螺旋式上升，最终带来可持续性的效能提升。统一方法论的反复应用让经验积累与认知提升相互促进，最终形成人机协作的良性生态。</li>\n</ol>\n<p><img loading=\"lazy\" src=\"/../images/cognitive_improve_process_by_using_AI.png\" alt=\"cognitive_improve_process_by_using_AI.png\"></p>\n<p>通过这次实践，我们初步探索了PDD方法论在数据领域复杂SQL场景下的应用可能性，并总结出了一套在Data领域处理复杂SQL的可行路径（<a href=\"https://github.com/gszhangwei/structured-prompts-driven-development/blob/main/data/en/DATA-PROCESSING-TEMPLATE-EN.md\">点击查看模板</a>）。这为后续类似场景的工作提供了可参考的实践基础。</p>\n<p>展望未来，随着在更多实际项目中的应用和方法论的持续优化，PDD有潜力在软件开发效率和质量方面带来一定程度的改善。通过不断的实践反思和经验积累，我们期望能够逐步完善这一方法论的适用边界和操作细节。</p>\n",
            "tags": [
                "AI",
                "Prompts"
            ]
        },
        {
            "id": "https://gszhangwei.github.io/2025/04/10/AI-workflows-improve-software-development-efficiencyt-en/",
            "url": "https://gszhangwei.github.io/2025/04/10/AI-workflows-improve-software-development-efficiencyt-en/",
            "title": "Structured Prompt-Driven Development Workflow - Transforming Software Development from 2 Days to 1 Hour",
            "date_published": "2025-04-10T04:00:00.000Z",
            "content_html": "<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>Since the publication of <a href=\"https://central.thoughtworks.net/blogs/ls/content/136117101053029/structured-prompts-driven-development-1781b9ae-3d4b-458d-8b8b-0265fa34e1cb\">Structured Prompt-Driven Development in Practice</a>, the feedback I’ve received highlights <strong>two core needs</strong>: <strong>lowering the methodology application threshold</strong> and <strong>providing directly reusable practice templates</strong>.</p>\n<p>As a practitioner who has experienced the transition from traditional development to AI-assisted programming, I deeply feel that software development is currently undergoing a dual transformation: accelerating industry iteration and an efficiency revolution triggered by generative AI. When AI-generated code acceptance rates jump from 30% to 95%, this signals a fundamental paradigm shift in development.</p>\n<p>Facing the reality that methodology is <strong>“easy to understand but difficult to implement”</strong>, could we adopt a progressive model of <strong>“use-understand-innovate”</strong> to help developers naturally master methodological essentials while achieving immediate results?</p>\n<p>In fact, the <strong>universal solution layer</strong> in the structured prompt-driven development methodology can serve as the key breakthrough - <strong>building reusable abstract solution templates</strong>.</p>\n<p>[Prompts Strategy Image Placeholder]</p>\n<p>Our practicing confirms that these templated solutions can reduce the time novices need to build effective prompts <strong>from 2 hours to 30 minutes</strong>, with prompt output quality improving by over 50%.</p>\n<p>This article introduces a battle-tested AI-enhanced software development workflow that has been applied and verified in over a hundred actual development tasks. Through a systematic 5-stage, 14-step process, we’ve observed significant efficiency improvements: development tasks that traditionally required 2 days can now be completed in under 1 hour, achieving nearly 16x speed improvement in optimal cases. This workflow integrates structured prompt engineering with software development best practices, suitable for developers at all experience levels—whether you’re new to AI-assisted development or an experienced engineer seeking efficiency breakthroughs—I hope everyone can derive substantial value from it, significantly accelerating development pace while maintaining or even improving code quality.</p>\n<h2 id=\"Background-and-Tool-Ecosystem\"><a href=\"#Background-and-Tool-Ecosystem\" class=\"headerlink\" title=\"Background and Tool Ecosystem\"></a>Background and Tool Ecosystem</h2><h3 id=\"Limitations-of-Traditional-Development-Processes\"><a href=\"#Limitations-of-Traditional-Development-Processes\" class=\"headerlink\" title=\"Limitations of Traditional Development Processes\"></a>Limitations of Traditional Development Processes</h3><p>Traditional software development processes typically include requirements analysis, design, coding, testing, and deployment. Although agile methodologies have improved iteration speed, developers still need to manually complete numerous repetitive tasks: writing boilerplate code, building test cases, handling edge conditions, etc. While necessary, these tasks consume time that could be used for innovation and solving core business problems.</p>\n<h3 id=\"Core-Tools-Introduction\"><a href=\"#Core-Tools-Introduction\" class=\"headerlink\" title=\"Core Tools Introduction\"></a>Core Tools Introduction</h3><p>My AI-assisted development workflow is primarily built on the following tools:</p>\n<ul>\n<li><strong>Cursor</strong>: AI-driven code editor supporting code generation, completion, refactoring, and intelligent dialogue</li>\n<li><strong>Jira MCP Server</strong>: For obtaining and managing business requirements</li>\n<li><strong>Aupro</strong>: Our engineering practice governance platform, including MCP end, page end, and server end, enabling technology governance teams to establish and share code delivery standards so AI code assistants can generate high-quality code. (<strong>For teams without similar platforms, lightweight alternatives can be adopted</strong>: organizing structured prompt templates in local version control systems through structured folders and clear naming conventions to achieve template management, which can still yield significant benefits)</li>\n<li><strong>Various testing frameworks</strong>: Such as JUnit 5 (Jupiter), etc., working with workflows to implement automated testing</li>\n</ul>\n<p>[AI Development Process Abstract Image Placeholder]</p>\n<h2 id=\"AI-Enhanced-Software-Development-Workflow-Detailed-Analysis-of-5-Stages-and-14-Steps\"><a href=\"#AI-Enhanced-Software-Development-Workflow-Detailed-Analysis-of-5-Stages-and-14-Steps\" class=\"headerlink\" title=\"AI-Enhanced Software Development Workflow: Detailed Analysis of 5 Stages and 14 Steps\"></a>AI-Enhanced Software Development Workflow: Detailed Analysis of 5 Stages and 14 Steps</h2><h3 id=\"Requirements-Analysis-and-Planning-Stage\"><a href=\"#Requirements-Analysis-and-Planning-Stage\" class=\"headerlink\" title=\"Requirements Analysis and Planning Stage\"></a>Requirements Analysis and Planning Stage</h3><ul>\n<li><strong>Requirements Analysis and Solution Conceptualization</strong><ul>\n<li>First, retrieve historical implementations and best practices for similar tasks to establish a knowledge base</li>\n<li>Collaborate with AI to explore technical solutions, quickly evaluating feasibility and tradeoffs of different implementation paths</li>\n<li>Clearly define specific problems AI needs to solve, setting expected outcomes and success metrics</li>\n<li>This step lays the foundation for the entire development process; clear problem definition makes subsequent AI generation more precise and efficient</li>\n</ul>\n</li>\n<li><strong>Requirements Structuring</strong><ul>\n<li>Leverage AI’s natural language processing capabilities to convert business requirements into structured user stories</li>\n<li>Ensure requirement descriptions are complete and include clear acceptance criteria and expected results</li>\n<li>This step transforms abstract business concepts into clear technical tasks, reducing requirement understanding deviations</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Design-and-Solution-Definition-Stage\"><a href=\"#Design-and-Solution-Definition-Stage\" class=\"headerlink\" title=\"Design and Solution Definition Stage\"></a>Design and Solution Definition Stage</h3><ul>\n<li><strong>Implementation Solution Design</strong><ul>\n<li>Apply pre-summarized solution templates to generate structured prompts</li>\n<li>Clearly define technical path, data structures, interface definitions, and key business logic implementation details</li>\n<li>Design staged implementation strategies for complex functionalities, breaking large tasks into manageable small modules</li>\n<li>High-quality prompts are key to obtaining high-quality code output, requiring a combination of business knowledge and technical experience</li>\n</ul>\n</li>\n<li><strong>Solution Detail Adjustment and Optimization</strong><ul>\n<li>Comprehensively evaluate quality, completeness, and technical feasibility of generated prompts</li>\n<li>Analyze whether the solution considers scalability, performance, and long-term maintenance requirements</li>\n<li>Adjust implementation strategies based on business requirements and technical constraints, anticipating potential problems</li>\n<li>This step is equivalent to architecture review in traditional development, ensuring implementation direction is correct</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Code-Development-and-Implementation-Stage\"><a href=\"#Code-Development-and-Implementation-Stage\" class=\"headerlink\" title=\"Code Development and Implementation Stage\"></a>Code Development and Implementation Stage</h3><ul>\n<li><strong>Code Generation and Initial Verification</strong><ul>\n<li>Use optimized prompts to generate implementation code, including basic architecture and core functionality</li>\n<li>Perform quick validation to check if basic functions and code structure meet expectations</li>\n<li>Evaluate whether generated code follows project coding standards and best practices</li>\n</ul>\n</li>\n<li><strong>Code Review and Prompt Fine-tuning</strong><ul>\n<li>Carefully examine code quality, performance, security, and boundary handling</li>\n<li>Identify potential “code smells” and optimization points in the code</li>\n<li>Make targeted adjustments to prompts to address discovered issues, providing clear correction guidance</li>\n<li>This is an iterative optimization process, continuously improving code quality through precise prompt adjustments</li>\n</ul>\n</li>\n<li><strong>Final Code Generation</strong><ul>\n<li>Use finalized prompts to regenerate optimized implementation code</li>\n<li>Ensure code style consistency, readability, and adherence to best practices</li>\n<li>The finalized prompts incorporate optimization experiences from previous iterations, significantly improving generated code quality</li>\n</ul>\n</li>\n<li><strong>Functionality Verification</strong><ul>\n<li>Manually verify core functionality and boundary scenarios to ensure functional completeness</li>\n<li>Perform static code analysis to check code quality metrics and potential issues</li>\n<li>Verify code compatibility and integration points with existing systems</li>\n<li>The verification process checks both functional correctness and code maintainability and performance optimization space</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Testing-and-Quality-Assurance-Stage\"><a href=\"#Testing-and-Quality-Assurance-Stage\" class=\"headerlink\" title=\"Testing and Quality Assurance Stage\"></a>Testing and Quality Assurance Stage</h3><ul>\n<li><strong>Test Prompt Generation</strong><ul>\n<li>Based on solution implementation detail prompts, combined with summarized test prompt templates, generate structured test cases</li>\n<li>Ensure tests cover normal processes, boundary conditions, and exception handling scenarios</li>\n<li>Good test design can prevent future regression issues and serve as living documentation of code functionality</li>\n</ul>\n</li>\n<li><strong>Test Planning Confirmation</strong><ul>\n<li>Review completeness and appropriateness of test scenarios to ensure comprehensive test coverage</li>\n<li>Ensure tests cover not only normal situations but also exception handling and various boundary conditions</li>\n</ul>\n</li>\n<li><strong>Test Code Generation and Execution</strong><ul>\n<li>Use test prompts to generate automated test code, ensuring test code quality</li>\n<li>Execute automated tests and collect detailed results, analyzing test coverage</li>\n<li>Verify whether tests can effectively discover potential problems and boundary situations</li>\n<li>AI-generated tests are often more comprehensive than manually written tests, covering more boundary situations and exception paths</li>\n</ul>\n</li>\n<li><strong>Problem Fixing and Iteration</strong><ul>\n<li>Categorize problem causes based on test results and establish priority fix order</li>\n<li>Make targeted fixes, distinguishing handling strategies: <strong>For uncovered boundary scenarios</strong>, modify implementation code and update corresponding prompts; <strong>For test data issues</strong>, optimize test data without changing implementation logic</li>\n<li>Repeat testing until all tests pass, ensuring code quality and stability</li>\n<li>Record experiences and discoveries during the fixing process as material for prompt template optimization</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"Delivery-and-Knowledge-Management-Stage\"><a href=\"#Delivery-and-Knowledge-Management-Stage\" class=\"headerlink\" title=\"Delivery and Knowledge Management Stage\"></a>Delivery and Knowledge Management Stage</h3><ul>\n<li><strong>Integration and Submission</strong><ul>\n<li>Ensure code compatibility with existing systems, verifying integration points work properly</li>\n<li>Submit code and structured prompts solutions to facilitate team understanding of development ideas</li>\n</ul>\n</li>\n<li><strong>Knowledge Precipitation</strong><ul>\n<li>Extract effective prompt patterns and update team template library, forming reusable assets</li>\n<li>Summarize lessons learned, recording successful strategies and challenges encountered</li>\n<li>Share innovative practices and technological breakthroughs, promoting overall team capability improvement</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Core-Advantages-and-Potential-Challenges\"><a href=\"#Core-Advantages-and-Potential-Challenges\" class=\"headerlink\" title=\"Core Advantages and Potential Challenges\"></a>Core Advantages and Potential Challenges</h2><ul>\n<li><p><strong>Core Advantages:</strong></p>\n<ul>\n<li><strong>Structural Benefits</strong>: Structured prompts implement a systematic approach to code generation, creating a more organized and predictable development process while providing a stable framework for consistency.</li>\n<li><strong>Standardization Improvements</strong>: The integration of templates with AI generation techniques significantly enhances both code and test case consistency and completeness, effectively minimizing variations that typically arise from manual development.</li>\n<li><strong>Quality Assurance Framework</strong>: A comprehensive multi-level verification system—spanning from design conception through implementation to functional testing—creates a complete quality control loop that thoroughly ensures code reliability at every stage.</li>\n<li><strong>Significant Efficiency Gains</strong>: Through this optimized methodology, development efficiency achieves remarkable improvement, transforming work that traditionally required two full days into tasks completable within a single hour—representing an approximate 95% reduction in time investment.</li>\n</ul>\n</li>\n<li><p><strong>Potential Challenges:</strong></p>\n<ul>\n<li><strong>Skill Requirement</strong>: Effective prompt engineering mastery requires both technical skill depth and a persistent refinement mindset</li>\n<li><strong>Quality Standard</strong>: It is essential to ensure high-quality structured-prompt documentation and maintainable generated code</li>\n<li><strong>AI Dependency Risk</strong>: Over-reliance dependence on AI may lead to insufficient understanding of generated code</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Practical-Case\"><a href=\"#Practical-Case\" class=\"headerlink\" title=\"Practical Case\"></a>Practical Case</h2><p>Theory needs verification through practice. To better demonstrate the methodology and workflow’s actual effects, I’ve recorded a complete development example video. Welcome to watch and provide valuable suggestions.<br>[Video Placeholder]</p>\n<p>The process shown in the video is mainly summarized as follows:<br>[Sequence Diagram Placeholder]</p>\n<h2 id=\"Best-Practices-and-Experience-Summary\"><a href=\"#Best-Practices-and-Experience-Summary\" class=\"headerlink\" title=\"Best Practices and Experience Summary\"></a>Best Practices and Experience Summary</h2><p><strong>Effective prompts</strong> are the core of the entire workflow. Here are some key techniques:</p>\n<ul>\n<li><strong>Abstract first, then concrete</strong>: Start with task decomposition and framework building, rather than getting bogged down in details initially. First clearly define the abstract steps for implementing functionality and relationships between these steps, then elaborate on each abstract step according to effective prompt construction principles. This “top-down” approach ensures generated code has reasonable structure and clear layers.</li>\n<li><strong>Structured information provision</strong>: Describe solutions following structured prompt-driven development principles, clarifying code generation paths. Structured prompts not only make it easier for AI to understand development intentions but also make subsequent iterations and adjustments clearer and more controllable, laying the foundation for code quality.</li>\n<li><strong>Context association management</strong>: Include necessary association information in prompts, such as inheritance relationships between business models and call relationships between framework layers. Good context association enables AI to understand overall system architecture, generating code that conforms to established architectural styles and design philosophies, reducing later refactoring needs.</li>\n<li><strong>Prompt governance</strong>: Manage effective prompts to form a reusable template library. As projects progress, continuously accumulate and optimize prompt templates, gradually forming a prompt asset library suitable for team and project characteristics, improving long-term efficiency.</li>\n</ul>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p><strong>AI-assisted development</strong> is not just introducing efficiency tools, but <strong>a transformation of software development paradigms</strong>. Through the AI-assisted development workflow introduced in this article, developers can achieve efficient collaboration with AI, delegating repetitive work to AI while <strong>focusing</strong> on more <strong>creative</strong> and <strong>strategic</strong> tasks.</p>\n<p>Practice has proven that using this workflow, after becoming proficient, <strong>development efficiency can improve by at least 3 times</strong> while maintaining or even improving code quality. With continuous advancement of AI technology, there’s even more room for efficiency improvement. For individual developers and teams, now is the best time to embrace this transformation.</p>\n<p>Mastering AI-assisted development is not about replacing developers, but redefining how we work. The real value lies in human-machine collaboration—AI handles execution and assistance, while humans handle innovation and decision-making. Future top developers will be compound talents who both master technology itself and can effectively guide AI to complete tasks.</p>\n<h2 id=\"Some-Thoughts-on-the-Essence-of-Code\"><a href=\"#Some-Thoughts-on-the-Essence-of-Code\" class=\"headerlink\" title=\"Some Thoughts on the Essence of Code\"></a>Some Thoughts on the Essence of Code</h2><p>This transformation also prompts me to rethink the essence of code: <strong>Code itself is a symbolic expression of human intent, constantly evolving with technological development.</strong> From punched paper tapes to assembly language, to high-level languages like C++, Python, Java, etc., each technological innovation has expanded the boundaries of <strong>“what is code.”</strong> Today, as AI can precisely understand human language and intent through specific methods, <strong>natural language programming</strong> becomes a <strong>new stage</strong> in programming paradigms’ natural evolution—not replacing traditional programming but extending it.</p>\n<p>Different programming paradigms each have their value and scenarios, just as in transportation’s historical evolution, new technology doesn’t mean complete elimination of old technology. Forward-thinking developers will be inclusive—mastering traditional programming’s rigorous structure while using that experience to explore more possibilities in prompt engineering, perhaps even developing more direct human-machine interaction methods in the future. In technological change waves, understanding essential principles behind technology and maintaining an open learning attitude are far more important than adhering to specific implementation forms—<strong>Code forms will continue to iterate, while the spirit of innovation remains constant.</strong></p>\n<p>Let us actively embrace this transformation, rethink the essence of software development, and create greater value in the new era of human-machine collaboration.</p>\n<h2 id=\"Appendix\"><a href=\"#Appendix\" class=\"headerlink\" title=\"Appendix\"></a>Appendix</h2><p>As a practical tool for this methodology, we provide a set of battle-tested prompt templates covering API CRUD operations and accompanying test strategies (click this link to view the complete template library). These templates can not only be used directly in daily development but also serve as examples for learning structured prompt engineering.</p>\n<p>We sincerely invite all developers to innovate and expand based on their own project needs while using these basic templates, jointly promoting the continuous evolution of this methodology.</p>\n",
            "tags": [
                "AI",
                "Prompts"
            ]
        },
        {
            "id": "https://gszhangwei.github.io/2025/03/21/AI-workflows-improve-software-development-efficiencyt/",
            "url": "https://gszhangwei.github.io/2025/03/21/AI-workflows-improve-software-development-efficiencyt/",
            "title": "AI-Workflow革命：2天任务仅需50分钟！",
            "date_published": "2025-03-21T09:00:00.000Z",
            "content_html": "<h2 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a><strong>引言</strong></h2><p>自<a href=\"https://mp.weixin.qq.com/s/Hade1vZDIRMwbTyZxB0-Hg\">《结构化提示词驱动开发实践》</a>发布以来，我收到的反馈中有两个核心诉求格外突出：降低方法论应用门槛和提供可直接复用的实践模板。<br>作为亲历从传统开发到AI辅助编程转变的实践者，我深感当下软件开发正经历双重变革：行业迭代加速与生成式AI引发的效率革命。当AI生成代码的通过率从30%跃升至95%，这预示着开发范式的根本性转变。<br>面对方法论”知易行难”的现实，是否可以通过”<strong>使用-理解-创新</strong>“的渐进模式，让开发者在获得即时成果的同时自然习得方法论精髓？<br>其实，在《结构化提示词驱动开发实践》方法论中的<strong>通用方案层</strong>恰好可作为破局关键 - <strong>构建可重复使用的抽象的解决方案模板</strong>。<br><br><img loading=\"lazy\" src=\"/../images/Prompts_engineering_strategies.png\" alt=\"Prompts_engineering_strategies.png\"><br><br>实测表明，这种模板化方案将新手<strong>构建有效Prompt</strong>的时间<strong>从2小时缩短至30分钟</strong>，Prompt输出质量提升50%以上。<br>本文将分享一套经过50余个开发任务验证的AI增强工作流，通过5阶段14个精炼步骤，将传统开发中耗时2天的任务最短压缩至55分钟内完成，效率最高提升16倍。无论你是AI开发新人还是资深工程师，希望这套融合结构化思维与AI辅助的工作流都能为你开启效率新维度。</p>\n<h2 id=\"背景与工具生态\"><a href=\"#背景与工具生态\" class=\"headerlink\" title=\"背景与工具生态\"></a><strong>背景与工具生态</strong></h2><h3 id=\"传统开发流程的局限性\"><a href=\"#传统开发流程的局限性\" class=\"headerlink\" title=\"传统开发流程的局限性\"></a><strong>传统开发流程的局限性</strong></h3><p>传统软件开发流程通常包含需求分析、设计、编码、测试和部署等环节。尽管敏捷方法论提高了迭代速度，但开发者仍需手动完成大量重复性工作：编写样板代码、构建测试用例、处理边界条件等。这些任务虽然必要，却占用了大量本可用于创新和解决核心业务问题的时间。</p>\n<h3 id=\"核心工具介绍\"><a href=\"#核心工具介绍\" class=\"headerlink\" title=\"核心工具介绍\"></a><strong>核心工具介绍</strong></h3><p>我的AI辅助开发工作流主要基于以下工具构建：</p>\n<ul>\n<li><strong>Cursor</strong>：AI驱动的代码编辑器，支持代码补全、重构和智能对话</li>\n<li><strong>Jira MCP Server</strong>：获取和管理业务需求的项目管理工具</li>\n<li><strong>Aupro</strong>：我们构建的工程实践治理平台，使技术治理团队能够制定和共享代码交付标准，从而使AI代码助手能够生成高质量的代码。（如果暂时没有这样的平台也不要紧，可以手动将模板放到本地管理起来）</li>\n<li><strong>各类测试框架</strong>：例如JUnit 5 (Jupiter)等，配合工作流实现自动化测试</li>\n</ul>\n<h3 id=\"AI增强软件开发工作流方案详解\"><a href=\"#AI增强软件开发工作流方案详解\" class=\"headerlink\" title=\"AI增强软件开发工作流方案详解\"></a><strong>AI增强软件开发工作流方案详解</strong></h3><p>这套AI增强软件开发工作流是一个系统化的14步流程，将AI工具融入软件开发的各个环节，从技术需求分析到代码提交。通过实践证明，这套工作流可以将原本需要两天完成的工作量（如创建一个根据给出的动态查询条件，进行资源分页查询API及其完整测试场景）压缩到1小时内完成，效率提升显著。抽象的步骤如下：<br><br><img loading=\"lazy\" src=\"/../images/AI_workflow.png\" alt=\"AI_workflow.png\"></p>\n<h3 id=\"AI增强软件开发工作流：5阶段14步骤详细解析\"><a href=\"#AI增强软件开发工作流：5阶段14步骤详细解析\" class=\"headerlink\" title=\"AI增强软件开发工作流：5阶段14步骤详细解析\"></a><strong>AI增强软件开发工作流：5阶段14步骤详细解析</strong></h3><ul>\n<li><p><strong>需求分析与规划阶段</strong></p>\n<ul>\n<li>需求分析与解决方案构思<ul>\n<li>首先检索相似任务的历史实现和最佳实践，建立知识基础</li>\n<li>与AI协作探索技术方案，快速评估不同实现路径的可行性和优劣</li>\n<li>明确定义AI需要解决的具体问题，设定预期成果和成功指标</li>\n<li>这一步奠定了整个开发过程的基础，清晰的问题定义让后续AI生成更精准高效</li>\n</ul>\n</li>\n<li>需求结构化<ul>\n<li>利用AI的自然语言处理能力将业务需求转换为结构化用户故事</li>\n<li>确保需求描述完整且包含明确的验收标准和预期结果</li>\n<li>这一步将抽象业务概念转化为明确的技术任务，减少需求理解偏差</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>设计与方案定义阶段</strong></p>\n<ul>\n<li><p>实现方案设计</p>\n<ul>\n<li>应用预先总结的方案模板，生成结构化prompts</li>\n<li>明确定义技术路径、数据结构、接口定义和关键业务逻辑实现细节</li>\n<li>为复杂功能设计分层实现策略，将大任务分解为可管理的小模块</li>\n<li>高质量的prompt是获得高质量代码输出的关键，需结合业务知识和技术经验设计</li>\n</ul>\n</li>\n<li><p>方案细节调整与优化</p>\n<ul>\n<li>全面评估生成的prompts质量、完整性和技术可行性</li>\n<li>分析方案是否考虑了可扩展性、性能和长期维护需求</li>\n<li>根据业务需求和技术限制调整实现策略，预见潜在问题</li>\n<li>这一步相当于传统开发中的架构评审，确保实现方向正确无误</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>代码开发与实现阶段</strong></p>\n<ul>\n<li><p>代码生成与初步验证</p>\n<ul>\n<li>利用优化后的prompts生成实现代码，包括基础架构和核心功能</li>\n<li>通过快速验证检查基本功能和代码结构是否符合预期</li>\n<li>评估生成代码是否遵循项目编码规范和最佳实践</li>\n</ul>\n</li>\n<li><p>代码审核与prompt微调</p>\n<ul>\n<li>细致检查代码质量、性能、安全性和边界处理</li>\n<li>识别代码中的潜在”代码异味”(Code Smells)和可优化点</li>\n<li>针对性调整prompt以解决发现的问题，进行明确的修正指导</li>\n<li>这是一个迭代优化过程，通过prompt精确调整不断提高代码质量</li>\n</ul>\n</li>\n<li><p>代码最终生成</p>\n<ul>\n<li>使用定稿prompts重新生成优化后的实现代码</li>\n<li>确保代码风格一致性、可读性和最佳实践遵循</li>\n<li>验证代码是否包含适当的注释和文档化</li>\n<li>定稿的prompt包含前几轮迭代中的优化经验，生成的代码质量显著提高</li>\n</ul>\n</li>\n<li><p>功能验证</p>\n<ul>\n<li>手动验证核心功能和边界场景，确保功能完整性</li>\n<li>执行静态代码分析，检查代码质量指标和潜在问题</li>\n<li>验证代码与现有系统的兼容性和集成点</li>\n<li>验证过程既检查功能正确性，也关注代码可维护性和性能优化空间</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>测试与质量保障阶段</strong></p>\n<ul>\n<li><p>测试设计</p>\n<ul>\n<li>基于<strong>解决方案实现细节prompts</strong>，结合总结的测试Prompt模板，生成结构化测试用例</li>\n<li>确保测试覆盖正常流程、边界条件和异常处理场景</li>\n<li>良好的测试设计能防止未来的回归问题，并作为代码功能的活文档</li>\n</ul>\n</li>\n<li><p>测试规划确认</p>\n<ul>\n<li>审核测试场景的完整性和适当性，确保测试范围全面</li>\n<li>确保测试不仅覆盖正常情况，也包含异常处理和各种边界条件</li>\n</ul>\n</li>\n<li><p>测试代码生成与执行</p>\n<ul>\n<li>使用测试prompts生成自动化测试代码，确保测试代码质量</li>\n<li>执行自动化测试并收集详细结果，分析测试覆盖率</li>\n<li>验证测试是否能有效发现潜在问题和边界情况</li>\n<li>AI生成的测试通常比手写测试更全面，能覆盖更多边界情况和异常路径</li>\n</ul>\n</li>\n<li><p>问题修复与迭代</p>\n<ul>\n<li>根据测试结果分类问题原因，建立优先修复顺序</li>\n<li>针对性进行修复，区分处理策略：<ul>\n<li>对于未覆盖的边界场景，修改实现代码并更新对应prompts</li>\n<li>对于测试数据问题，优化测试数据而不更改实现逻辑</li>\n</ul>\n</li>\n<li>重复测试直至全部通过，确保代码质量和稳定性</li>\n<li>记录修复过程中的经验和发现，作为prompt模版优化的素材</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>交付与知识管理阶段</strong></p>\n<ul>\n<li><p>集成与提交</p>\n<ul>\n<li>确保代码与现有系统完全兼容，验证所有集成点正常工作</li>\n<li>提交代码和结构化Prompts方案，便于团队理解开发思路</li>\n</ul>\n</li>\n<li><p>知识沉淀</p>\n<ul>\n<li>提炼有效prompt模式并更新团队模板库，形成可复用资产</li>\n<li>总结经验教训，记录成功策略和遇到的挑战</li>\n<li>分享创新实践和技术突破，促进团队整体能力提升</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"核心优势与潜在挑战\"><a href=\"#核心优势与潜在挑战\" class=\"headerlink\" title=\"核心优势与潜在挑战\"></a>核心优势与潜在挑战</h2><ul>\n<li><strong>核心优势：</strong><ul>\n<li>结构化流程使开发过程更加条理化和可预测</li>\n<li>利用模板和AI生成提高了代码和测试的一致性和完整性</li>\n<li>多层次验证确保代码质量，从设计到实现到测试全覆盖</li>\n<li>显著提升效率，能将两天工作量缩减至一小时内完成</li>\n</ul>\n</li>\n<li><strong>潜在挑战：</strong><ul>\n<li>工作流效果很大程度依赖于AI响应的质量和精准度（目前claude-3.7-sonnet完全够用）</li>\n<li>过度依赖AI可能导致对生成代码理解不足</li>\n<li>Prompt工程需要专业技能和持续调优</li>\n<li>需要注意生成代码的文档质量和可维护性</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"实践案例\"><a href=\"#实践案例\" class=\"headerlink\" title=\"实践案例\"></a>实践案例</h2><p>理论终需实践验证。为了直观展示方法论的实际效果，我录制了完整的开发实例视频，欢迎各位观摩并提出宝贵意见。<br><a href=\"\">点击观看视频演示</a>。视频中所展示的流程，整体如下所示：<br></p>\n<div style=\"text-align: center;\"><img loading=\"lazy\" src=\"../images/AI_workflow_sequence_diagram.png\" alt=\"AI_workflow_sequence_diagram\"></div>\n<div style=\"text-align: center;\"><span style=\"font-size: x-small; color: grey; \">Workflow关键的工作时序</span></div>\n\n<h2 id=\"最佳实践与经验总结\"><a href=\"#最佳实践与经验总结\" class=\"headerlink\" title=\"最佳实践与经验总结\"></a>最佳实践与经验总结</h2><p>有效的prompt是整个工作流的核心，以下是一些关键技巧：</p>\n<ul>\n<li><strong>先抽象再具体</strong>：从框架搭建开始，而非一开始就陷入细节。先定义清楚实现功能的抽象步骤以及各步骤之间的关联关系，然后再根据这些抽象步骤，以有效构建提示词的指导思想，逐个进行细化。这种”自顶向下”的方法可以确保生成的代码结构合理、层次分明。</li>\n<li><strong>结构化信息提供</strong>：以结构化提示词驱动开发的指导思想，描述解决方案，明确代码生成路径。结构化的prompt不仅使AI更易理解开发意图，也使后续的迭代和调整更加清晰可控，为代码质量奠定基础。</li>\n<li><strong>上下文关联管理</strong>：在prompt中包含必要的关联信息，如业务建模之间的继承关系，使用的框架各层之间的调用关系等。良好的上下文关联使AI能够理解系统整体架构，生成符合既定架构风格和设计理念的代码，减少后期重构需求。</li>\n<li><strong>版本控制</strong>：对有效prompt进行管理，形成可复用的模板库。随着项目进展，不断积累和优化prompt模板，逐步形成适合团队和项目特点的prompt资产库，提升长期效率。</li>\n</ul>\n<h2 id=\"未来展望\"><a href=\"#未来展望\" class=\"headerlink\" title=\"未来展望\"></a>未来展望</h2><p>未来，我们将重点优化以下两个方面，充分发挥 AI 的能力：首先是提升 Workflow 的自动化程度；其次是构建 Prompt 自进化体系。该体系不仅依靠人工总结，更侧重于 AI 的智能学习，使其能基于优质 Prompt 实现方案，定期生成高质量模板。</p>\n<ul>\n<li><strong>自动化程度提升</strong>：通过自动化开发流程中的更多环节，例如基于需求自动创建 Jira Story、触发自动化代码审查等，来减少手动操作，提高流程一致性和效率。</li>\n<li><strong>Prompt自进化体系</strong>：建立基于强化学习的 Prompt 模板演进体系，使 AI 能够学习现有模板库中的成功模式，并根据用户反馈和需求变化，自动优化模板并生成新的解决方案，形成持续改进的良性循环。</li>\n</ul>\n<h2 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h2><p>AI辅助开发不仅是效率工具引入，更是软件开发范式的转变。通过本文介绍的AI辅助开发工作流，开发者可以实现与AI的高效协作，将重复性工作交由AI处理，而自身则专注于更具创造性和战略性的任务。<br>实践证明，采用这套工作流可以将开发效率提升至少3倍，同时保持甚至提高代码质量。随着AI技术的持续进步，这种效率提升还有更大空间。对于个人开发者和团队而言，现在正是拥抱这一变革的最佳时机。<br>掌握AI辅助开发不是要取代开发者的角色，而是重新定义我们的工作方式。真正的价值在于人机协作——AI负责执行和辅助，人类负责创新和决策。未来的顶尖开发者将是那些既精通技术本身，又能有效指导AI完成任务的复合型人才。<br>让我们积极拥抱这一变革，重新思考软件开发的本质，并在人机协作的新时代中创造更大的价值。</p>\n<h2 id=\"最后，对于代码本质的一些思考\"><a href=\"#最后，对于代码本质的一些思考\" class=\"headerlink\" title=\"最后，对于代码本质的一些思考\"></a>最后，对于代码本质的一些思考</h2><p>代码的本质是人类意图的符号化表达，它是随着技术浪潮不断演化的。从早期的打孔纸带到汇编语言，从C++到Python、Java，每一次技术革新都拓展了我们对”何为代码”的理解边界。自然语言编程并非对传统编程的替代，而是编程范式自然演进的延伸——当AI能够精准理解人类意图与语境需求时，代码的表达方式自然而然地向更接近人类思维的形态靠拢。<br>不同的编程范式各有其价值与适用场景，就如同历史上各种交通工具的演进一样，新技术的出现并不意味着旧技术的全面淘汰。富有远见的开发者往往能够兼容并蓄：精通传统编程的严谨结构，同时探索prompt工程的无限可能，未来或许还能开拓更直接的人机交互方式。在这技术变革的洪流中，洞悉技术背后的本质原理与保持开放的学习心态，远比拘泥于某种具体实现形式更为宝贵——因为代码的形态会不断迭代，而创新的精神却是恒久不变的。</p>\n<h2 id=\"附录\"><a href=\"#附录\" class=\"headerlink\" title=\"附录\"></a>附录</h2><p>本文收录了，基于结构化提示词驱动开发方法论，创建的API相关CRUD操作以及测试模板集（<a href=\"https://github.com/gszhangwei/structured-prompts-driven-development/tree/main\"><strong>点击这个链接</strong></a>），这些模板已经过实践验证并可直接应用于日常开发。诚挚地邀请各位开发者在使用这些基础模板的同时，根据自身项目需求进行创新与扩展，共同推动这一方法论的持续演进。</p>\n",
            "tags": [
                "AI",
                "Prompts"
            ]
        },
        {
            "id": "https://gszhangwei.github.io/2025/02/27/prompts-driven-development/",
            "url": "https://gszhangwei.github.io/2025/02/27/prompts-driven-development/",
            "title": "结构化提示词驱动开发实践",
            "date_published": "2025-02-27T09:00:00.000Z",
            "content_html": "<p>上周六参加了公司组织的关于AI实践的对外直播，我分享的内容是《结构化提示词驱动开发实践》。现在将其记录成一篇博客，在此与大家分享我们团队在提示词驱动开发领域的一些实践与思考。随着大语言模型的不断成熟，我们逐步认识到，如何高效运用结构化提示词，引导AI生成高质量代码，已成为提升软件开发效率与质量的关键所在。本文将围绕“<strong>结构化提示词驱动开发</strong>”这一主题，从设计理念、实践路径到治理策略，全面解析提示词在软件开发中如何落地应用，并以数据与实例展示其显著成效。</p>\n<h2 id=\"解锁解决方案：克服关键挑战\"><a href=\"#解锁解决方案：克服关键挑战\" class=\"headerlink\" title=\"解锁解决方案：克服关键挑战\"></a>解锁解决方案：克服关键挑战</h2><p>在早期的AI辅助开发过程中，我们曾经深刻体会到传统开发模式的诸多局限。常规的对话式代码生成往往存在如下问题：生成的代码可用性不足、质量参差不齐，以至于后续大量人工返工成为常态；同时，对话式生成方案不易形成固化、可复用的工程产物，制约了AI在工程级项目中的大规模应用。数据显示，AI生成代码的采纳率普遍低于50%，而代码质量的波动也使得实际交付面临诸多风险。</p>\n<p>正因如此，我们开始思考：如果能够突破这些障碍，充分激发AI在交付过程中的潜能，那么软件开发的方式将会迎来怎样的变革？经过反复探索与实践，我们认为解决这一问题需要三大能力作支撑：</p>\n<ul>\n<li>第一是迅速分析问题，并精准识别出根本原因，从而为后续提供有效解决方案；</li>\n<li>第二是依托严谨的工程实践，通过标准流程来保障代码质量；</li>\n<li>第三也是最为关键的，即通过使用结构化提示词，让AI生成的代码具备可解释性、可追溯性和高采纳率。<br><img loading=\"lazy\" src=\"/../images/Challenge_and_Solution.png\" alt=\"Challenge_and_Solution.png\"><br>正是这三大支柱构成了我们突破传统模式、推动AI赋能开发的核心思路，也为后续实践提供了坚实的理论基础和指导方向。</li>\n</ul>\n<h2 id=\"试点项目探索与验证\"><a href=\"#试点项目探索与验证\" class=\"headerlink\" title=\"试点项目探索与验证\"></a>试点项目探索与验证</h2><p>为了验证这一方法论的可行性，我们开展了一系列试点项目，尽管具体业务细节不便透露，但整体项目任务包括了前端服务的初始化、基础设施及持续集成（CI）的搭建、前后端代码与测试的新编写，以及现有前端系统的重构。项目交付后，通过对数据的详细采集与对比分析，我们获得了非常振奋的成果。</p>\n<p>例如，在从零开始构建系统的任务中，传统开发模式需要消耗约19人天，而采用结构化提示词模式后，仅耗时7人天，且代码采纳率高达95%，这不仅大幅缩短了开发周期，也使得生产力实现近3倍的提升。而针对遗留系统的需求响应，开发时间则由原先的5人天缩短至3人天，生产效率提升约1.7倍。此外，我们还观察到，通过这种模式，代码重复率显著降低，单元测试覆盖率由65%增加至96%。这些数据充分说明，结构化提示词不仅提高了交付效率，更在实际工程中提升了代码质量和系统稳定性。<br><img loading=\"lazy\" src=\"/../images/Background_of_Project.png\" alt=\"Background_of_Project.png\"><br>数据背后的意义在于，我们成功为AI赋能软件开发找到了一条有效路径，这种路径打破了传统开发方式的局限，将原本模糊难控的生成过程转化为可管理、可复制的工程实践，为整个团队带来了崭新的开发体验和惊人的效率提升。</p>\n<p>让我们先探讨如何在软件开发领域中构建有效提示词，再逐步回顾我们所做的具体实践。</p>\n<h2 id=\"结构化提示词设计框架\"><a href=\"#结构化提示词设计框架\" class=\"headerlink\" title=\"结构化提示词设计框架\"></a>结构化提示词设计框架</h2><h3 id=\"如何有效构建提示词？\"><a href=\"#如何有效构建提示词？\" class=\"headerlink\" title=\"如何有效构建提示词？\"></a>如何有效构建提示词？</h3><p>在软件开发领域利用自然语言驱动AI生成代码过程中，一个根本性矛盾一直存在：人类思维具有天然的发散性，而AI执行指令则要求极高的结构化和精确性。传统提示词设计容易陷入两种误区：一方面，开发者可能过分依赖表面化描述，如直接要求“生成800字带小标题的营销文案”，这种方式虽然直白，但往往流于形式；另一方面，则可能陷入抽象概念的模糊描述，让AI自行揣测具体意图，从而导致输出偏离预期目标。</p>\n<p>为突破这一矛盾，我们强调思维方式的跃迁，即要从简单的特征描述（例如颜色、形状等外在属性）转向对事物本质的抽象提炼（例如功能内核与运行机制）。只有当我们首先明确定义对象的核心功能与本质特性，再辅以具体细节描述，才能为AI提供一条唯一且明确的生成路径，最大程度降低AI的幻觉。例如，在需要生成一个“白色冰箱”的场景中（假设我们并不知道冰箱这一名词概念），如果仅以“生成一个四方形白色物体，下方有四个小轮子”进行描述，可能误导AI产生与冰箱无关的对象；而若从本质出发，先定义“维持低温环境”这一核心功能，然后再补充其它特征，便能精准锁定冰箱这一概念，同时也为其它如冷库、冷链车留下一定扩展空间。正如我们所倡导的，“特征决定细节，本质决定边界”，只有先明晰本质，后注重具体细节，才能确保生成结果符合预期。<br><img loading=\"lazy\" src=\"/../images/Construct_prompts_effectively.png\" alt=\"Construct_prompts_effectively.png\"></p>\n<h3 id=\"组件描述法在提示词构建中的实践\"><a href=\"#组件描述法在提示词构建中的实践\" class=\"headerlink\" title=\"组件描述法在提示词构建中的实践\"></a>组件描述法在提示词构建中的实践</h3><p>基于上述理念，在构建提示词时我们引入了组件描述法。以后端开发为例，我们为每个组件设计了涵盖类名、方法名、异常处理等在内的多达10个标准化维度。通过这种方法，每个组件不仅能够明确界定其功能边界，也实现了逻辑上的严谨封装。需要说明的是，这里的“组件”概念与传统开发框架中的概念有所不同，它指的是构成结构化提示词的基本单元，是对功能职责和实现过程的细致拆解。<br><img loading=\"lazy\" src=\"/../images/Component_Description.png\" alt=\"Component_Description.png\"><br>组件描述法的引入，使得每个提示词单元既自成体系又相互衔接，在明确各自核心职责的同时，通过精准定义其属性和操作范围，有效避免了各组件之间的混淆和重叠，保障了整体提示词的严密性和生成代码的一致性。这样的设计不仅极大增强了系统的可维护性，也为处理复杂业务场景提供了有力支撑。</p>\n<h3 id=\"结构化Prompt设计整体框架\"><a href=\"#结构化Prompt设计整体框架\" class=\"headerlink\" title=\"结构化Prompt设计整体框架\"></a>结构化Prompt设计整体框架</h3><p>在组件定义的基础上，我们进一步构建了完整的结构化Prompt设计框架，该框架总体分为以下五个部分：</p>\n<p>需求锚定阶段要求我们准确描述业务需求，确保开发目标的精准定位；结构定义阶段则负责明确实现功能所需模块之间的依赖关系和相互作用；任务编排阶段将整体需求拆解成具体操作单元，通过逐一定义各组件来形成连续的工作流；在通用任务阶段，我们对数据校验、异常处理等高频操作进行标准化处理，以模板化的方式复用解决方案；最后，约束控制阶段为整个系统设置安全围栏，限定组件调用的范围和引用关系，防止因边界不清引发的潜在问题。</p>\n<p>在此过程中，我们始终坚持两个黄金准则：首先，本质抽象必须优先于特征描述，只有清晰定义组件的核心职责，才能避免陷入形式化穷举的困境；其次，组件应被视为逻辑单元，而非仅仅是代码片段，就如一部优秀剧本强调刻画人物内在动机，而非干涉演员细微表现。正是在这两个准则的指引下，结构化Prompt设计框架为团队构建了一套系统、严谨且高效的提示词应用体系，极大地提升了整体开发效率和代码质量。<br><img loading=\"lazy\" src=\"/../images/Structured_prompts_strategy.png\" alt=\"Structured_prompts_strategy.png\"></p>\n<h2 id=\"提示词工程：AI资产的治理策略\"><a href=\"#提示词工程：AI资产的治理策略\" class=\"headerlink\" title=\"提示词工程：AI资产的治理策略\"></a>提示词工程：AI资产的治理策略</h2><p>要确保提示词在长期开发过程中的持续有效性，仅仅构建高质量提示词是不够的，还需要建立一套完善的治理策略。我们将提示词治理分为多个层级：规则层、行业垂直方案抽象层、解决方案具体实现层以及AI执行层。各层级之间分工明确，架构师、技术负责人与开发工程师各自履行职责，共同维护提示词资产的高标准和可持续性。<br><img loading=\"lazy\" src=\"/../images/prompts_governance_policy.png\" alt=\"prompts_governance_policy.png\"><br>目前，在试点项目中，我们已经通过预定义工作流引入日志规则，将其置于规则层，确保在代码生成前通过提示词预先注入必要的安全和质量标准。这不仅使得生成代码符合日志规范，同时也为整个系统构建起一道坚固的防护屏障。随着实施的不断推进，这一治理策略将逐步完善并推广到更广泛的开发场景中，为软件工程在质量控制和流程管理上提供制度化保障。</p>\n<h2 id=\"项目实践实例解析\"><a href=\"#项目实践实例解析\" class=\"headerlink\" title=\"项目实践实例解析\"></a>项目实践实例解析</h2><p>为了更直观地展示结构化提示词的应用效果，我们以一个获取用户权限信息的API项目为例进行说明。该API不仅需支持根据用户ID、邮箱、姓名及角色进行多条件查询，还要求具备分页、排序以及对数据进行分组合并的功能。在实际开发中，我们依照需求、结构、任务、通用任务与约束控制等模块，系统地组织和编写提示词，同时在组件定义中严格遵循组件描述法和构建提示词的核心指导思想。依托这一完备的体系，我们最终实现了一个能够根据多参数动态构建查询条件，并对数据进行复杂分组与合并操作的API。</p>\n<p>统计结果显示，在后端实现API模块，团队共使用6组提示词，总行数达592行，最终生成代码1849行，涉及37个文件的新增或修改。如此覆盖复杂业务场景的实践成果，不仅充分验证了结构化提示词模式的可行性，也大大提升了交付效率和代码质量，为传统开发模式带来了全新的变革思路。<br><img loading=\"lazy\" src=\"/../images/Examples_of_project_practices.png\" alt=\"Examples_of_project_practices.png\"><br>在项目推进过程中，我们也直面了一些实际问题，并据此积累了宝贵经验。尤其对于初次接触该模式的团队而言，我们发现并不需要从零开始书写提示词，而可以从现有代码中提炼有效提示，从而逐步优化和完善提示体系，实现快速落地和迭代。</p>\n<h2 id=\"有效撰写提示词的技巧与实战建议\"><a href=\"#有效撰写提示词的技巧与实战建议\" class=\"headerlink\" title=\"有效撰写提示词的技巧与实战建议\"></a>有效撰写提示词的技巧与实战建议</h2><p>通过多次项目实践，我们总结出如下几种撰写高质量提示词的有效策略。首先，当解决方案已经存在于现有代码中时，可以利用AI自动解析代码，提取出有效提示词，并在此基础上进行优化，这种做法既节约了时间，也避免了从零开始的风险。其次，当解决方案尚存于开发者脑海时，借助可复用的结构化策略，并根据不同任务灵活调整细节，能迅速形成符合预期的提示词。对于那些暂时无解的情况，我们建议先与AI展开开放性协作探索，通过不断收敛思路后，借助结构化模板生成初步提示词，再由开发者进行必要的人工微调和确认。</p>\n<p>在实践中我们还发现，写作提示词时应特别注意避免过度抽象。对于简单场景，往往无需构建复杂的提示体系，此时采用直接的对话式编程反而更为高效。此外，提示词的撰写要做到本质与特征兼顾——既要确保功能和职责的清晰表述，又不能忽略细节描述的重要性。最关键的是，无论是为了让AI生成优秀代码，还是为了便于开发者后期审核修正，最终形成的提示词都必须具备良好的可读性和实用性，能够以清晰自然的语言传达预期意图。<br><img loading=\"lazy\" src=\"/../images/Advice_of_practice.png\" alt=\"Advice_of_practice.png\"></p>\n<h2 id=\"结语\"><a href=\"#结语\" class=\"headerlink\" title=\"结语\"></a>结语</h2><p>总体而言，提示词驱动开发作为一种全新的软件开发模式，展现出前所未有的创新潜力。凭借结构化提示词的系统设计与严格治理策略，我们不仅成功激发了AI在代码生成中的潜能，还显著提升了开发效率和代码质量，为我们团队引入了全新的思考方式和工作模式。</p>\n<p>展望未来，我们将持续关注AI技术的前沿动态，并在更为复杂的实际场景下不断完善提示词设计与治理体系。我们期待，在广大开发者不断探索和实践的推动下，人与AI的深度融合能够开启软件开发的新纪元，共同推动整个行业迈向更高效、更智能的未来。通过不断迭代与优化，相信结构化提示词必将成为工程实践中的一项核心技术，为软件开发注入源源不断的创新动力和竞争优势。</p>\n<h2 id=\"附录\"><a href=\"#附录\" class=\"headerlink\" title=\"附录\"></a>附录</h2><p>这篇文章在外部发表后，大家围绕“特征决定细节，本质决定边界”展开了一些讨论。但我发现，大家对“特征”和“本质”的基本定义可能存在不同理解。为此，我在这里补充了它们的基本定义，以便统一认知，从而帮助大家更好地理解这句话的深层含义。<br>特征（Features）：指问题或需求中可观测、可描述的具体属性，包括输入输出形式、约束条件、交互场景、技术参数等显性要素。核心特点：可量化，可验证，具象化（容易观察到的）。<br>本质（Essence）：指问题背后需要解决的根本矛盾或核心目标，是决定解决方案有效性的底层逻辑。核心特点：抽象性，方向性，不可妥协性（那个能够决定成为它的东西）。</p>\n",
            "tags": [
                "AI",
                "Prompts"
            ]
        },
        {
            "id": "https://gszhangwei.github.io/2025/02/27/prompt-driven-development-en/",
            "url": "https://gszhangwei.github.io/2025/02/27/prompt-driven-development-en/",
            "title": "Structured prompts driven development",
            "date_published": "2025-02-27T09:00:00.000Z",
            "content_html": "<p>Recently, I took part in an external livestream hosted by our company in China, where I presented on the topic of ‘Structured Prompt-Driven Development Practice. I am now recording this experience as a blog post to share some of our team’s practices and reflections on prompt-driven development. As large language models continue to mature, we are increasingly realizing that efficiently leveraging structured prompts to guide AI in generating high-quality code is key to improving both software development efficiency and quality. This article, centered on <strong>Structured Prompt-Driven Development</strong>, comprehensively analyzes how prompts can be applied in software development—from design philosophy and practical implementation to governance strategies—supporting our discussion with data and practical examples.</p>\n<h2 id=\"Unlocking-Solutions-Overcoming-Key-Challenges\"><a href=\"#Unlocking-Solutions-Overcoming-Key-Challenges\" class=\"headerlink\" title=\"Unlocking Solutions: Overcoming Key Challenges\"></a>Unlocking Solutions: Overcoming Key Challenges</h2><p>In the early stages of AI-assisted development, we experienced firsthand the many limitations of conventional development models. The typical conversational code generation approach often suffers from several issues: the generated code frequently lacks usability and exhibits uneven quality, leading to extensive manual rework; moreover, conversational generation methods do not readily produce fixed, reusable engineering deliverables, which limits the large-scale application of AI in enterprise-level projects. Data shows that the adoption rate of AI-generated code is generally below 50%, and the fluctuations in code quality pose significant risks for actual delivery.</p>\n<p>This led us to ponder: if we could overcome these obstacles and fully harness the potential of AI during the delivery process, how might software development transform? After extensive exploration and practical trials, we determined that addressing these challenges requires three core capabilities:</p>\n<ul>\n<li>The ability to quickly analyze problems and accurately identify root causes, thereby providing effective subsequent solutions.</li>\n<li>A reliance on rigorous engineering practices that ensure code quality through standardized processes.</li>\n<li>Most critically, employing structured prompts that enable AI to generate code which is interpretable, traceable, and highly adoptable.</li>\n</ul>\n<p>Challenge_and_Solution.png</p>\n<p>These three pillars form the core of our breakthrough from traditional methodologies and drive our AI-empowered development approach, providing a solid theoretical foundation and clear guidance for our subsequent practices.</p>\n<h2 id=\"Pilot-Project-Exploration-and-Validation\"><a href=\"#Pilot-Project-Exploration-and-Validation\" class=\"headerlink\" title=\"Pilot Project Exploration and Validation\"></a>Pilot Project Exploration and Validation</h2><p>To validate the feasibility of our methodology, we conducted a series of pilot projects. Although specific business details cannot be disclosed, the overall project tasks included initializing new frontend services, setting up infrastructure and continuous integration (CI), writing new backend and frontend code and tests, and refactoring existing frontend systems. After project delivery, detailed data collection and comparative analysis yielded remarkably encouraging results.</p>\n<p>For instance, in a system built from scratch, the traditional development model required approximately 19 man-days, whereas the structured prompt-driven approach took only 7 man-days—with a code adoption rate as high as 95%. This not only reduced the development cycle dramatically but also nearly tripled productivity. Similarly, for legacy system requirements, development time was reduced from 5 to 3 man-days, representing about a 1.7-times improvement in efficiency. Additionally, we observed a significant reduction in code duplication and an increase in unit test coverage from 65% to 96%. These results clearly demonstrate that structured prompts not only improve delivery efficiency but also enhance code quality and overall system stability.</p>\n<p>Background_of_Project.png</p>\n<p>The significance of these results is that we have successfully discovered an effective path to harness AI for software development. This approach breaks through the limitations of traditional methods by transforming the inherently ambiguous generation process into a manageable, replicable engineering practice—bringing our team a brand-new development experience along with astonishing efficiency gains.</p>\n<p>Let’s first explore how to construct effective prompts in the realm of software development, and then review our specific practices.</p>\n<h2 id=\"Structured-Prompt-Design-Framework\"><a href=\"#Structured-Prompt-Design-Framework\" class=\"headerlink\" title=\"Structured Prompt Design Framework\"></a>Structured Prompt Design Framework</h2><h3 id=\"How-to-Construct-Effective-Prompts\"><a href=\"#How-to-Construct-Effective-Prompts\" class=\"headerlink\" title=\"How to Construct Effective Prompts?\"></a>How to Construct Effective Prompts?</h3><p>In software development, using natural language to drive AI code generation presents a fundamental contradiction: human thinking is inherently divergent, while AI requires highly structured and precise instructions. Traditional prompt design can fall into two pitfalls. On one hand, developers might overly rely on superficial descriptions—such as simply asking “Generate an 800-word marketing copy with subheadings.” Although straightforward, this approach often turns out to be formulaic. On the other hand, overly abstract descriptions leave too much room for interpretation, causing AI to stray from the intended target.</p>\n<p>To overcome this contradiction, we emphasize a shift in thinking—from merely describing features (like color or shape) to abstracting the essence of an object (its core functionality and operational mechanism). Only by first clearly defining an object’s core function and essential characteristics, and then supplementing that with specific details, can we provide AI with a unique, unambiguous generation path that minimizes “hallucinations.” For example, consider a scenario where we need to generate a “white refrigerator” (assuming we are not familiar with the abstract concept of a refrigerator). If we only describe it as “a square white object with four small wheels at the bottom,” AI might produce something unrelated to a refrigerator. However, if we begin by defining its essential function as “maintaining a low-temperature environment” and then add additional feature details, we can accurately target the concept of a refrigerator while still leaving room for related ideas such as cold storage or refrigerated trucks. As we advocate, “features determine details, while essence defines boundaries.” Only by clearly articulating the essence first and then supplementing with specifics can we ensure that the generated outcome aligns with our expectations.</p>\n<p>Construct_prompts_effectively.png</p>\n<h3 id=\"Application-of-the-Component-Description-Method-in-Prompt-Construction\"><a href=\"#Application-of-the-Component-Description-Method-in-Prompt-Construction\" class=\"headerlink\" title=\"Application of the Component Description Method in Prompt Construction\"></a>Application of the Component Description Method in Prompt Construction</h3><p>Based on the above philosophy, we introduced the component description method into our prompt construction. Taking backend development as an example, we designed up to 10 standardized dimensions for each component, covering aspects such as class names, method names, and exception handling. This method does not simply replicate the conventional framework concept; rather, the “components” here represent the basic units of a structured prompt that break down both functional responsibilities and the implementation process in detail.</p>\n<p>Component_Description.png</p>\n<p>The introduction of the component description method ensures that each prompt element is both self-contained and seamlessly integrated with others. By clearly defining its core responsibility and precisely specifying attributes and operational scope, we effectively avoid confusion and overlap between components, thereby safeguarding the integrity of the overall prompt and ensuring consistency in the generated code. This design not only significantly enhances maintainability but also provides robust support for tackling complex business scenarios.</p>\n<h3 id=\"Overall-Framework-for-Structured-Prompt-Design\"><a href=\"#Overall-Framework-for-Structured-Prompt-Design\" class=\"headerlink\" title=\"Overall Framework for Structured Prompt Design\"></a>Overall Framework for Structured Prompt Design</h3><p>Building on the component definitions, we further established a comprehensive framework for structured prompt design, divided into the following five parts:</p>\n<ul>\n<li><strong>Requirement Anchoring:</strong> Accurately describing business requirements to ensure precise development targets.</li>\n<li><strong>Structural Definition:</strong> Clarifying the dependencies and interactions among the modules required to implement functionality.</li>\n<li><strong>Task Scheduling:</strong> Breaking down overall requirements into specific operational units and systematically defining each component to form a coherent workflow.</li>\n<li><strong>Common Tasks:</strong> Standardizing high-frequency operations—such as data validation and exception handling—using template-driven solutions.</li>\n<li><strong>Constraint Control:</strong> Setting up safety boundaries for the entire system by limiting the scope of component calls and references to prevent issues arising from unclear boundaries.</li>\n</ul>\n<p>Throughout this process, we adhere to two golden principles: first, the abstraction of the essence must take precedence over mere feature descriptions—only by clearly defining a component’s core responsibilities can we avoid superficial enumeration. Second, components should be regarded as logical units rather than merely fragments of code; similar to an excellent screenplay that emphasizes a character’s internal motivations rather than dictating every detail of an actor’s performance. Guided by these principles, the framework for structured prompt design has enabled our team to develop a systematic, rigorous, and highly efficient prompting system, thereby greatly enhancing overall development efficiency and code quality.</p>\n<p>Structured_prompts_strategy.png</p>\n<h2 id=\"Prompt-Engineering-Governance-Strategy-for-AI-Assets\"><a href=\"#Prompt-Engineering-Governance-Strategy-for-AI-Assets\" class=\"headerlink\" title=\"Prompt Engineering: Governance Strategy for AI Assets\"></a>Prompt Engineering: Governance Strategy for AI Assets</h2><p>Ensuring that prompts remain effective throughout the development process requires more than just crafting high-quality prompts—it necessitates a comprehensive governance strategy. We have divided prompt governance into multiple layers: the rule layer, the industry-specific abstract solution layer, the specific solution implementation layer, and the AI execution layer. Responsibilities are clearly delineated across these layers, with architects, technical leads, and developers each playing their part to maintain high standards and sustainability of our prompt assets.</p>\n<p>prompts_governance_policy.png</p>\n<p>In our pilot projects, we have already integrated logging rules into a predefined workflow, placing them at the rule layer to ensure that, prior to code generation, the necessary safety and quality standards are embedded via prompts. This approach not only ensures that the generated code adheres to logging standards but also builds a robust safety barrier for the entire system. As implementation continues, this governance strategy will be further refined and extended to a broader range of development scenarios, providing structured assurance for quality control and process management in software engineering.</p>\n<h2 id=\"Analysis-of-a-Practical-Project-Example\"><a href=\"#Analysis-of-a-Practical-Project-Example\" class=\"headerlink\" title=\"Analysis of a Practical Project Example\"></a>Analysis of a Practical Project Example</h2><p>To illustrate the effectiveness of structured prompts more intuitively, let’s consider an API project for retrieving user permission information. This API is designed not only to support multi-criteria queries based on user ID, email, name, and role, but also to handle pagination, sorting, and the grouping and merging of data. In practice, we systematically organized and written the prompts according to the modules—requirements, structure, tasks, common tasks, and constraint controls. In the component definitions, we strictly adhered to the component description method and the core guiding principles of prompt construction. With this comprehensive framework, we successfully implemented an API that dynamically constructs query conditions based on multiple parameters and handles complex data grouping and merging.</p>\n<p>Statistical results indicate that for the backend API module, the team employed 6 sets of prompts totaling 592 lines, which resulted in 1849 lines of generated code and modifications or creations of 37 files. Such a comprehensive practice covering complex business scenarios not only validates the feasibility of the structured prompt model but also significantly enhances delivery efficiency and code quality, heralding a revolutionary approach to traditional development methods.</p>\n<p>Examples_of_project_practices.png</p>\n<p>During the project, we also encountered real-world challenges and gathered valuable experiences. For teams new to this model, we found that there is no need to write prompts entirely from scratch—effective prompts can be extracted from existing code, thereby enabling rapid implementation and iterative refinement of the prompt system.</p>\n<h2 id=\"Tips-and-Practical-Advice-for-Writing-Effective-Prompts\"><a href=\"#Tips-and-Practical-Advice-for-Writing-Effective-Prompts\" class=\"headerlink\" title=\"Tips and Practical Advice for Writing Effective Prompts\"></a>Tips and Practical Advice for Writing Effective Prompts</h2><p>Based on multiple projects, we have developed several strategies for writing high-quality prompts. First, when a solution already exists in current code, AI can be used to automatically parse and extract effective prompts, which can then be further refined. This approach not only saves time but also avoids the risks of starting from scratch. Second, when a solution is still in a developer’s mind, leveraging a reusable structured strategy and adjusting details according to the task at hand can quickly produce prompts that meet expectations. For those situations where the solution is not immediately clear, we recommend engaging in an open collaborative exploration with AI, gradually converging on ideas before using a structured template to generate initial prompts, followed by necessary manual refinement.</p>\n<p>Our practical experience has also shown that it is crucial to avoid excessive abstraction when writing prompts. For simpler scenarios, there is often no need to develop an overly complex prompting system; direct conversational coding can be more efficient. Moreover, prompt writing should strike a balance between articulating the essence and detailing features—ensuring clarity in functionality and responsibilities without neglecting essential details. Ultimately, whether the aim is to generate excellent code via AI or to streamline subsequent review and modifications by developers, the final prompts must be both highly readable and practical, conveying the intended idea in clear, natural language.</p>\n<p>Advice_of_practice.png</p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>In summary, prompt-driven development represents a novel software development model with unprecedented innovative potential. With the systematic design of structured prompts and rigorous governance strategies, we have successfully harnessed AI’s potential for code generation while significantly enhancing development efficiency and code quality. This approach has introduced a fundamentally new mode of thinking and working within our team.</p>\n<p>Looking ahead, we will continue to monitor cutting-edge AI technologies and further refine our prompt design and governance strategies in even more complex scenarios. We anticipate that, propelled by ongoing exploration and practice by developers, the deep integration of human and AI efforts will usher in a new era in software development—driving the entire industry toward a more efficient and intelligent future. Through continuous iteration and optimization, we are confident that structured prompts will become a core technology in engineering practice, injecting endless innovative energy and competitive advantages into software development.</p>\n<h2 id=\"Appendix\"><a href=\"#Appendix\" class=\"headerlink\" title=\"Appendix\"></a>Appendix</h2><p>After this article was published externally, discussions ensued around the saying “features determine details, while essence defines boundaries” I noticed that there are varying interpretations regarding the basic definitions of “features” and “essence.” Therefore, I have included their fundamental definitions here to harmonize our understanding and help clarify the deeper meaning behind the statement.</p>\n<p><strong>Features:</strong><br>Refers to the observable and describable specific attributes of a problem or requirement, including input&#x2F;output formats, constraints, interaction scenarios, technical parameters, and other explicit elements.<br><em>Core characteristics:</em> Quantifiable, verifiable, and concrete (readily observable).</p>\n<p><strong>Essence:</strong><br>Refers to the fundamental conflict or core objective underlying a problem, which serves as the foundational logic determining the effectiveness of a solution.<br><em>Core characteristics:</em> Abstract, directional, and non-negotiable (the decisive factor that defines the solution).</p>\n",
            "tags": [
                "AI",
                "Prompts"
            ]
        },
        {
            "id": "https://gszhangwei.github.io/2024/12/11/guidelines-prompts-driven-development/",
            "url": "https://gszhangwei.github.io/2024/12/11/guidelines-prompts-driven-development/",
            "title": "Guidelines of Prompts Driven Development",
            "date_published": "2024-12-11T09:00:00.000Z",
            "content_html": "<p>This document outlines the structure and steps necessary to implement a business function within a software application, focusing on granularity, implementation steps, and the relationships between various components in the architecture.</p>\n<h3 id=\"Business-Function-Overview\"><a href=\"#Business-Function-Overview\" class=\"headerlink\" title=\"Business Function Overview\"></a>Business Function Overview</h3><p>A business function refers to a specific operation or capability that a software application provides to fulfill business requirements. This can range from simple tasks like retrieving data to more complex operations like processing transactions.</p>\n<h4 id=\"Granularity\"><a href=\"#Granularity\" class=\"headerlink\" title=\"Granularity\"></a>Granularity</h4><ul>\n<li><strong>Minimum Granularity</strong>: One method</li>\n<li><strong>Highest Granularity</strong>: One API</li>\n</ul>\n<h3 id=\"Implementation-Steps\"><a href=\"#Implementation-Steps\" class=\"headerlink\" title=\"Implementation Steps\"></a>Implementation Steps</h3><p>The implementation of a business function involves multiple layers, each with specific responsibilities. Below are the detailed steps required for implementing this function.</p>\n<h4 id=\"Standard-Components\"><a href=\"#Standard-Components\" class=\"headerlink\" title=\"Standard Components\"></a>Standard Components</h4><ol>\n<li><p><strong>Component Structure</strong></p>\n<ul>\n<li><strong>Class Name</strong>: Describe the class name</li>\n<li><strong>Method Name</strong>: Describe the method name</li>\n<li><strong>Responsibility</strong>: Describe the responsibilities of the component</li>\n<li><strong>Request Body</strong>: Describe the request parameters (use ‘no parameters’ if there are none)</li>\n<li><strong>Return Body</strong>: Returns data of type {Return Type}</li>\n<li><strong>Endpoint</strong>: Describe the endpoint path</li>\n<li><strong>Request Method</strong>: {GET&#x2F;POST&#x2F;PUT&#x2F;DELETE}</li>\n<li><strong>Return Status Code</strong>: {HTTP status code}</li>\n<li><strong>Attribute Definition</strong>: Define attributes as needed</li>\n<li><strong>Static Members</strong>: List any static members if applicable</li>\n<li><strong>Exceptions Handling</strong>: Describe how exceptions will be handled</li>\n<li><strong>Elements</strong>: Identify any other necessary elements</li>\n</ul>\n</li>\n<li><p><strong>Regular Example</strong></p>\n<h5 id=\"Backend-Components\"><a href=\"#Backend-Components\" class=\"headerlink\" title=\"Backend Components\"></a>Backend Components</h5><ul>\n<li><p><strong>Controller</strong></p>\n<ul>\n<li>Class Name: <code>&#123;EntityName&#125;Controller</code></li>\n<li>Method Name: <code>&#123;methodName&#125;</code></li>\n<li>Endpoint: <code>/&#123;endpoint&#125;</code></li>\n<li>Request Method: {GET&#x2F;POST&#x2F;PUT&#x2F;DELETE}</li>\n<li>Request Body: {Describe the request parameters, write ‘no parameters’ if there are none}</li>\n<li>Return Body: Returns data of type {Return Type}</li>\n<li>Responsibility:<ul>\n<li>Handling HTTP requests</li>\n<li>Parameter verification</li>\n<li>Call application services</li>\n<li>Processing response</li>\n</ul>\n</li>\n<li>Return Status Code: {HTTP status code}</li>\n</ul>\n</li>\n<li><p><strong>Service</strong></p>\n<ul>\n<li>Class Name: <code>&#123;EntityName&#125;DomainService</code></li>\n<li>Method Name: <code>&#123;methodName&#125;</code></li>\n<li>Responsibility:<ul>\n<li>Implement core business logic</li>\n<li>Maintain object status</li>\n<li>Ensure business rules</li>\n</ul>\n</li>\n<li>Request Body: {Description of Request Parameters}</li>\n<li>Return Body: Returns data of type {Return Type}</li>\n</ul>\n</li>\n<li><p><strong>Repository</strong></p>\n<ul>\n<li>Class Name: <code>&#123;EntityName&#125;Repository</code></li>\n<li>Method Name: <code>&#123;methodName&#125;</code></li>\n<li>Responsibility:<ul>\n<li>Implement data persistence</li>\n<li>Implement data query</li>\n<li>Database operation encapsulation</li>\n</ul>\n</li>\n<li>Request Body: {Description of Request Parameters}</li>\n<li>Return Body: Returns data of type {Return Type}</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Frontend Frameworks</strong><br>Utilize popular frameworks such as React, Angular, or Vue for building user interfaces that interact with the backend.</p>\n</li>\n</ol>\n<h3 id=\"API-Implementation-Example-of-Hexagonal-architecture-No-architecture-modeling\"><a href=\"#API-Implementation-Example-of-Hexagonal-architecture-No-architecture-modeling\" class=\"headerlink\" title=\"API Implementation Example of Hexagonal architecture(No architecture modeling)\"></a>API Implementation Example of Hexagonal architecture(No architecture modeling)</h3><p>To implement an API that can update an entity (e.g., user information), follow these guidelines:</p>\n<ul>\n<li><p><strong>Controller Layer</strong></p>\n<ul>\n<li>Class Name: <code>&#123;EntityName&#125;Controller</code></li>\n<li>Method Name: <code>&#123;methodName&#125;</code></li>\n<li>Endpoint: <code>/&#123;endpoint&#125;</code></li>\n<li>Request Method: {GET&#x2F;POST&#x2F;PUT&#x2F;DELETE}</li>\n<li>Request Body: {Describe the request parameters, write ‘no parameters’ if there are none}</li>\n<li>Return Body: Returns data of type {Return Type}</li>\n<li>Logic: Call the corresponding method of the Application layer to obtain data.</li>\n<li>Return Status Code: {HTTP status code}</li>\n</ul>\n</li>\n<li><p><strong>Application Service Layer</strong></p>\n<ul>\n<li>Class Name: <code>&#123;EntityName&#125;ApplicationService</code></li>\n<li>Method Name: <code>&#123;methodName&#125;</code></li>\n<li>Logic: Call relevant methods of the Query layer or Domain layer.</li>\n<li>Request Body: {Description of Request Parameters}</li>\n<li>Return Body: Returns data of type {Return Type}</li>\n<li>Exception Handling Required.</li>\n</ul>\n</li>\n<li><p><strong>Query Layer</strong><br>This layer is responsible for obtaining resources.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Class Name: `&#123;EntityName&#125;Query`</span><br><span class=\"line\">Method Name: `&#123;methodName&#125;`</span><br><span class=\"line\">Logic: Call infrastructure-related methods.</span><br><span class=\"line\">Request Body: &#123;Description of Request Parameters&#125;</span><br><span class=\"line\">Return Body: Returns data of type &#123;Return Type&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>Domain Layer</strong><br>Implements business logic details.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Class Name:`&#123;EntityName&#125;DomainService`</span><br><span class=\"line\">Method Name:`&#123;methodName&#125;`</span><br><span class=\"line\">Logic: Call relevant methods of the Infrastructure layer.</span><br><span class=\"line\">Request Body:&#123;Description of Request Parameters&#125;</span><br><span class=\"line\">Return Body:&#123;Returns data of type &#123;Return Type&#125;&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>Repository Layer</strong><br>Handles database interactions.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Class Name:`&#123;EntityName&#125;Repository`</span><br><span class=\"line\">Method Name:`&#123;methodName&#125;`</span><br><span class=\"line\">Logic: Call relevant methods of the database.</span><br><span class=\"line\">Request Body:&#123;Description of Request Parameters&#125;</span><br><span class=\"line\">Return Body:&#123;Returns data of type &#123;Return Type&#125;&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"Testing-Strategy\"><a href=\"#Testing-Strategy\" class=\"headerlink\" title=\"Testing Strategy\"></a>Testing Strategy</h3><p>Generate unit tests and integration tests based on the methods defined in each layer. Use tools and frameworks appropriate for your programming environment to facilitate this process.</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>Implementing a business function requires careful planning and adherence to structured guidelines across various layers of an application. By following these guidelines, developers can ensure that their applications are robust, maintainable, and scalable.</p>\n",
            "tags": [
                "AI",
                "Prompts"
            ]
        },
        {
            "id": "https://gszhangwei.github.io/2024/11/15/refactoring-with-cursor-en/",
            "url": "https://gszhangwei.github.io/2024/11/15/refactoring-with-cursor-en/",
            "title": "Building Prompts Using the ReAct Framework for Efficient Code Refactoring with Cursor",
            "date_published": "2024-11-15T11:00:00.000Z",
            "content_html": "<h2 id=\"Introduction-A-Journey-from-Simple-to-Complex-AI-Programming\"><a href=\"#Introduction-A-Journey-from-Simple-to-Complex-AI-Programming\" class=\"headerlink\" title=\"Introduction: A Journey from Simple to Complex AI Programming\"></a>Introduction: A Journey from Simple to Complex AI Programming</h2><p>In my previous blog, I enthusiastically shared how I used Cursor to achieve a 0 to 1 implementation of an AI tool in just half a day. At that time, I felt like a child who had just learned to ride a bicycle, filled with excitement about this “new toy.” However, I soon realized that this was just the tip of the iceberg. While code generation is thrilling, the challenges we face in actual development are far more complex.<br>This realization propelled me into a deeper exploration, particularly in the area of code refactoring. I discovered that Cursor exhibited surprising potential. When I provided it with some refactoring instructions, it not only understood my intentions but also restructured the code as expected, and at an astonishing speed. This made me dream: if I could apply this skill in daily development, I could be leisurely “fishing” while others were still buried in refactoring!</p>\n<h2 id=\"The-Reality-Check-The-Dilemma-of-Refactoring-Complex-Projects\"><a href=\"#The-Reality-Check-The-Dilemma-of-Refactoring-Complex-Projects\" class=\"headerlink\" title=\"The Reality Check: The Dilemma of Refactoring Complex Projects\"></a>The Reality Check: The Dilemma of Refactoring Complex Projects</h2><p>However, reality often delivers a harsh blow. When I confidently applied this straightforward refactoring approach to real projects, I quickly hit a wall. The complexity of real projects far exceeded that of simple AI tool code projects, and simple prompts felt like trying to use a toothpick to move an elephant.</p>\n<p><strong>Main Issues Encountered:</strong></p>\n<ol>\n<li><strong>Uncontrolled Scope of Changes</strong>: Each refactor felt like a random walk without a map.</li>\n<li><strong>Chaotic Refactoring Strategies</strong>: Relying entirely on large models to select refactoring strategies was akin to handing the steering wheel to a blindfolded driver.</li>\n<li><strong>Contextual Understanding Barriers</strong>: Even with @Codebase, Cursor seemed oblivious to the overall project context, leading to aimless refactoring.</li>\n<li><strong>Code Review Burden</strong>: Each change required meticulous review; otherwise, it felt like dancing on the edge of a knife.</li>\n<li><strong>Vicious Cycle</strong>: The more I refactored, the messier it became, ultimately forcing me to start over and fall into a “start over - fail again” loop.</li>\n</ol>\n<p><strong>Root Cause Analysis:</strong><br>After deep reflection, several core reasons emerged:</p>\n<ol>\n<li><strong>Excessive Logical Complexity:</strong> Real projects often contain intricate business logic and conditional judgments.</li>\n<li><strong>Lack of Framework Knowledge:</strong> There was insufficient onboarding for AI regarding frameworks.</li>\n<li><strong>Diversity in Coding Styles:</strong> Variations in coding styles among different developers posed challenges.</li>\n<li><strong>Overwhelming Contextual Burden:</strong> Too many interrelated codes formed a complex web of dependencies.</li>\n<li><strong>Unpredictable Behavior:</strong> It was impossible to accurately anticipate the AI’s next actions.</li>\n</ol>\n<h2 id=\"Human-vs-Machine-Differences-in-Refactoring-Mindsets\"><a href=\"#Human-vs-Machine-Differences-in-Refactoring-Mindsets\" class=\"headerlink\" title=\"Human vs. Machine: Differences in Refactoring Mindsets\"></a>Human vs. Machine: Differences in Refactoring Mindsets</h2><p>To find a breakthrough, I pondered why human developers could navigate refactoring relatively smoothly while AI frequently stumbled.</p>\n<p><strong>Human Developer’s Refactoring Approach:</strong></p>\n<ol>\n<li><strong>Systematic Analysis:</strong> Identifying and eliminating code smells first to enhance readability and scalability.</li>\n<li><strong>Object-Oriented Optimization:</strong> Refactoring based on best practices in object-oriented design.</li>\n<li><strong>Architectural Adjustments:</strong> Reorganizing code into appropriate hierarchical structures.</li>\n<li><strong>Testing Validation:</strong> Ensuring that the refactored code still meets expected behaviors.<br><strong>AI’s Refactoring Approach:</strong> It resembles a mechanical worker simply repeating the action of “modifying code,” lacking the systematic thinking and strategic planning we expect.</li>\n</ol>\n<h2 id=\"Inspiration-Strikes-Insights-from-the-ReAct-Framework\"><a href=\"#Inspiration-Strikes-Insights-from-the-ReAct-Framework\" class=\"headerlink\" title=\"Inspiration Strikes: Insights from the ReAct Framework\"></a>Inspiration Strikes: Insights from the ReAct Framework</h2><p>In my quest for solutions, I encountered Agent and ReAct along with Chain-of-Thought (CoT), which sparked an idea. If we could enable AI to think like humans and reason in ways we expect, could we overcome the current challenges?</p>\n<h3 id=\"Overview-of-the-ReAct-Framework\"><a href=\"#Overview-of-the-ReAct-Framework\" class=\"headerlink\" title=\"Overview of the ReAct Framework\"></a>Overview of the ReAct Framework</h3><p>The ReAct framework was proposed by Shunyu Yao et al. in 2022 as a method that combines reasoning and action to enhance the performance of large language models (LLMs). This framework allows LLMs to alternately generate <strong>reasoning traces</strong> and <strong>task-specific operations</strong>, enabling models to induce, track, and update operational plans effectively while handling exceptions.</p>\n<ul>\n<li><strong>Interaction with External Tools:</strong> The ReAct framework enables LLMs to interact with external knowledge bases or environments for additional information, providing more reliable responses.</li>\n<li><strong>Performance Improvement:</strong> Research shows that ReAct outperforms several advanced baseline models in language and decision-making tasks while enhancing LLMs’ interpretability and trustworthiness.</li>\n<li><strong>Integration with Chain-of-Thought (CoT):</strong> Combining ReAct with CoT allows for leveraging both internal knowledge and external information during reasoning for optimal results.</li>\n</ul>\n<h2 id=\"Practice-Constructing-Refactoring-Specific-Prompts\"><a href=\"#Practice-Constructing-Refactoring-Specific-Prompts\" class=\"headerlink\" title=\"Practice: Constructing Refactoring-Specific Prompts\"></a>Practice: Constructing Refactoring-Specific Prompts</h2><p><strong>Designing Refactoring Steps:</strong></p>\n<ol>\n<li><strong>Basic Cleanup:</strong> Identify and address fundamental code smells.</li>\n<li><strong>Object-Oriented Optimization:</strong> Apply best practices in object-oriented design.</li>\n<li><strong>Architectural Adjustments:</strong> Restructure code according to hexagonal architecture standards.</li>\n<li><strong>Error Fixing:</strong> Address obvious compilation errors.</li>\n<li><strong>Manual Review:</strong> Supplement detail optimization.</li>\n<li><strong>Test Fixes:</strong> Ensure all test cases pass.</li>\n</ol>\n<p><img loading=\"lazy\" src=\"/../images/new_refactoring_processing.jpg\" alt=\"new_refactoring_processing.jpg\"></p>\n<p><strong>Designing Refactoring Prompt Template:</strong></p>\n<ol>\n<li><strong>Context Explanation:</strong> Provide architectural standards and best practice guidelines.</li>\n<li><strong>Thinking Steps:</strong> Guide AI through systematic analysis.</li>\n<li><strong>Observation Records:</strong> Document identified issues. </li>\n<li><strong>Action Guidelines:</strong> Clearly outline specific refactoring steps.</li>\n</ol>\n<p><img loading=\"lazy\" src=\"/../images/prompt_design.jpg\" alt=\"prompt_design.jpg\"></p>\n<p>Based on insights from ReAct and CoT, I designed a prompt for DDD refactoring on the current project (covering the first three steps):<br><strong>Prompt for Refactoring</strong></p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Context: In hexagonal architecture DDD implementation patterns, there are best practices and experiences regarding the design and implementation of Application Service, Domain Service, Query, and Port. Here are summarized experiences and suitable refactoring prompts:</span><br><span class=\"line\"><span class=\"bullet\">1.</span> Application Service</span><br><span class=\"line\"><span class=\"bullet\">    -</span> Primarily responsible for use case orchestration and transaction management without business logic; instead calls Domain Service for specific operations.</span><br><span class=\"line\"><span class=\"bullet\">2.</span> Domain Service</span><br><span class=\"line\"><span class=\"bullet\">    -</span> Contains core business logic handling complex business rules and operations as part of the domain model.</span><br><span class=\"line\"><span class=\"bullet\">3.</span> Port</span><br><span class=\"line\"><span class=\"bullet\">    -</span> Defines interfaces between applications and external systems, including definitions of Repository interfaces within inbound and outbound packages.</span><br><span class=\"line\"></span><br><span class=\"line\">Based on this context, please refer to @AcademyCourseDomainService.java for refactoring the current file; modifications involving @EnrollmentApplicationService.java can also be made. Use the ReAct framework following these thinking steps to analyze, observe, execute, and ultimately complete code refactoring.</span><br><span class=\"line\">Thought-1: Are there any obvious code smells?</span><br><span class=\"line\">Observation-1: There is a code smell: xxxx</span><br><span class=\"line\">Action-1: Refactor using best practices corresponding to this code smell.</span><br><span class=\"line\"></span><br><span class=\"line\">Thought-2: Are there any particularly concerning code smells regarding object-oriented principles such as single responsibility principle (SRP), open/closed principle (OCP), Liskov substitution principle (LSP), dependency inversion principle (DIP), and interface segregation principle (ISP)?</span><br><span class=\"line\">Observation-2: Several areas xxxx do not comply with SRP.</span><br><span class=\"line\">Action-2: Understand best practices of SRP and perform code refactoring accordingly.</span><br><span class=\"line\"></span><br><span class=\"line\">Thought-3: What layer does the current file belong to? What responsibilities must be adhered to?</span><br><span class=\"line\">Observation-3: The current file is a Domain service; some implementations xxxx do not align with domain service responsibilities.</span><br><span class=\"line\">Action-3: Refactor these implementations based on Domain service responsibilities by analyzing which layer&#x27;s duties they belong to and relocating them accordingly.</span><br></pre></td></tr></table></figure>\n<p>Output of Cursor composer:<br><img loading=\"lazy\" src=\"/../images/prompt_result_1.png\" alt=\"prompt_result_1.png\"><br><img loading=\"lazy\" src=\"/../images/prompt_result_2.png\" alt=\"prompt_result_2.png\"></p>\n<h2 id=\"Experimental-Results-and-Lessons-Learned\"><a href=\"#Experimental-Results-and-Lessons-Learned\" class=\"headerlink\" title=\"Experimental Results and Lessons Learned\"></a>Experimental Results and Lessons Learned</h2><p>Through multiple experiments, this ReAct-based prompt writing method demonstrated significant advantages:</p>\n<ol>\n<li><strong>Higher Result Accuracy:</strong> This refactoring method achieved expected results quickly within two or three interactions. The combination of ReAct and CoT allowed models to integrate information effectively during reasoning processes, reducing errors and uncertainties significantly.</li>\n<li><strong>Goal-Oriented Thinking Process:</strong> After adopting the ReAct framework, the model’s reasoning direction aligned more closely with user expectations rather than being influenced by hallucinations (i.e., generating false information). By introducing additional information sources (observational steps provide necessary information), models could verify and update their reasoning paths in real time.</li>\n<li><strong>Visualized Thinking and Modification Process:</strong> In Cursor’s composer mode, users can clearly see intermediate thought processes and modifications. This transparency enables developers to track each modification step better understand how models arrive at conclusions.</li>\n</ol>\n<h3 id=\"Key-Factors-for-Improved-Outcomes\"><a href=\"#Key-Factors-for-Improved-Outcomes\" class=\"headerlink\" title=\"Key Factors for Improved Outcomes:\"></a>Key Factors for Improved Outcomes:</h3><ol>\n<li><strong>Clear Boundaries:</strong> Context limits AI’s behavioral scope.</li>\n<li><strong>Defined Steps:</strong> Guides AI through predetermined thought processes during refactoring.</li>\n<li><strong>Feedback Loop Closure:</strong> Each step has clear observation and action guidelines.</li>\n</ol>\n<h3 id=\"Shift-in-Mindset\"><a href=\"#Shift-in-Mindset\" class=\"headerlink\" title=\"Shift in Mindset:\"></a>Shift in Mindset:</h3><p>This process made me realize that using AI effectively hinges on shifting our mindset from “problem-oriented” to “guided collaboration.” Our roles should be:</p>\n<ul>\n<li>Humans handle analysis and planning,</li>\n<li>AI executes specific details,</li>\n<li>Establish bridges through well-designed prompts.</li>\n</ul>\n<h2 id=\"Conclusion-A-New-Paradigm-for-AI-Coding-Assistants\"><a href=\"#Conclusion-A-New-Paradigm-for-AI-Coding-Assistants\" class=\"headerlink\" title=\"Conclusion: A New Paradigm for AI Coding Assistants\"></a>Conclusion: A New Paradigm for AI Coding Assistants</h2><p>This exploration not only helped me tackle the challenges of code refactoring but also imparted an important lesson: AI is not omnipotent; however, through proper guidance, it can become a powerful assistant that amplifies our capabilities. When we learn to interact with AI as “coaches” rather than “questioners,” we often achieve better results.<br>Of course, this brings new demands; understanding best practices and methodologies becomes essential when collaborating with AI so that we can guide it effectively towards completing tasks correctly.<br>This shift in thinking allows us to better leverage human creativity alongside AI execution capabilities—enhancing both code quality and development efficiency (ideally saving time should promote improved code quality rather than merely pursuing high output; otherwise, we risk falling into another quagmire where chasing output neglects quality). Balancing efficiency with quality is crucial; by emphasizing readability, maintainability, and sound best practices in our codebase, we can accelerate development while ensuring long-term software health and stability.<br>In future endeavors, this human-machine collaborative model may emerge as a new paradigm in software development.</p>\n<h2 id=\"Final-Note-Prompt-Sharing\"><a href=\"#Final-Note-Prompt-Sharing\" class=\"headerlink\" title=\"Final Note: Prompt Sharing\"></a>Final Note: Prompt Sharing</h2><p>Through practice, we’ve developed a prompt template specifically targeting code smell refactoring. If you’re interested in addressing code smells in your projects, feel free to try out this Prompt Template available on our site tailored for developers seeking efficient prompts that enhance their experience using Cursor for refactoring—especially aimed at those looking to optimize their refactoring processes through AI assistance.<br>This prompt template site aims at providing practical resources designed for developers looking for actionable templates that enhance their experience using Cursor for refactoring tasks while minimizing potential hallucinations during such processes.<br>We encourage everyone using Cursor or other AI coding assistance tools to share effective prompts within this repository. Together we can build a practical collection of refactoring prompts that enhance daily efficiency and code quality during development tasks!</p>\n<p>Thank you all for reading!</p>\n",
            "tags": [
                "Cursor"
            ]
        },
        {
            "id": "https://gszhangwei.github.io/2024/11/04/refactoring-with-cursor/",
            "url": "https://gszhangwei.github.io/2024/11/04/refactoring-with-cursor/",
            "title": "参考ReAct框架构建Prompt，使用Cursor高效对代码进行重构",
            "date_published": "2024-11-04T08:42:03.000Z",
            "content_html": "<h2 id=\"前言：从简单到复杂的AI编程之旅\"><a href=\"#前言：从简单到复杂的AI编程之旅\" class=\"headerlink\" title=\"前言：从简单到复杂的AI编程之旅\"></a>前言：从简单到复杂的AI编程之旅</h2><p>还记得<a href=\"https://mail.google.com/mail/u/0/#search/%F0%9F%A4%AF/KtbxLrjhzfdqjKmxgVHVCjNmBkwrQzSPSq\"><strong>上一篇博客</strong></a>中，我兴致勃勃地分享了如何用Cursor在短短半天内完成AI工具的从0到1实现。那时的我，就像刚学会骑自行车的孩子，对这个”新玩具”充满了热情。但很快，我就意识到这只是冰山一角。单纯的代码生成固然令人兴奋，但在实际开发中，我们面临的挑战远不止于此。</p>\n<p>于是，我开始了一段更深入的探索之旅。特别是在代码重构这个领域，我发现Cursor展现出了令人惊喜的潜力。当我给它一些重构指令时，它不仅能理解我的意图，还能按照预期重构代码，而且速度快得惊人。这让我不禁幻想：如果能把这项技能应用到日常开发中，那岂不是能在别人还在埋头重构时，我已经可以优哉游哉地”摸鱼”了？</p>\n<h2 id=\"现实给的一记重拳：复杂项目重构的困境\"><a href=\"#现实给的一记重拳：复杂项目重构的困境\" class=\"headerlink\" title=\"现实给的一记重拳：复杂项目重构的困境\"></a>现实给的一记重拳：复杂项目重构的困境</h2><p>然而，现实总是喜欢给我们一记重拳。当我满怀信心地将这种简单直接的重构方式应用到实际项目中时，很快就碰了壁。由于真实项目的复杂度远超简单的AI工具代码项目，简单的prompt在这里就像是用牙签撬动大象。</p>\n<p><strong>遇到的主要问题：</strong></p>\n<ol>\n<li><strong><strong>改动范围失控</strong></strong>：就像是没有地图的探险，每次重构都像是在随机漫步。</li>\n<li><strong><strong>重构策略混乱</strong></strong>：完全依赖大模型自动选择重构策略，就像是把方向盘交给了蒙着眼睛的司机。</li>\n<li><strong><strong>上下文理解障碍</strong></strong>：即使使用了@Codebase，Cursor依然像是对整个项目背景一无所知，重构起来漫无目标。</li>\n<li><strong><strong>代码审查负担</strong></strong>：每次改动都需要仔细审查，否则就像是在刀尖上跳舞，提心吊胆。</li>\n<li><strong><strong>恶性循环</strong></strong>：越重构越乱，最后不得不推倒重来，陷入了一个”推倒重来-再次失败”的死循环。</li>\n</ol>\n<p><strong>问题根源分析：</strong><br>经过深入思考，总结出了几个核心原因：</p>\n<ol>\n<li><strong><strong>逻辑复杂度过高</strong></strong>：现实项目往往包含复杂的业务逻辑和条件判断。</li>\n<li><strong><strong>框架知识欠缺</strong></strong>：没有给AI进行适当的框架Onboarding。</li>\n<li><strong><strong>代码风格多样性</strong></strong>：不同开发者的编码风格差异带来的挑战。</li>\n<li><strong><strong>上下文负担过重</strong></strong>：相互关联的代码太多，形成了复杂的依赖网络。</li>\n<li><strong><strong>行为不可预测</strong></strong>：无法准确预知AI的下一步操作。</li>\n</ol>\n<h2 id=\"人机对比：重构思维的差异\"><a href=\"#人机对比：重构思维的差异\" class=\"headerlink\" title=\"人机对比：重构思维的差异\"></a>人机对比：重构思维的差异</h2><p>为了找到突破口，我开始思考：为什么同样是重构，人类开发者能够相对顺利地完成，而AI却频频踩坑？</p>\n<p>人类开发者的重构方式：</p>\n<ol>\n<li><strong><strong>系统性分析</strong></strong>：首先识别和消除code smell，提升代码的可读性和可扩展性。</li>\n<li><strong><strong>面向对象优化</strong></strong>：基于面向对象的最佳实践进行重构。</li>\n<li><strong><strong>架构层面调整</strong></strong>：将代码重新组织到合适的层次结构中。</li>\n<li><strong><strong>测试验证</strong></strong>：确保重构后的代码仍然符合预期行为。</li>\n</ol>\n<p>AI的重构方式：就像一个机械工人，简单地重复”修改代码”这个动作，缺乏我们<strong>期望的</strong>系统性思维和策略性规划。</p>\n<h2 id=\"灵感突现：ReAct框架的启发\"><a href=\"#灵感突现：ReAct框架的启发\" class=\"headerlink\" title=\"灵感突现：ReAct框架的启发\"></a>灵感突现：ReAct框架的启发</h2><p>在寻找解决方案的过程中，我接触到了Agent和ReAct以及COT，这让我眼前一亮。如果能让AI像人类一样思考并且按照人类期望的方式进行思考，是否就能克服当前的困境？</p>\n<h3 id=\"ReAct框架简介\"><a href=\"#ReAct框架简介\" class=\"headerlink\" title=\"ReAct框架简介\"></a>ReAct框架简介</h3><p>ReAct 框架是由 Shunyu Yao 等人在 2022 年提出的一种方法，旨在结合推理与行动，以提高大型语言模型（LLMs）的性能。该框架允许 LLMs 交替生成<strong>推理轨迹</strong>和<strong>任务特定操作</strong>，从而使模型能够诱导、跟踪和更新操作计划，并有效处理异常情况。</p>\n<ul>\n<li><strong><strong>与外部工具的交互</strong></strong>：ReAct 框架使 LLMs 能够与外部知识库或环境进行交互，从而获取额外信息，提供更可靠的回应。</li>\n<li><strong><strong>性能提升</strong></strong>：研究表明，ReAct 在语言和决策任务上表现优于多个先进的基线模型，同时提高了 LLMs 的可解释性和可信度。</li>\n<li><strong><strong>与链式思考（CoT）的结合</strong></strong>：将 ReAct 与链式思考结合使用，可以在推理过程中同时利用内部知识和外部信息，从而取得最佳效果。</li>\n</ul>\n<h2 id=\"实践：构建重构专用的Prompt\"><a href=\"#实践：构建重构专用的Prompt\" class=\"headerlink\" title=\"实践：构建重构专用的Prompt\"></a>实践：构建重构专用的Prompt</h2><p><strong>重构步骤设计：</strong></p>\n<ol>\n<li><strong><strong>基础清理</strong></strong>：识别和处理基本的code smell</li>\n<li><strong><strong>面向对象优化</strong></strong>：应用面向对象的最佳实践</li>\n<li><strong><strong>架构调整</strong></strong>：根据六边形架构的规范重组代码</li>\n<li><strong><strong>错误修复</strong></strong>：处理明显的编译错误</li>\n<li><strong><strong>人工复查</strong></strong>：补充细节优化</li>\n<li><strong><strong>测试修复</strong></strong>：确保所有测试用例通过</li>\n</ol>\n<p><img loading=\"lazy\" src=\"/../images/new_refactoring_processing.jpg\" alt=\"new_refactoring_processing.jpg\"></p>\n<p><strong>重构Prompt模板设计：</strong></p>\n<ol>\n<li><strong><strong>上下文说明</strong></strong>：提供架构规范和最佳实践指南</li>\n<li><strong><strong>思考步骤</strong></strong>：引导AI进行系统性分析</li>\n<li><strong><strong>观察记录</strong></strong>：记录发现的问题</li>\n<li><strong><strong>行动指南</strong></strong>：明确具体的重构步骤</li>\n</ol>\n<p><img loading=\"lazy\" src=\"/../images/prompt_design.jpg\" alt=\"prompt_design.jpg\"></p>\n<p>基于ReAct和COT的启发，设计了一个用于当前项目上DDD重构的Prompt（包含重构设计的前3步）：<br><strong>Prompt for Refactoring</strong></p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Context: 在六边形架构的DDD实现模式中，Application Service、Domain Service、Query 以及 Port 的设计和实现有一些最佳实践和经验。以下是总结的经验和适合的重构提示：</span><br><span class=\"line\"><span class=\"bullet\">1.</span> Application Service</span><br><span class=\"line\"><span class=\"bullet\">    -</span> 主要负责用例编排和事务管理，不包含业务逻辑，而是调用 Domain Service 执行具体操作。</span><br><span class=\"line\"><span class=\"bullet\">2.</span> Domain Service</span><br><span class=\"line\"><span class=\"bullet\">    -</span> 包含核心业务逻辑，处理复杂业务规则和操作，是领域模型的一部分。</span><br><span class=\"line\"><span class=\"bullet\">3.</span> Port</span><br><span class=\"line\"><span class=\"bullet\">    -</span> 定义应用程序与外部系统之间接口，包括 inbound 和 outbound 包中的 Repository 接口定义。</span><br><span class=\"line\"></span><br><span class=\"line\">基于上述上下文。请参照 @AcademyCourseDomainService.java  对当前文件进行重构，如果涉及 @EnrollmentApplicationService.java  的修改，也可以进行修改。使用 ReAct 框架按照下面的思考步骤来指导此过程，以分析、观察、执行并最终完成代码重构。</span><br><span class=\"line\">Thought-1：是否有明显的code smell？</span><br><span class=\"line\">Observation-1：有code smell：xxxx</span><br><span class=\"line\">Action-1：使用这个code smell 对应的最佳实践进行重构</span><br><span class=\"line\"></span><br><span class=\"line\">Thought-2：是否有特别需要注意的code smell，比如说面向对象的5大基本原则，单一职责原则、开放封闭原则、里氏替换原则、依赖倒置原则和接口隔离原则？</span><br><span class=\"line\">Observation-2：这几个地方xxxx，不符合单一职责原则</span><br><span class=\"line\">Action-2：了解单一职责原则的最佳实践，并进行代码重构</span><br><span class=\"line\"></span><br><span class=\"line\">Thought-3：当前文件是什么Layer，需要遵循哪些职责？</span><br><span class=\"line\">Observation-3：当前的文件是Domain service，里面有些实现xxxx做的事情不属于domain service的职责</span><br><span class=\"line\">Action-3：根据Domain service职责，对这几个实现进行重构，分析这些做的事情属于哪些layer的职责，将实现挪到对应的Layer中去</span><br></pre></td></tr></table></figure>\n<p>Cursor composer的输出：<br><img loading=\"lazy\" src=\"/../images/prompt_result_1.png\" alt=\"prompt_result_1.png\"><br><img loading=\"lazy\" src=\"/../images/prompt_result_2.png\" alt=\"prompt_result_2.png\"></p>\n<h2 id=\"实验效果与经验总结\"><a href=\"#实验效果与经验总结\" class=\"headerlink\" title=\"实验效果与经验总结\"></a>实验效果与经验总结</h2><p>通过多轮实验，这种基于ReAct的重构Prompt编写方法展现出了明显的优势：</p>\n<ol>\n<li><strong>更高的结果正确性：</strong> 这种重构方法能够在仅需两到三轮的互动中，快速达成预期结果。与传统方法相比，ReAct和COT的结合使得模型能够更有效地整合信息和推理过程，从而减少了错误和不确定性。这种高效性使得用户在处理复杂问题时，能够更快地获得准确答案。</li>\n<li><strong>目标导向的思考过程：</strong> 采用ReAct框架后，模型的思考方向更加符合用户的期望，而不是受到幻觉（即生成不实信息）的影响。通过引入额外信息源（观察的步骤会引入需要的信息），模型能够实时验证和更新其推理路径，从而确保所生成的信息是基于事实而非虚构。这一特性显著提升了用户对模型输出的信任度。</li>\n<li><strong>可视化的思考与修改过程：</strong> 在Cursor的composer模式中，用户可以清晰地看到思考和修改的中间过程。这种透明性让开发者能够追踪每一步的修改步骤，从而更好地理解模型如何得出最终结论。通过展示具体的推理轨迹和所采取的行动，用户不仅能审视模型的决策过程，还能对其进行必要的调整和优化。</li>\n</ol>\n<h3 id=\"效果提升的关键因素：\"><a href=\"#效果提升的关键因素：\" class=\"headerlink\" title=\"效果提升的关键因素：\"></a>效果提升的关键因素：</h3><ol>\n<li><strong><strong>边界清晰</strong></strong>：通过Context限定了AI的行为范围</li>\n<li><strong><strong>步骤明确</strong></strong>：让AI按照预定的思维方式进行重构</li>\n<li><strong><strong>反馈闭环</strong></strong>：每个步骤都有明确的观察和行动指南</li>\n</ol>\n<h3 id=\"思维方式的转变：\"><a href=\"#思维方式的转变：\" class=\"headerlink\" title=\"思维方式的转变：\"></a>思维方式的转变：</h3><p>这个过程让我意识到，使用AI的关键在于转变思维方式：从”问题导向”转向”引导式合作”。我们需要做的是：</p>\n<ul>\n<li>人负责分析和规划</li>\n<li>AI负责执行具体细节</li>\n<li>通过精心设计的Prompt建立桥梁</li>\n</ul>\n<h2 id=\"结语：AI编码助手使用的新范式\"><a href=\"#结语：AI编码助手使用的新范式\" class=\"headerlink\" title=\"结语：AI编码助手使用的新范式\"></a>结语：AI编码助手使用的新范式</h2><p>这次的探索不仅帮助我解决了代码重构的难题，更重要的是让我领悟到了一个道理：AI不是万能的，但通过正确的引导，它可以成为我们强大的助手，起到能力放大器的作用。当我们学会用”教练”而不是”提问者”的身份与AI互动时，往往能获得更好的结果。当然，这也给我们带来了新的要求：在与AI进行协作时，了解<strong>最佳实践</strong>和<strong>方法论</strong>成为了前提条件，只有这样，我们才能更有效地引导AI以正确的方式完成工作。</p>\n<p>这种思维方式的转变，让我们能够更好地发挥人类的创造力和AI的执行力，从而提升<strong>代码质量</strong>和<strong>开发效率</strong>（在理想情况下，提升开发效率应当将节省下来的时间来促进代码质量的提高，而不是单纯追求高产出，否则我们可能会陷入另一个无法自拔的漩涡。追逐产出而忽视质量，最终仍然会导致技术债务高筑，增加维护成本，甚至影响项目的可持续性。因此，平衡效率与质量至关重要。通过注重代码的可读性、可维护性和合理的使用最佳实践，我们不仅能够提高开发速度，还能确保软件的长期健康和稳定性）。在未来，这种人机协作的模式也许会成为软件开发的一种新范式。</p>\n<h2 id=\"最后：Prompt分享\"><a href=\"#最后：Prompt分享\" class=\"headerlink\" title=\"最后：Prompt分享\"></a>最后：Prompt分享</h2><p>经过实践，我们总结出了一个专门针对代码基础异味（code smell）重构的提示模板。如果你对代码基础异味的重构感兴趣，欢迎尝试使用这个站点下的<a href=\"https://aigo.netlify.app/docs/codesemell/%E4%BB%A3%E7%A0%81%E5%9D%8F%E5%91%B3%E9%81%93\"><strong>Prompt模板</strong></a>。这个重构Prompt模版站点是为开发者量身打造的资源分享平台，旨在提供高效可行的Prompt模版，以提升使用Cursor进行重构的体验，主要面向希望通过AI优化重构过程的同事。其Prompt特色在于能够显著降低AI在重构时可能出现的幻觉。大家可以通过参考站点的<a href=\"https://aigo.netlify.app/#%e5%bf%ab%e9%80%9f%e5%bc%80%e5%a7%8b\"><strong>Quick Start</strong></a>操作快速上手，从而提升重构的准确率和效率，让大家更专注于最佳实践和方法论的思考。此外，我们也欢迎大家在使用Cursor或其他AI代码辅助工具的过程中，将有效的提示分享至<a href=\"https://github.com/AI-performance-improvement/prompt-helper\"><strong>这个代码库</strong></a>中。通过共同努力，我们可以建立一个实用的重构提示集合，帮助大家提升日常的重构效率和代码质量。后续我们将逐步总结出面向对象重构及DDD等框架型代码重构的可复用Prompt模板，并上传到这个<a href=\"https://aigo.netlify.app/docs/codesemell/%E4%BB%A3%E7%A0%81%E5%9D%8F%E5%91%B3%E9%81%93/\"><strong>站点</strong></a>，如有需要请持续关注。</p>\n<p>到此，我们的讨论就告一段落了。感谢大家的阅读和关注!</p>\n",
            "tags": [
                "Cursor"
            ]
        },
        {
            "id": "https://gszhangwei.github.io/2024/09/26/AI_explore_Cursor/",
            "url": "https://gszhangwei.github.io/2024/09/26/AI_explore_Cursor/",
            "title": "AI工具探索：震惊🤯，使用Cursor仅用半天就完成了AI工具的实现",
            "date_published": "2024-09-26T11:42:26.000Z",
            "content_html": "<p>前段时间，我看到了一篇博客介绍Cursor的，使得Cursor再次进入了我的视野。最近我有一个构建AI工具的想法，但由于对AI编程相关技术栈的局限，暂时搁置。因此，我重新安装了Cursor进行尝试，发现它已经进化成真正意义上的AI编程助手，于是决定利用Cursor实现之前的构想。</p>\n<h2 id=\"需求来源\"><a href=\"#需求来源\" class=\"headerlink\" title=\"需求来源\"></a>需求来源</h2><p>相信BAU是所有团队都在处理的事情。对我而言，这些任务往往是“不想做，但又不得不做”的烦恼。尤其是当你已经总结出SOP后，每次处理时都感觉自己只是一个无情的BAU处理机器，繁琐而乏味。在这种情况下，我想，是否可以每次只需执行最简单的命令，就能自动执行我期望的动作？而Python脚本正是一个很好的解决方案。因此，我列出了所有需要完成的任务，并通过Python脚本实现了这些功能。</p>\n<p>通过这一过程，我成功实现了一键执行期望动作。下次再遇到相同的任务时，只需运行这个脚本，就能省去寻找资源和逐个文件分配权限的时间，效率瞬间从15分钟提升至1分钟。然而，这一解决方案仍然存在角色限制，仅限于技术性角色使用。那么，如何让非技术性角色也能使用这个解决方案呢？</p>\n<p>这个问题一直萦绕在我的脑中，直到我想到最近学习的LLM（大型语言模型）编程。灵光一现，我意识到可以让非技术性角色通过自然语言描述，然后由大模型解析这些自然语言并翻译成需要执行的脚本，再进行自动执行。</p>\n<h2 id=\"调研\"><a href=\"#调研\" class=\"headerlink\" title=\"调研\"></a>调研</h2><p>说干就干，晚上下班后，我开始寻找合适的大模型。我的期望是：</p>\n<ul>\n<li>能在本地运行（方便调试）</li>\n<li>我的机器能够支持（有一台常规配置的MacOS M1 16G物理机）</li>\n<li>参数足够以进行文本到行为的转换</li>\n</ul>\n<p>最终，Ollama的Llama3.1 8B模型引起了我的注意。该模型可以直接在本地运行，而我2020年的M1 MacBook Pro恰好符合其要求，并且在自然语言处理方面表现出色。官方是这么描述的：<img loading=\"lazy\" src=\"/../images/llama3.1_8_B_description.png\" alt=\"llama3.1_8_B_description\"></p>\n<h2 id=\"分析与任务规划\"><a href=\"#分析与任务规划\" class=\"headerlink\" title=\"分析与任务规划\"></a>分析与任务规划</h2><p>有了想法和大模型后，剩下的就是考虑如何实现。经过两周晚上的LLM编程入门课程学习，我了解到了Langchain这个框架，以及如何使用。有了Langchain，实现这一需求变得轻而易举。大致步骤如下：</p>\n<ul>\n<li>在本地使用Ollama运行Llama3.1 8B模型。</li>\n<li>构建脚本并放入合适的包中。</li>\n<li>使用Langchain构建链，并对方法及Prompt进行封装。</li>\n<li>调用封装好的方法进行自然语言分析，选择并执行需要的脚本。</li>\n</ul>\n<p>还有个问题在于，如何让大模型知道有哪些可执行脚本及其作用。针对这一问题，有多种选择，比如：</p>\n<ul>\n<li>使用提示模板</li>\n<li>使用分类器</li>\n<li>使用上下文管理</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>方法</th>\n<th>优点</th>\n<th>缺点</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>提示模板</strong></td>\n<td>简单易用：设计和实现相对简单，用户只需提供自然语言输入。</td>\n<td>局限性：可能无法涵盖所有可能的输入，导致复杂请求无法处理。</td>\n</tr>\n<tr>\n<td></td>\n<td>灵活性高：可以根据不同的需求快速调整模板。</td>\n<td>维护成本：随着需求变化，模板需要不断更新和维护。</td>\n</tr>\n<tr>\n<td></td>\n<td>可读性强：用户易于理解提示内容，便于培训和上手。</td>\n<td></td>\n</tr>\n<tr>\n<td><strong>分类器</strong></td>\n<td>自动化程度高：能够自动识别和分类输入，适合处理大量数据。</td>\n<td>复杂性：需要大量的训练数据和时间来构建和优化分类器。</td>\n</tr>\n<tr>\n<td></td>\n<td>适应性强：可以通过训练模型提高分类精度。</td>\n<td>不确定性：分类结果可能不准确，导致错误的脚本被调用。</td>\n</tr>\n<tr>\n<td><strong>上下文管理</strong></td>\n<td>状态跟踪：能够在多个交互中保持上下文。</td>\n<td>实现复杂：需要设计复杂的状态管理机制。</td>\n</tr>\n<tr>\n<td></td>\n<td>智能响应：根据上下文信息提供更精准的响应。</td>\n<td>资源消耗：维护上下文信息可能消耗更多计算资源和内存。</td>\n</tr>\n</tbody></table>\n<p>经过权衡优缺点、实现复杂度与当前需求，我决定采用提示模板作为解决方案。</p>\n<p>万事俱备，“东风” - Cursor，也来了，直接开整。</p>\n<h2 id=\"实现步骤（Pair-with-Cursor）\"><a href=\"#实现步骤（Pair-with-Cursor）\" class=\"headerlink\" title=\"实现步骤（Pair with Cursor）\"></a>实现步骤（Pair with Cursor）</h2><h3 id=\"任务1-给定多个Python脚本，使用Ollama和大模型llama3-1，根据输入的一段话分析出需要执行哪个Python脚本，并执行。\"><a href=\"#任务1-给定多个Python脚本，使用Ollama和大模型llama3-1，根据输入的一段话分析出需要执行哪个Python脚本，并执行。\" class=\"headerlink\" title=\"任务1: 给定多个Python脚本，使用Ollama和大模型llama3.1，根据输入的一段话分析出需要执行哪个Python脚本，并执行。\"></a><strong>任务1: 给定多个Python脚本，使用Ollama和大模型llama3.1，根据输入的一段话分析出需要执行哪个Python脚本，并执行。</strong></h3><p><strong>execute_script_with_ollama.py</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> subprocess</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.llms <span class=\"keyword\">import</span> Ollama</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">execute_script</span>(<span class=\"params\">script_name</span>):</span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        result = subprocess.run([<span class=\"string\">&#x27;python&#x27;</span>, script_name], capture_output=<span class=\"literal\">True</span>, text=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(result.stdout)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;Error executing script <span class=\"subst\">&#123;script_name&#125;</span>: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">analyze_and_execute</span>(<span class=\"params\">input_text</span>):</span><br><span class=\"line\">    llm = Ollama(model=<span class=\"string\">&quot;llama3.1&quot;</span>)</span><br><span class=\"line\">    prompt = <span class=\"string\">f&quot;根据以下输入选择一个Python脚本并执行：\\n\\n<span class=\"subst\">&#123;input_text&#125;</span>\\n\\n可选脚本：\\n1. assign_access_for_dataset.py\\n2. generate_random_num_from_1_100.py\\n3. sum_1_100.py\\n\\n请返回要执行的脚本文件名。&quot;</span></span><br><span class=\"line\">    response = llm(prompt)</span><br><span class=\"line\">    </span><br><span class=\"line\">    script_name = response.strip()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> script_name <span class=\"keyword\">in</span> [<span class=\"string\">&#x27;assign_access_for_dataset.py&#x27;</span>, <span class=\"string\">&#x27;generate_random_num_from_1_100.py&#x27;</span>, <span class=\"string\">&#x27;sum_1_100.py&#x27;</span>]:</span><br><span class=\"line\">        execute_script(<span class=\"string\">f&#x27;src/python_scripts/<span class=\"subst\">&#123;script_name&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;No matching script found or invalid response.&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&quot;__main__&quot;</span>:</span><br><span class=\"line\">    user_input = <span class=\"built_in\">input</span>(<span class=\"string\">&quot;请输入一段话: &quot;</span>)</span><br><span class=\"line\">    analyze_and_execute(user_input)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"任务2-将Prompt进行封装，使得提示更容易扩展，并放到新的文件中。\"><a href=\"#任务2-将Prompt进行封装，使得提示更容易扩展，并放到新的文件中。\" class=\"headerlink\" title=\"任务2: 将Prompt进行封装，使得提示更容易扩展，并放到新的文件中。\"></a>任务2: 将Prompt进行封装，使得提示更容易扩展，并放到新的文件中。</h3><p><strong>prompt_generator.py</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PromptGenerator</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, scripts</span>):</span><br><span class=\"line\">        self.scripts = scripts</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">generate_prompt</span>(<span class=\"params\">self, input_text</span>):</span><br><span class=\"line\">        script_list = <span class=\"string\">&quot;\\n&quot;</span>.join([<span class=\"string\">f&quot;<span class=\"subst\">&#123;i+<span class=\"number\">1</span>&#125;</span>. <span class=\"subst\">&#123;script&#125;</span>&quot;</span> <span class=\"keyword\">for</span> i, script <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(self.scripts)])</span><br><span class=\"line\">        prompt = <span class=\"string\">f&quot;根据以下输入选择一个Python脚本并执行：\\n\\n<span class=\"subst\">&#123;input_text&#125;</span>\\n\\n可选脚本：\\n<span class=\"subst\">&#123;script_list&#125;</span>\\n\\n请返回要执行的脚本文件名。&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> prompt</span><br></pre></td></tr></table></figure>\n<p><strong>execute_script_with_ollama.py</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> subprocess</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.llms <span class=\"keyword\">import</span> Ollama</span><br><span class=\"line\"><span class=\"keyword\">from</span> prompt_generator <span class=\"keyword\">import</span> PromptGenerator</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">execute_script</span>(<span class=\"params\">script_name</span>):</span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        result = subprocess.run([<span class=\"string\">&#x27;python&#x27;</span>, script_name], capture_output=<span class=\"literal\">True</span>, text=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(result.stdout)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;Error executing script <span class=\"subst\">&#123;script_name&#125;</span>: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">analyze_and_execute</span>(<span class=\"params\">input_text</span>):</span><br><span class=\"line\">    scripts = [</span><br><span class=\"line\">        <span class=\"string\">&#x27;assign_access_for_dataset.py&#x27;</span>,</span><br><span class=\"line\">        <span class=\"string\">&#x27;generate_random_num_from_1_100.py&#x27;</span>,</span><br><span class=\"line\">        <span class=\"string\">&#x27;sum_1_100.py&#x27;</span></span><br><span class=\"line\">    ]</span><br><span class=\"line\">    </span><br><span class=\"line\">    prompt_generator = PromptGenerator(scripts)</span><br><span class=\"line\">    llm = Ollama(model=<span class=\"string\">&quot;llama3.1&quot;</span>)</span><br><span class=\"line\">    prompt = prompt_generator.generate_prompt(input_text)</span><br><span class=\"line\">    response = llm(prompt)</span><br><span class=\"line\">    </span><br><span class=\"line\">    script_name = response.strip()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> script_name <span class=\"keyword\">in</span> scripts:</span><br><span class=\"line\">        execute_script(<span class=\"string\">f&#x27;src/python_scripts/<span class=\"subst\">&#123;script_name&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;No matching script found or invalid response.&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&quot;__main__&quot;</span>:</span><br><span class=\"line\">    user_input = <span class=\"built_in\">input</span>(<span class=\"string\">&quot;请输入一段话: &quot;</span>)</span><br><span class=\"line\">    analyze_and_execute(user_input)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"任务3-在配置文件中维护脚本列表，以便管理多种类型的PromptGenerator。尝试使用工厂模式。\"><a href=\"#任务3-在配置文件中维护脚本列表，以便管理多种类型的PromptGenerator。尝试使用工厂模式。\" class=\"headerlink\" title=\"任务3: 在配置文件中维护脚本列表，以便管理多种类型的PromptGenerator。尝试使用工厂模式。\"></a>任务3: 在配置文件中维护脚本列表，以便管理多种类型的PromptGenerator。尝试使用工厂模式。</h3><p><strong>config.py</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#配置文件，用于维护脚本列表</span></span><br><span class=\"line\">SCRIPTS = [</span><br><span class=\"line\">    <span class=\"string\">&#x27;assign_access_for_dataset.py&#x27;</span>,</span><br><span class=\"line\">    <span class=\"string\">&#x27;generate_random_num_from_1_100.py&#x27;</span>,</span><br><span class=\"line\">    <span class=\"string\">&#x27;sum_1_100.py&#x27;</span></span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n<p><strong>prompt_generator.py</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> config <span class=\"keyword\">import</span> SCRIPTS</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PromptGenerator</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        self.scripts = SCRIPTS</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">generate_prompt</span>(<span class=\"params\">self, input_text</span>):</span><br><span class=\"line\">        script_list = <span class=\"string\">&quot;\\n&quot;</span>.join([<span class=\"string\">f&quot;<span class=\"subst\">&#123;i+<span class=\"number\">1</span>&#125;</span>. <span class=\"subst\">&#123;script&#125;</span>&quot;</span> <span class=\"keyword\">for</span> i, script <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(self.scripts)])</span><br><span class=\"line\">        prompt = <span class=\"string\">f&quot;根据以下输入选择一个Python脚本并执行：\\n\\n<span class=\"subst\">&#123;input_text&#125;</span>\\n\\n可选脚本：\\n<span class=\"subst\">&#123;script_list&#125;</span>\\n\\n请返回要执行的脚本文件名。&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> prompt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">AdvancedPromptGenerator</span>(<span class=\"title class_ inherited__\">PromptGenerator</span>):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">generate_prompt</span>(<span class=\"params\">self, input_text</span>):</span><br><span class=\"line\">        script_list = <span class=\"string\">&quot;\\n&quot;</span>.join([<span class=\"string\">f&quot;<span class=\"subst\">&#123;i+<span class=\"number\">1</span>&#125;</span>. <span class=\"subst\">&#123;script&#125;</span>&quot;</span> <span class=\"keyword\">for</span> i, script <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(self.scripts)])</span><br><span class=\"line\">        prompt = <span class=\"string\">f&quot;高级模式：根据以下输入选择一个Python脚本并执行：\\n\\n<span class=\"subst\">&#123;input_text&#125;</span>\\n\\n可选脚本：\\n<span class=\"subst\">&#123;script_list&#125;</span>\\n\\n请返回要执行的脚本文件名。&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> prompt</span><br></pre></td></tr></table></figure>\n<p><strong>execute_script_with_ollama.py</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> subprocess</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.llms <span class=\"keyword\">import</span> Ollama</span><br><span class=\"line\"><span class=\"keyword\">from</span> prompt_generator_factory <span class=\"keyword\">import</span> PromptGeneratorFactory</span><br><span class=\"line\"><span class=\"keyword\">from</span> config <span class=\"keyword\">import</span> SCRIPTS</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">execute_script</span>(<span class=\"params\">script_name</span>):</span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        result = subprocess.run([<span class=\"string\">&#x27;python&#x27;</span>, script_name], capture_output=<span class=\"literal\">True</span>, text=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(result.stdout)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;Error executing script <span class=\"subst\">&#123;script_name&#125;</span>: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">analyze_and_execute</span>(<span class=\"params\">input_text, generator_type=<span class=\"string\">&quot;default&quot;</span></span>):</span><br><span class=\"line\">    prompt_generator = PromptGeneratorFactory.get_prompt_generator(generator_type)</span><br><span class=\"line\">    llm = Ollama(model=<span class=\"string\">&quot;llama3.1&quot;</span>)</span><br><span class=\"line\">    prompt = prompt_generator.generate_prompt(input_text)</span><br><span class=\"line\">    response = llm(prompt)</span><br><span class=\"line\">    </span><br><span class=\"line\">    script_name = response.strip()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> script_name <span class=\"keyword\">in</span> SCRIPTS:</span><br><span class=\"line\">        execute_script(<span class=\"string\">f&#x27;src/python_scripts/<span class=\"subst\">&#123;script_name&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;No matching script found or invalid response.&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&quot;__main__&quot;</span>:</span><br><span class=\"line\">    user_input = <span class=\"built_in\">input</span>(<span class=\"string\">&quot;请输入一段话: &quot;</span>)</span><br><span class=\"line\">    generator_type = <span class=\"built_in\">input</span>(<span class=\"string\">&quot;请输入生成器类型（default或advanced）: &quot;</span>)</span><br><span class=\"line\">    analyze_and_execute(user_input, generator_type)</span><br></pre></td></tr></table></figure>\n\n<p>到此为止，基本上就可以运行了。我还进行了多轮任务，包括让它帮助优化、重构、生成前端页面等，但不一一列举了。有兴趣的人可以下载<a href=\"https://github.com/gszhangwei/task_assistant/tree/main\"><strong>完整代码库</strong></a>进行体验。</p>\n<h2 id=\"成果展示\"><a href=\"#成果展示\" class=\"headerlink\" title=\"成果展示\"></a>成果展示</h2><p>我准备了两类脚本任务进行测试：常规类和任务分配类。期望的结果是能够根据输入的自然语言，正确分析应该使用哪个提示生成器，并使用该提示模板构建Prompt，让大模型根据Prompt做出正确的脚本选择，并执行。</p>\n<ul>\n<li>常规类<ul>\n<li>生成1到100之间的随机数</li>\n<li>计算1到100的和</li>\n<li>获取当前日期和时间</li>\n</ul>\n</li>\n<li>任务分配类<ul>\n<li>为数据集分配访问权限</li>\n<li>为Dashboard分配访问权限</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"场景一：让助手给我执行获取当天日期的脚本\"><a href=\"#场景一：让助手给我执行获取当天日期的脚本\" class=\"headerlink\" title=\"场景一：让助手给我执行获取当天日期的脚本\"></a>场景一：让助手给我执行获取当天日期的脚本</h3><p>前端执行：<br><img loading=\"lazy\" src=\"/../images/frontend_result_1.png\" alt=\"frontend_result_1.png\"><br>后端输出结果：<br><img loading=\"lazy\" src=\"/../images/Backend_result_1.png\" alt=\"Backend_result_1.png\"></p>\n<h3 id=\"场景二：让助手给我执行-Dataset-权限分配的脚本\"><a href=\"#场景二：让助手给我执行-Dataset-权限分配的脚本\" class=\"headerlink\" title=\"场景二：让助手给我执行 Dataset 权限分配的脚本\"></a>场景二：让助手给我执行 Dataset 权限分配的脚本</h3><p>前端执行：<br><img loading=\"lazy\" src=\"/../images/Frontend_result_2.png\" alt=\"Frontend_result_2.png\"><br>后端输出结果：<br><img loading=\"lazy\" src=\"/../images/Backend_result_2.png\" alt=\"Backend_result_2.png\"><br>结果符合预期，AC通过。凭借微薄的AI编程基础，使用Cursor成功的开发了AI工具，并成功实现了阶段性目标！🎉</p>\n<h2 id=\"相关环境\"><a href=\"#相关环境\" class=\"headerlink\" title=\"相关环境\"></a>相关环境</h2><ul>\n<li>Python 3.10</li>\n<li><a href=\"https://ollama.com/\">Ollama</a></li>\n<li><a href=\"https://ollama.com/library/llama3.1\">Llama 3.1 8B</a></li>\n<li><a href=\"https://www.cursor.com/\">Cursor</a>试用版本，2周试用期</li>\n<li>Cursor大模型使用情况：claude-3.5-sonnet与gpt-4o切换使用</li>\n<li>AI编程的预备知识可以通过<a href=\"https://thoughtworks.udemy.com/course/llm-app-dev/learn/lecture/43397076#overview\">LLM编程入门课程</a>来学习。即使是零基础的学习者也能理解课程内容，从中掌握一些基本的AI编程概念和框架，但要深入了解仍需自行探索。</li>\n</ul>\n<h2 id=\"Cursor的体验感受\"><a href=\"#Cursor的体验感受\" class=\"headerlink\" title=\"Cursor的体验感受\"></a>Cursor的体验感受</h2><ul>\n<li><strong>易用性</strong>：整个过程，没写一行代码，扮演“嘴强王者”，让Cursor成为你的码仔，它能无怨无求的按照要求写出代码。通过自然语言输入，用户可以轻松生成代码或执行复杂任务，这种无缝集成大大降低了技术门槛。</li>\n<li><strong>高效的代码生成</strong>：生成的代码通常处于可用状态，只需进行少量修改即可满足具体需求。</li>\n<li><strong>智能化的上下文理解</strong>：Cursor升级了模型后，支持超长上下文(200k)，使得它能够理解项目背景、代码结构及编码风格。这种能力不仅提升了生成代码的相关性和准确性，还使得模型能够根据历史交互进行智能响应，提供更为精准的建议和解决方案。能按照生成的既有风格进行演进。</li>\n<li><strong>知识储备与效能提升</strong>：你的技术知识储备越丰富，那么Cursor帮你提升的效率会越高。比如，如果方法论知识丰富，它能帮忙快速的提升代码的可读性、可维护性和扩展性，在重构方面表现尤为突出。</li>\n</ul>\n<p>当然现在的边界情况是简单产品，其实进入到深水区后，依然会遇到很多卡点，不过没关系，迈出的这一步很重要。</p>\n<h2 id=\"最后的最后\"><a href=\"#最后的最后\" class=\"headerlink\" title=\"最后的最后\"></a>最后的最后</h2><h3 id=\"附上一些使用Cursor的技巧\"><a href=\"#附上一些使用Cursor的技巧\" class=\"headerlink\" title=\"附上一些使用Cursor的技巧\"></a>附上一些使用Cursor的技巧</h3><ul>\n<li><strong>表述需求时的注意事项</strong>：在表述需求时，务必做到明确而谨慎。清晰的需求能够帮助 Cursor 精确理解并执行，而不必要的信息可能会增加实现的复杂性。例如，当我错误地提到使用Ollama3.1时，实际上在编写代码时，Cursor是将Ollama3.1作为模型引用的，但正确的模型应该是Llama3.1。同时，过度简化需求可能会导致 Cursor 自由发挥，产生不必要的偏差。例如，我最开始并没有提到使用大模型来满足需求，只是简单描述了需求。在最初的代码版本中，就并未采用大模型框架来实现。</li>\n<li><strong>README.md 文档的重要性</strong>：让 Cursor 从项目一开始就写 README.md 文档，记录清楚产品功能、实现技术栈等，并且让它在完成关键节点后及时更新。</li>\n<li><strong>中间版本的救命工具</strong>：用 Git 做版本管理，在关键成功节点提交时写好说明内容，给自己复原的机会。否则，等一段时间后，你想要回退到某一节点的时候，那个节点就已经找不到了。</li>\n<li><strong>使用 Composer 和 Chat 功能的建议</strong>：使用 Composer 和 Chat 功能时，尽量多 @codebase，否则 Cursor 常有幻觉，不知道项目内容是什么。</li>\n<li><strong>文档链接的整合</strong>：常用的文档链接可以加入 Docs 中，比如你习惯使用的 AI API 调用文档，方便在使用时随时 @。（太好用了）</li>\n<li><strong>隐私选项</strong>：如果用于项目，请记得勾上<a href=\"https://www.cursor.com/privacy\">Privacy Mode</a>，这个选项能够保证你的代码不被第三方或者Cursor官方保存。（明面上是这么说的）</li>\n</ul>\n<p>到此，我们的讨论就告一段落了。感谢大家的阅读和关注!</p>\n",
            "tags": [
                "Cursor"
            ]
        },
        {
            "id": "https://gszhangwei.github.io/2022/08/31/hello-world/",
            "url": "https://gszhangwei.github.io/2022/08/31/hello-world/",
            "title": "Hello World",
            "date_published": "2022-08-31T04:29:30.315Z",
            "content_html": "<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n",
            "tags": [
                "Hexo"
            ]
        }
    ]
}